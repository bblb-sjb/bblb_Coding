# Java基础面试篇

## 概念

### Java特点

- 平台无关
- 面向对象
- 拥有自己的内存管理机制

### Java为什么跨平台

Java 之所以能够跨平台，是因为不同操作系统都有对应版本的 Java 虚拟机（JVM）。只要在目标平台上安装相应的 JVM，就能运行 Java 的字节码文件（`.class`）。

字节码本身是跨平台的，但 JVM 并不能跨平台。不同操作系统需要不同版本的 JVM 来解析 Java 字节码，并将其转换为该平台对应的机器码，以供执行。

### JVM、JDK、JRE关系

JVM<JDK<JRE

- JVM是虚拟机，本质上是一个用来运行java字节码文件的程序。
  - 对字节码文件中的指令，实时的解释成机器码供计算机执行
  - 自动为对象、方法分配内存以及回收不再使用的对象
  - 即时编译**JIT**对热点代码进行优化，提升效率，JIT将热点代码转为机器码后存至RAM，下次运行可以直接从RAM中调用。

- JRE是虚拟机+类库，也就是java的运行时环境。

- JDK包含虚拟机jvm、编译器javac、调试器jdb和一些标准库和工具库。

## 面向对象

### 什么是面向对象，什么是封装继承多态？

面向对象就是将事物抽象成对象，提取出对应的属性或者方法，以该对象为中心，通过对象与对象之间交互来完成所需功能。

java面向对象三大特征：封装、继承、多态

- 封装：将对象的属性和方法结合起来，对外隐藏内部细节，仅通过对象的暴露出来的接口进行交互。
- 继承：子类共享父类数据结构和方法，是代码复用的主要手段。
- 多态：指多个不同类的对象对同一接口展现出不同的运行状态，也就是不同实力对同一接口表现出来的不同操作，多态分为编译时多态（重载）和运行时多态（重写）。

### 多态体现的方面

- 方法重载：同一个命名的方法可以有多种参数列表。

- 方法重写：子类可以重写父类同名行为。

- 接口与实现：多个类可以实现同一接口，多个动物类实现动物接口并调用动物接口的方法则会出发对应的实现。

- 向上向下转型

  - 向上：父类引用指向子类对象，只能调用父类忒的那个方法，不能调用子类特有方法。

    ```java
    Animal animal = new Dog();
    animal.makeSound();
    ```

  - 向下：父类引用转为子类类型，转为子类类型必须强制转换，可以调用子类特有方法。

    ```java
    Animal animal = new Dog();
    Dog dog = (Dog) animal;
    dog.makeSound();
    dog.wagTail();`
    ```

    向下转有风险

### 面向对象设计原则

- 单一职责：一个类只负责一项职责。
- 开放封闭：对拓展开放，对修改封闭。
- 里氏替换：子类对象应该能够替换所有父类对象，并且程序的行为不会发生变化。
- 接口隔离：接口应该设计得小而专，同时通过依赖注入管理依赖关系。强调通过依赖注入来管理类之间的依赖关系，而不是在类内部直接创建依赖，从而实现松耦合。
- 依赖倒置：高层次模块不应该依赖于底层模块。
- 最少知识：一个类应该对其他对象最少了解。

### 重载重写的区别

- 重载是对同一命名方法可以拥有不同的参数列表，编译器根据调用时参数类型来自动选择调用哪个方法
- 重写是子类重写父类同名方法，通过`@override`来标注

### 抽象类和实体类区别

- 实例化：抽象类不能被实例化，只能被继承
- 方法：抽象类的方法可以没有具体实现
- 继承：
  - 一个类只能继承一个普通类，但可以实现多个接口
  - 一个类只能继承一个抽象类，但可以实现多个接口。
- 实现限制：抽象类一般是基类，供其他类继承。而实体类可以被其他类继承和使用。

### Java抽象类和接口的区别

- 两者特点：
  - 抽象类描述类的共同特性和行为，可以有具体的成员变量，构造方法和具体方法。
  - 接口可以多实现，只能有常量字段和抽象方法。

- 两者区别：
  - 实现接口的关键字`implements`，继承为`extends`，一个类可以实现多个接口，一个类只能继承一个抽象类。
  - 接口只能有定义，而抽象类可以有定义与实现。
  - 接口成员默认是常量，`public static final`，并且必须赋初值不能背修改，方法为`public abstract`的。抽象类成员默认`default`，可以被定义，抽象方法由`abstract`修饰，必须以分号结尾，不带花括号。
  - 抽象类可以由实体变量和静态变量，而接口只能由静态常量。

### 抽象类可以被实例化吗

抽象类不能通过new被实例化，但是抽象类可以有自己的构造器，在子类实例化的过程也会被调用，以便进行必要的初始化工作。

## 深拷贝浅拷贝

### 区别

浅拷贝类似于快捷方式，只是创建了一个新的对象，指向原复制对象的地址，所以如果原复制对象发生变化，他也跟着变化。

深拷贝是指在复制对象的同时，重新开辟一部分内存区域，将其的全部字面值都复制一份，创建一个新对象，与原复制对象除了值一样外没有其他关系。

### 实现深拷贝方法

- 实现Cloneable接口并重现clone方法
- 使用序列化和反序列化：将对象序列化为字节流，在通过字节流反序列化为对象实现深拷贝，要求对象和引用类型实现`Serializable`接口
- 手动复制

## 对象

### java创建对象的方式

- new `MyClass obj = new MyClass();`

- 通过反射使用Class类的newInstance()方法：

  ```java
  MyClass obj = (MyClass) Class.forName("com.example.MyClass").newInstance();
  ```

- 使用Constructor类的newInstance()方法：

  ```java
  Constructor<MyClass> constructor = MyClass.class.getConstructor(String.class);
  MyClass obj = constructor.newInstance("John");
  ```

- 使用clone()方式，`clone()` 方法是 `Object` 类的一个方法，必须实现 `Cloneable` 接口才能正常工作。

  ```java
  class MyClass implements Cloneable {
      private String name;
      MyClass(String name) {
          this.name = name;
      }
      void display() {
          System.out.println("Name: " + name);
      }
      @Override
      protected Object clone() throws CloneNotSupportedException {
          return super.clone();  // 调用 Object 的 clone() 方法
      }
  }
  public class Main {
      public static void main(String[] args) throws CloneNotSupportedException {
          MyClass original = new MyClass("John");
          MyClass cloned = (MyClass) original.clone();
          cloned.display();
      }
  }
  ```

- 使用反序列化

  ```java
  class MyClass implements Serializable {
      private String name;
      MyClass(String name) {
          this.name = name;
      }
      void display() {
          System.out.println("Name: " + name);
      }
  }
  public class Main {
      public static void main(String[] args) throws IOException, ClassNotFoundException {
          ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream("obj.dat"));
          MyClass original = new MyClass("John");
          out.writeObject(original);
          out.close();
          ObjectInputStream in = new ObjectInputStream(new FileInputStream("obj.dat"));
          MyClass deserialized = (MyClass) in.readObject();
          in.close();
          deserialized.display();
      }
  }
  ```

### new出来的对象什么时候回收

new出来的对象由GC进行回收，根据一下算法进行检测：

- 引用计数法：为每个对象维护一个引用计数器，当对象被引用时加1，取消引用时减1。
- 可达性分析法：可达性分析将对象分类两类：垃圾回收的根对象GCRoot和普通对象，如果某个到GCRoot对象是可达的那么这个对象就不可回收。

## 反射

### 什么是反射

指在运行中对于任意一个类或对象都知道其所有属性及方法，这种动态获取信息及动态调用对象的方法称为java的反射机制。

- 运行时类信息访问。

- 动态对象创建，即使不知道具体类名也可以通过Class类的newInstance()或Constructor对象的newInstance()创建。

- 动态方法调用，通过调用Method的invoke方法实现，可以调用私有方法。

- 访问和修改字段值，即使时私有的也可以通过Field类的get()和set()方法调用。

  ```java
  class Person {
      private String name = "Alice";
      private void sayHello() {
          System.out.println("Hello, my name is " + name);
      }
  }
  Person person = new Person();
  Field nameField = Person.class.getDeclaredField("name");
  nameField.setAccessible(true); // 允许访问私有字段
  System.out.println("Before modification: " + nameField.get(person));
  nameField.set(person, "Bob"); // 修改私有字段的值
  Method sayHelloMethod = Person.class.getDeclaredMethod("sayHello");
  sayHelloMethod.setAccessible(true); // 允许访问私有方法
  sayHelloMethod.invoke(person); // 调用方法
  ```

### 反射应用场景

- 加载数据库驱动

  我们可以根据实际情况通过反射动态的来加载驱动类，那么在JDBC连接数据库的时候我们就可以通过Class.forName()通过反射加载对应的驱动类

  ```java
  Class.forName("com.mysql.cj.jdbc.Driver");
  Connection connection = DriverManager.getConnection(url, user, password);
  ```

  通过反射加载 MySQL JDBC 驱动类，触发其 `static` 代码块注册驱动。通过 `DriverManager` 获取数据库连接，内部会调用已注册的驱动程序的 `connect()` 方法。

- 配置文件加载

  Spring IoC 容器的本质可以类比为一个 HashMap，其中 Key 是 Bean 的名称，Value 是实例化的对象。Spring 通过反射动态加载 Bean：

  - 解析 `beans.xml` 或 `@Configuration` 类，获取需要实例化的类名。
  - 通过 `Class.forName()` 反射加载类，并使用`Constructor.newInstance()` 创建对象。
  - 通过 `Field.set()` 或 方法反射调用（setter、构造器）进行依赖注入。
  - `BeanPostProcessor` 允许在 Bean 初始化前后 进行额外处理，如AOP 代理等。

## 注解

注解可以作用在类上、方法上、字段上。注解的本质时继承了Annotation的特殊接口，具体实现时java运行时生成的动态代理。

## 异常

### 常见异常

错误和异常均继承于Throwable类

- 错误Error：OutOfMemoryError、StackOverflowError
- 异常Exception：
  - 非运行时异常：文件不存在、类未找到
  - 运行时异常：空指针、数组越界

### 异常处理

通过try-catch

```java
try {
    int result = 10 / 0; // 触发 ArithmeticException
} catch (ArithmeticException e) {
    System.out.println("捕获异常：" + e.getMessage());
} finally {
    System.out.println("无论如何都会执行的 finally 代码块！");
}
```

## object

### ==和equals

==对比的是对象的首地址是否一样，equals是对比字面值是否一样

### String、StringBuilder和StringBuffer

- String是不可变字符串，String对象是在jdk8之后是存储在堆中，具体存储位置取决于创建方式，常量创建放在字符串常量池，new出来放在普通对象中，`intern()`手动放入常量池。StringBuilder和StringBuffer也都创建在队中的普通对象里。

- StringBuffer线程安全，StringBuilder线程不安全
- 因为StringBuffer引入了线程安全，所以速度要慢于StringBuilder，但都大于String，因为String修改要频繁进行字符串常量池变更。

## 序列化

### 怎么把一个对象从一个jvm转移到另一个jvm

- 序列化与反序列化`Serializable`
- 使用中间件的消息传递机制
- 远程方法调用RPC（Remote Procedure Call）、Feign等
- 共享数据库

#### 序列化和反序列化让你自己实现你会怎么做?

要么手动拼接JSON字符串，要么使用成熟的框架Fastjson、Jackson

```java
import com.alibaba.fastjson.JSON;
public class FastjsonExample {
    public static void main(String[] args) {
        MyClass obj = new MyClass("John", 30);
        String json = JSON.toJSONString(obj);// 序列化
        System.out.println(json);  // Output: {"age":30,"name":"John"}
        // 反序列化
        MyClass deserializedObj = JSON.parseObject(json, MyClass.class);
        System.out.println(deserializedObj.name);  // Output: John
        System.out.println(deserializedObj.age);   // Output: 30
    }
}
```

## 设计模式

### volatile和sychronized如何实现单例模式

`volatile` 关键字可以防止 JVM 对变量进行指令重排，`sychronized`是加锁

有两种实现方式

- 单sychronized，保证一个线程只会拿到一把锁

  ```java
  public class Singleton {
      private static Singleton instance;
      private Singleton() {}// 私有构造函数，防止外部实例化
      public static synchronized Singleton getInstance() {
          if (instance == null) {
              instance = new Singleton(); // 创建实例
          }
          return instance;
      }
  }
  ```

​	缺点：每个线程所对应的对象无论是否创建都会拿锁，性能较差

- volatile+更细粒度的锁

  ```java
  public class Singleton {
      private static volatile Singleton instance;// 使用 volatile 防止指令重排
      private Singleton() {} // 私有构造函数，防止外部实例化
      public static Singleton getInstance() {
          // 第一次检查，不加锁
          if (instance == null) {
              synchronized (Singleton.class) {
                  // 第二次检查，加锁
                  if (instance == null) {
                      instance = new Singleton(); // 创建实例
                  }
              }
          }
          return instance;
      }
  }
  ```

​	只有在第一次检查没有实例化的时候才会加锁，`volatile`保证顺序，更细粒度的锁`synchronized`保证性能。

### 代理模式和适配器模式有什么区别？

代理模式通过是增强功能，例如AOP来对目标方法进行增强，比如增加日志、事务、权限检查等。

适配器模式是为了适配不同场景不同应用。

## I/O

### BIO、和NIO、AIO区别

| 特性         | BIO                                | NIO                                         | AIO                              |
| ------------ | ---------------------------------- | ------------------------------------------- | -------------------------------- |
| **工作模式** | 阻塞 I/O，数据流模式               | 非阻塞 I/O，缓冲区 + 通道 + 选择器          | 异步 I/O，回调通知               |
| **线程模型** | 每个连接一个线程                   | 多个通道由单线程管理（通过 Selector）       | 异步 I/O，无需阻塞或轮询         |
| **I/O 操作** | 阻塞，直到 I/O 操作完成            | 非阻塞，可以轮询多个通道的事件              | 异步，不会阻塞，完成时回调通知   |
| **适用场景** | 并发连接数少，低性能需求           | 高并发、大量连接，I/O 密集型应用            | 超高并发、大数据量 I/O 操作      |
| **性能问题** | 并发连接数多时性能差（线程开销大） | 比 BIO 性能好，但需要轮询（高并发时更有效） | 性能非常高，尤其适用于高并发应用 |

### NIO如何实现

![1](C:\Users\93752\Desktop\工作面试\小林coding\01-java基础res\1.png)

每个客户端通过通道（Channel）与服务端进行数据交互，客户端通过端口向服务端发送连接请求。服务端使用一个线程，通过多路复用器（Selector）来监听多个客户端的连接请求和数据事件，服务端会将每个客户端的通道注册到 Selector 上进行管理。

### Netty

Netty 是建立在 Java NIO 之上的框架，它在底层使用 NIO 的 Selector（选择器）和非阻塞 I/O来处理并发的连接。与传统的 NIO 编程方式相比，有一下特点：

- 事件驱动和异步机制：通过回调机制处理I/O结果
- boss-worker：boss只负责管理，具体处理交给worker，还有更加智能的线程管理
- 内存管理优化：采用内存池化区别于传统每次读取都会分配新的字节数组
- 支持多种协议：不仅有tcp、udp还有http、https、websocket等等

## 其他

### 有一个学生类，想按照分数排序，再按学号排序，应该怎么做？

- 如果这个学生类是原生数组，直接用Array.sort配合内部函数实现自定义排序。

  ```java
  Arrays.sort(students,(a,b)->{
      if(a.getScore()!=b.getScore()){
          return b.getScore()-a.getScore();
      }else{
          return a.getId()-b.getId();
      }
  });
  ```

- 如果这个学生类是List集合，建议建议 `Student` 实现 `Comparable<Student>`以提供自定义排序

  ```java
  class Student implements Comparable<Student> {
      private int score;
      private int id;
  	... //构造、getter、setter
      @Override
      public int compareTo(Student other) {
          if (this.score != other.score) {
              return Integer.compare(other.score, this.score); // 降序
          }
          return Integer.compare(this.id, other.id); // 升序
      }
  }
  ```

### Native

在 Java 中，`native` 关键字用于声明本地方法（Native Method），这些方法的实现通常由 C 或 C++ 编写，并通过 JNI（Java Native Interface）调用。

用native声明的方法没有方法体

- native方法声明：

  ```java
  public class NativeExample {  
      public native void sayHello();// 声明一个 native 方法
      // 加载 C/C++ 库
      static {
          System.loadLibrary("NativeLib"); // 加载 "NativeLib.dll"（Windows）或 "libNativeLib.so"（Linux）
      }
      public static void main(String[] args) {
          new NativeExample().sayHello(); // 调用本地方法
      }
  }
  ```

- C/C++ 代码实现

  创建 `NativeExample.h`（通过 `javac` + `javah` 生成）

  ```java
  ###include <jni.h>
  ###include <stdio.h>
  ###include "NativeExample.h"
  JNIEXPORT void JNICALL Java_NativeExample_sayHello(JNIEnv *env, jobject obj) {
      printf("Hello from C!\n");
  }
  ```

  编译成动态库：

  - **Windows**: `gcc -shared -o NativeLib.dll NativeExample.c -I"%JAVA_HOME%\include" -I"%JAVA_HOME%\include\win32"`
  -  class Solution {    public int singleNumber(int[] nums) {        int single=0;        for(int num:nums){            single^=num;        }        return single;    }}java

# Java集合面试篇

## 概念

### 数组与集合区别

- 数组是定长的，集合是动态的
- 数组是可以包含基本数据类型和对象的，而集合只能包含对象
- 数组是可以直接通过下标进行访问元素的，而集合需要通过迭代器等进行访问。

### 常用集合

- ArrayList：动态数组，`add(E e)`

- LinkedList：双向链表，`add(E e)`
- HashMap：哈希map，`put(K, V)`
- HashSet：哈希set，`add(E e)`
- ArrayDeque：双向队列
  - 栈：使用 `push()` 入栈，`pop()` 出栈，`peek()` 查看栈顶
  - 队列：使用 `offer()` 入队尾，`poll()` 出队头，`offerFirst` 入队头，`pollLast` 出队尾

### Java中的线程安全的集合是什么？

常用的：

- `CopyOnWriteArrayList`：读操作无锁，写操作复制新数组，适用于读多写少的场景，如配置管理、黑名单等。写代价高，add每次都会创建新数组。

- `ConcurrentHashMap`：与 `HashTable` (也是线程安全的，表级别锁)的主要区别是二者加锁粒度的不同，支持行锁，适合高并发读写。
- `ConcurrentLinkedQueue`：基于 CAS（无锁队列），高效且支持高并发。适用于生产者-消费者模型（如任务队列）。

### Collections和Collection的区别

| **特点** | **`Collection`**                         | **`Collections`**                            |
| -------- | ---------------------------------------- | -------------------------------------------- |
| **类型** | 接口                                     | 工具类（类）                                 |
| **功能** | 定义集合的基本操作，如添加、删除、查询等 | 提供静态方法来操作集合，如排序、查找、同步等 |
| **用法** | 用作集合的父接口，具体集合类实现该接口   | 用于对集合进行操作，不能实例化               |

## List

### java中list的几种实现

- 线程不安全：
  - ArrayList：基于动态数据实现，支持随机访问，初始容量为10，满了会扩容是扩容50%
  - LinkedList：基于双向链表实现，不需要初始容量
- 线程安全：
  - Vector：基于动态数组实现，加上了synchronized关键字，初始容量为10，满了会扩容是扩容1倍。
  - CopyOnWriteArrayList：读操作无锁，写操作复制新数组，适用于读多写少的场景。

### 把ArrayList变成线程安全的有哪些方法？

- 使用`Collections.synchronizedXxx()`（包装同步集合），该方法对普通集合进行同步包装，使其线程安全，但在迭代时仍需手动同步。该方法只对集合的操作进行保护，并为队迭代操作及逆行自动加锁，所以迭代荣然需要显式的同步。

  ```java
  List<String> list = Collections.synchronizedList(new ArrayList<>());
  synchronized (list) { //多线程访问
      for (String s : list) {
          System.out.println(s);
      }
  }
  ```

- 使用CopyOnWriteArrayList或Vector类代替ArrayList

### CopyOnWriteArrayList是如何保证线程安全的？

- 读操作没有锁，因为在每次写操作前都会生成一个快照，读操作读的都是快照。
- 写操作，使用volatile关键字修饰数组，保证顺序和可见性，并且每次写入时都加锁并且会复制整个数组，并将修改后的新数组设置为当前数组。

## Map

### java中常见map

- 线程不安全：
  - HashMap：基于数组+链表+红黑树实现，支持随机访问，初始容量为16，扩容因子0.75，达到额定容量75%会进行，扩容是扩容一倍。
- 线程安全：
  - HashTable：实现方式与HashMap类似，但是在方法上加上`synchronized`保证线程安全，同一时刻只能有一个线程访问HashTable的方法，但是锁是表级锁。
  - ConcurrentHashMap：通过分段锁和 CAS 实现细粒度锁，适合高并发环境。

### HashMap实现原理介绍一下？

在JDK8之前的HashMap实现中，HashMap使用哈希算法将键（key）映射到数组中的索引位置。如果两个或多个键的哈希值相同，即发生了哈希冲突，HashMap会通过链表解决冲突：将新加入的元素以链表的形式存储在对应的索引位置，成为该位置的链表头节点（链表的第一个元素）。

JDK8之后，如果链表长度超过8就转为红黑树保存，小于6时原转为链表。

### HashMap为什么是线程不安全的？

`HashMap` 是线程不安全的，主要是因为它没有内建的同步机制。在多线程环境下，多个线程可能会同时访问和修改 `HashMap`，导致数据一致性问题、数据丢失和结构损坏。

**要确保线程安全：**

- 可以使用 `ConcurrentHashMap` 
- 通过 `Collections.synchronizedMap()` 对 `HashMap` 进行同步包装。

### 解决Hash冲突的办法

- 链表法
- 开放地址：在数组内找个新的地方放：线性探测（+1）、二次探测（+1^2+2^2+3^2）、双重哈希（使用第二个哈希函数）
- 再哈希：当负载因子超过某个阈值重新计算哈希表的大小。

### hashmap key可以为null吗？

可以为null，如果为null，那其哈希值直接为0。

### 重写HashMap的equal和hashcode方法需要注意什么？

- `equals()` 和 `hashCode()` 必须保持一致性：相等的对象 `equals()` 返回 `true`，则它们的 `hashCode()` 必须相同。

- 实现 `hashCode()` 时要确保散列值均匀，避免大量冲突。

### ConcurrentHashMap底层是怎么实现的

`ConcurrentHashMap` 通过以下技术实现了线程安全和高并发性：

1. **桶级锁**：将整个映射划分为多个桶，每个桶使用独立的锁，减少锁竞争。
2. **CAS 操作**：采用 CAS 操作来更新桶中的值，避免不必要的锁。
3. **链表和红黑树**：使用链表或红黑树来存储桶中的元素，优化查询性能。
4. **同步扩容**：在扩容时使用同步机制来保证线程安全。
5. **高效并发支持**：通过细粒度的锁和优化的锁策略，保证高并发场景下的性能。

### ConcurrentHashMap用了悲观锁还是乐观锁?

首先CAS是乐观锁，synchronized 是悲观锁。

- 乐观锁的基本思想是：假设多个线程不会发生冲突，因此在操作数据时，不会立即加锁，而是先进行尝试。如果出现冲突，才进行修正。`CAS` 是一种硬件支持的机制（通过 CPU 指令实现），它通过比较内存中的值与预期值是否相等，如果相等，就更新值，否则就不做任何操作，返回失败。它是无锁的，因此不会像传统的锁那样造成线程阻塞。

- 悲观锁的基本思想是：假设多个线程一定会发生冲突，因此在访问共享资源时会采取 加锁 的方式，保证同一时刻只有一个线程可以访问该资源。

`ConcurrentHashMap` 是一种高效的线程安全的 Map实现，它结合了乐观锁和悲观锁的思想，但总体上可以认为它采用的是分段锁（Segment Lock）和乐观锁结合的方式。具体来说，它在不同的操作中使用了不同的锁策略，来优化并发性能。

- JDK8之前是分段锁+synchronized悲观锁。

- JDK8之后`ConcurrentHashMap` 改进了实现，采用了CAS（乐观锁）与轻量级锁相结合的方式：
  - 读操作不加锁：对于 `get` 操作，它不加锁。
  - 写操作会使用 CAS 尝试更新数据。如果没有发生冲突，`CAS` 会直接更新值，不需要加锁，这也是一种乐观锁。如果 CAS 失败（即发生竞争），`ConcurrentHashMap` 会采用 悲观锁（例如使用 `ReentrantLock`）来保护更新操作，以保证线程安全。这是因为在竞争激烈的情况下，使用悲观锁能够确保写操作的正确性，避免数据不一致。
  - 扩容是会用悲观锁来同步该过程。

## Set

map和set插入时都是先用hashCode来判断位置，set使用equals来判断set中集合是否存在值相同的元素，如果存在则不会插入。

### 有序的set

- TreeSet是基于红黑树实现
- LinkedHashSet是基于双重链表和哈希表的结合来实现元素的有序存储

# Java并发面试篇

## 多线程

### java里面的线程和操作系统的线程一样吗？

在类 UNIX 系统中，Java 通过 `pthread_create` 来创建内核级线程，并且返回线程创建是否成功的状态码。大多数情况下，JVM 中的 Java 线程和操作系统的内核级线程是一一对应的。

###  使用多线程要注意哪些问题？

- 原子性：互斥访问（使用锁），使用支持原子操作的类（atomic包）
- 可见性：一个线程对内存的修改可以被其他线程感知，`volatile` 通过禁止线程的本地缓存和强制从主内存读取数据来确保变量的最新值对所有线程可见。
- 有序性：使用volatile关键字保证指令不会被重排。
- 避免死锁及线程饥饿。

### 保证数据一致性的方案有哪些？

- 事务：使用事务（事务属性：ACID原子性、一致性、隔离性、持久性）来控制一系列操作，要么都成功要么都失败回滚。
- 锁：悲观锁
- 版本控制：乐观锁

### 创建线程的方法

- 如果没有继承类的话可以直接继承Thread类，重写`run()`方法

  ```java
  class MyThread extends Thread {
      @Override
      public void run() {
          ... // 执行线程的任务
      }
  }
  public class Main {
      public static void main(String[] args) {
          MyThread thread = new MyThread();
          thread.start();  // 启动线程
      }
  }
  ```

  调用 `start()` 方法会启动线程，而不是直接调用 `run()` 方法。`start()` 方法会内部调用 `run()` 方法。

- 如果该类继承了某个类方法，那么可以实现`Runnable`接口并重写其`run()`方法

  ```java
  class MyRunnable implements Runnable {
      @Override
      public void run() {
          ... // 执行线程的任务
      }
  }
  public class Main {
      public static void main(String[] args) {
          MyRunnable myRunnable = new MyRunnable();
          Thread thread = new Thread(myRunnable);
          thread.start();  // 启动线程
      }
  }
  ```

- 也可以直接实现`Callable` 。`Callable` 是一个功能更强大的接口，类似于 `Runnable`，但是它可以返回结果。通过 `ExecutorService` 提交 `Callable` 任务，并通过 `Future` 获取任务的执行结果。

  ```java
  class MyCallable implements Callable<Integer> {
      @Override
      public Integer call() throws Exception {
      	...
          return 42; // 返回结果
      }
  }
  public class Main {
      public static void main(String[] args) throws InterruptedException, ExecutionException {
          MyCallable myCallable = new MyCallable(); // 创建 MyCallable 实例
          // 使用 FutureTask 来包装 Callable
          FutureTask<Integer> futureTask = new FutureTask<>(myCallable);
          Thread thread = new Thread(futureTask); // 使用 Thread 执行 FutureTask
          thread.start(); // 启动线程
          // 等待线程执行结果
          Integer result = futureTask.get(); // get() 会阻塞直到任务完成
          System.out.println("Task result: " + result);      
          // 等待线程结束
          thread.join(); // 等待线程执行完毕
      }
  }
  ```

- 线程池

  使用线程池来管理线程，`ExecutorService` 是 JDK 5 引入的线程池框架，提供了更高效、更方便的线程管理。可以使用 `ExecutorService` 来创建并执行线程池中的线程。

  ```java
  public class Main {
      public static void main(String[] args) {
          ExecutorService executorService = Executors.newFixedThreadPool(2);  // 创建定长线程池
          executorService.submit(() -> {
              ... // 执行任务   
          });
          executorService.shutdown();  // 关闭线程池
      }
  }
  ```

## 并发安全

### juc包下常用的类

- 线程池相关：
  - `ThreadPoolExecutor`：最核心的线程池类，线程池的真正实现类是 ThreadPoolExecutor
  - `Executors`：线程工厂类，Executors已经为我们封装好了 4 种常见的功能线程池，如下：
    - 定长线程池（FixedThreadPool）：只有核心线程，线程数量固定，执行完立即回收，任务队列为链表结构的有界队列。用于控制线程最大并发数。
    - 定时线程池（ScheduledThreadPool ）：核心线程数量固定，非核心线程数量无限，执行完闲置 10ms 后回收，任务队列为延时阻塞队列。用于执行定时或周期性的任务。
    - 可缓存线程池（CachedThreadPool）：无核心线程，非核心线程数量无限，执行完闲置 60s 后回收，任务队列为不存储元素的阻塞队列。用于执行大量、耗时少的任务。
    - 单线程化线程池（SingleThreadExecutor）：只有 1 个核心线程，无非核心线程，执行完立即回收，任务队列为链表结构的有界队列。不适合并发但可能引起 IO 阻塞性及影响 UI 线程响应的操作，如数据库操作、文件操作等。
- 线程安全的并发集合类：
  - CopyOnWriteArrayList
  - ConcurrentHashMap
- 同步工具类：
  - Semaphore：信号量
- 原子类

### 怎么保证多线程安全

除了使用juc包下的类，还可以通过一些关键字：

- `synchronized`关键字：可以使用`synchronized`关键字来同步代码块或方法，确保同一时刻只有一个线程可以访问这些代码。
- `volatile`关键字：`volatile` 通过禁止线程的本地缓存和强制从主内存读取数据来确保变量的最新值对所有线程可见。

### Java中常用的锁

- 悲观锁：

  - `synchronized` 锁：分为方法级锁和代码块级锁

  - `ReentrantLock`： 是 `java.util.concurrent.locks` 包中的一个显式锁实现，它实现了与 `synchronized` 相同的功能，但提供了更多的灵活性和控制，可以更更加细粒度的控制锁。

    ```java
    Lock lock = new ReentrantLock();
    lock.lock();  // 获取锁
    try {
        // 线程安全代码
    } finally {
        lock.unlock();  // 释放锁
    }
    ```

  - `ReadWriteLock` ：是一种读写分离的锁，适用于读多写少的场景。它允许多个线程并行读取共享资源，但写操作必须独占访问权限。读锁多个线程可以同时持有，并且并发读取。写锁是线程独占的。

    ```java
    ReadWriteLock rwLock = new ReentrantReadWriteLock();
    Lock readLock = rwLock.readLock();
    Lock writeLock = rwLock.writeLock();
    readLock.lock();// 获取读锁
    try {
        // 线程安全的读取操作
    } finally {
        readLock.unlock();
    }
    writeLock.lock();// 获取写锁
    try {
        // 线程安全的写操作
    } finally {
        writeLock.unlock();
    }
    ```

  - `Semaphore`：信号量也能被认为是一种特殊的锁，是一种控制同时访问共享资源的数量的同步机制。它通过一个计数器来控制访问权限，适用于控制并发访问的场景。

    ```java
    Semaphore semaphore = new Semaphore(3);  // 允许最多3个线程同时访问资源
    semaphore.acquire();  // 获取信号量，减少可用许可
    try {
        // 线程安全代码
    } finally {
        semaphore.release();  // 释放信号量，增加可用许可
    }
    ```


- 乐观锁：

  - 版本号或者时间戳：通过在每个数据对象中维护一个版本号，读线程读取数据时获取版本号，写线程修改时检查版本号是否一致。如果一致则更新数据，并更新版本号，否则重试。

  - CAS：使用 `AtomicInteger`、`AtomicReference` 等类中的原子操作来实现乐观锁。

    ```java
    AtomicInteger count = new AtomicInteger(0);
    count.incrementAndGet();  // 原子操作，避免竞争条件
    ```

### synchronized和reentrantlock区别？

| 特性           | **synchronized**                  | **ReentrantLock**                       |
| -------------- | --------------------------------- | --------------------------------------- |
| **锁类型**     | 内置锁                            | 显式锁                                  |
| **加锁和解锁** | 自动加锁，自动解锁                | 通过 `lock()` 加锁，`unlock()` 解锁     |
| **可中断性**   | 不可中断                          | 可中断，通过 `lockInterruptibly()` 方法 |
| **公平性**     | 非公平锁                          | 可设置为公平锁                          |
| **可重入性**   | 可重入                            | 可重入                                  |
| **条件变量**   | 内建条件变量 `wait()`、`notify()` | 通过 `Condition` 提供灵活的条件变量     |
| **性能**       | 较低，尤其在竞争激烈时            | 更高，特别在高并发和需要灵活控制的场景  |
| **死锁避免**   | 需要开发者小心避免死锁            | 提供 `tryLock()` 方法来避免死锁         |

### 如何理解可重入锁？

可重入锁允许同一线程在已经持有锁的情况下再次获取该锁，背后通过一个计数器来记录线程获取锁的次数。每当线程请求锁时，计数器增加；每次释放锁时，计数器减少，直到计数器为 0 时，锁才会被完全释放。这种机制避免了线程在递归或多层方法调用中因重复加锁导致的死锁问题。

### 死锁产生的必要条件

- 互斥条件：只有对必须互斥使用的资源的争抢才会导致死锁
- 不剥夺条件：进程所获得的资源在未使用完之前，不能由其他进程强行夺走，只能主动释放。
- 请求和保持条件：进程已经保持了至少一个资源，但又提出了新的资源请求，而该资源又被其他进程占有，此时请求进程被阻塞，但又对自己已有的资源保持不放。
- 循环等待条件：存在一种进程资源的循环等待链，链中的每一个进程已获得的资源同时被下一个进程所请求。

死锁的处理策略：破坏其中任意一个即可

## 线程池

### 线程池参数

```java
// 创建线程池
ThreadPoolExecutor threadPool = new ThreadPoolExecutor(CORE_POOL_SIZE,
                                             MAXIMUM_POOL_SIZE,
                                             KEEP_ALIVE,
                                             TimeUnit.SECONDS,
                                             sPoolWorkQueue,
                                             sThreadFactory);
// 向线程池提交任务
threadPool.execute(new Runnable() {
    @Override
    public void run() {
        ... // 线程执行的任务
    }
});
// 关闭线程池
threadPool.shutdown(); // 设置线程池的状态为SHUTDOWN，然后中断所有没有正在执行任务的线程
threadPool.shutdownNow(); // 设置线程池的状态为 STOP，然后尝试停止所有的正在执行或暂停任务的线程，并返回等待执行任务的列表
```

- `corePoolSize` 核心线程数（必需)

  默认情况下，核心线程会一直存活，但是当将 `allowCoreThreadTimeout`设置为 true 时，核心线程也会超时回收。

- `maximumPoolSize` 最大线程数（必需)

  线程池所能容纳的最大线程数。当活跃线程数达到该数值后，后续的新任务将会阻塞

- `keepAliveTime` 线程闲置超时时长（必需）

  如果超过该时长，非核心线程就会被回收。如果将`allowCoreThreadTimeout`设置为 true 时，核心线程也会超时回收。

- `unit` 超时时间的单位（必需）

  指定`keepAliveTime`参数的时间单位。常用的有：`TimeUnit.MILLISECONDS`（毫秒）、`TimeUnit.SECONDS`（秒）、`TimeUnit.MINUTES`（分）。

- workQueue（必需）

  任务队列。通过线程池的 execute() 方法提交的 Runnable 对象将存储在该参数中。其采用阻塞队列实现。

- threadFactory（可选）

  线程工厂。用于指定为线程池创建新线程的方式。线程工厂指定创建线程的方式，需要实现 ThreadFactory 接口，并实现 newThread(Runnable r) 方法。该参数可以不用指定，Executors 框架已经为我们实现了一个默认的线程工厂。

- handler（可选）

  拒绝策略。当达到最大线程数时需要执行的饱和策略。

#### 任务队列workQueue

任务队列是基于阻塞队列实现的，即采用生产者消费者模式，在 Java 中需要实现BlockingQueue 接口。但 Java 已经为我们提供了 7 种阻塞队列的实现

- ArrayBlockingQueue：一个由数组结构组成的有界阻塞队列（数组结构可配合指针实现一个环形队列）。
- LinkedBlockingQueue： 一个由链表结构组成的有界阻塞队列，在未指明容量时，容量默认为 Integer.MAX_VALUE。
- PriorityBlockingQueue： 一个支持优先级排序的无界阻塞队列，对元素没有要求，可以实现 Comparable 接口也可以提供 Comparator 来对队列中的元素进行比较。跟时间没有任何关系，仅仅是按照优先级取任务。
- DelayQueue： 类似于PriorityBlockingQueue，是二叉堆实现的无界优先级阻塞队列。要求元素都实现 Delayed 接口，通过执行时延从队列中提取任务，时间没到任务取不出来。
- SynchronousQueue： 一个不存储元素的阻塞队列，消费者线程调用 take() 方法的时候就会发生阻塞，直到有一个生产者线程生产了一个元素，消费者线程就可以拿到这个元素并返回；生产者线程调用 put() 方法的时候也会发生阻塞，直到有一个消费者线程消费了一个元素，生产者才会返回。
- LinkedBlockingDeque： 使用双向队列实现的有界双端阻塞队列。双端意味着可以像普通队列一样 FIFO（先进先出），也可以像栈一样 FILO（先进后出）。
- LinkedTransferQueue： 它是ConcurrentLinkedQueue、LinkedBlockingQueue 和 SynchronousQueue 的结合体，但是把它用在 ThreadPoolExecutor 中，和LinkedBlockingQueue 行为一致，但是是无界的阻塞队列。

注意有界队列和无界队列的区别：如果使用有界队列，当队列饱和时并超过最大线程数时就会执行拒绝策略；而如果使用无界队列，因为任务队列永远都可以添加任务，所以设置 maximumPoolSize 没有任何意义。

#### 拒绝策略handler

当线程池的线程数达到最大线程数时，需要执行拒绝策略。拒绝策略需要实现 RejectedExecutionHandler 接口，并实现 rejectedExecution(Runnable r, ThreadPoolExecutor executor) 方法。不过 Executors 框架已经为我们实现了 4 种拒绝策略：

- AbortPolicy（默认）：丢弃任务并抛出 RejectedExecutionException 异常。
- CallerRunsPolicy：由调用线程处理该任务。
- DiscardPolicy：丢弃任务，但是不抛出异常。可以配合这种模式进行自定义的处理方式。
- DiscardOldestPolicy：丢弃队列最早的未处理任务，然后重新尝试执行任务。

### 线程池参数经验

- CPU密集型：corePoolSize = CPU核数 + 1（避免过多线程竞争CPU）
- IO密集型：corePoolSize = CPU核数 x 2（或更高，具体看IO等待时间）

场景一：电商场景，特点瞬时高并发、任务处理时间短，线程池的配置可设置如下：

```java
ThreadPoolExecutor threadPool = new ThreadPoolExecutor(
    										 16, // 8核*2
                                             32, // 最大*2
                                             10, // 非核心线程空闲10s回收
                                             TimeUnit.SECONDS,
                                             new SynchronousQueue<>(), //不缓存任务，直接扩容
                                             new AbortPolicy() // 直接拒绝
                                 );
```

场景二：后台数据处理服务，特点稳定流量、任务处理时间长（秒级）、允许一定延迟，线程池的配置可设置如下：

```java
ThreadPoolExecutor threadPool = new ThreadPoolExecutor(
    										 8, // 
                                             8, // 禁止扩容，避免消耗资源
                                             0, // 为了稳定不回收
                                             TimeUnit.SECONDS,
                                             new ArrayBlockingQueue<>(1000), // 有界队列，容量1000
                                             new CallerRunsPolicy() // 满了叫人来
                                 );
```

场景三：微服务HTTP请求处理，特点IO密集型、依赖下游服务响应时间，线程池的配置可设置如下：

```
ThreadPoolExecutor threadPool = new ThreadPoolExecutor(
    										 16, // 8核*2
                                             64, // 下游满的话多设置一点
                                             60, // 非核心线程空闲60s回收
                                             TimeUnit.SECONDS,
                                             new ArrayBlockingQueue<>(200), // 有界队列，容量200
                                             new CustomRetryPolicy() // 自定义拒绝策略（重试或降级）
                                 );
```

### 功能线程池

- 定长线程池（FixedThreadPool）：只有核心线程，线程数量固定，执行完立即回收，任务队列为链表结构的有界队列。用于控制线程最大并发数。
- 定时线程池（ScheduledThreadPool ）：核心线程数量固定，非核心线程数量无限，执行完闲置 10ms 后回收，任务队列为延时阻塞队列。用于执行定时或周期性的任务。
- 可缓存线程池（CachedThreadPool）：无核心线程，非核心线程数量无限，执行完闲置 60s 后回收，任务队列为不存储元素的阻塞队列。用于执行大量、耗时少的任务。
- 单线程化线程池（SingleThreadExecutor）：只有 1 个核心线程，无非核心线程，执行完立即回收，任务队列为链表结构的有界队列。不适合并发但可能引起 IO 阻塞性及影响 UI 线程响应的操作，如数据库操作、文件操作等。

但是根据《阿里巴巴 Java 开发手册》中禁止使用Executors类来进行创建线程池，会引发大量的生产事故，所以线程池的使用还是要结合具体的使用场景。

### 线程池中shutdown ()，shutdownNow()这两个方法有什么作用？

- `shutdown ()`：设置线程池的状态为SHUTDOWN，然后中断所有没有正在执行任务的线程
- `shutdownNow()`：设置线程池的状态为 STOP，然后尝试停止所有的正在执行或暂停任务的线程，并返回等待执行任务的列表

# Java虚拟机篇

## 内存模型

### JVM内存模型介绍一下

运行时数据区负责管理jvm使用到的内存，比如创建对象和销毁对象。

![1](C:/Users/93752/Desktop/工作面试/小林coding/04-java虚拟机res/1.png)

- 程序计数器（PC寄存器）：对于单线程环境，保存下一条需要执行的指令的内存地址。对于进程来说，用于保存上下文信息，方便切换。因为程序计数器在每个线程只存储一个固定长度的内存地址，所以程序计数器并不会溢出。

- java虚拟机栈：随着线程的创建而创建，随着线程的销毁而回收，所以每个线程都有一个自己的虚拟机栈。虚拟机栈由一个个栈帧组成，栈帧由局部变量表、操作数栈、帧数据组成。

  - 局部变量表存放方法执行的所有局部变量，静态方法没有this，实例方法的局部变量表里存着this。
  - 操作数栈：在执行指令过程中存放中间数据的一部分区域
  - 帧数据：动态链接、方法出口、异常表

  虚拟机栈超过栈内存分配的最大大小就会出现内存溢出，出现StackOverflowError的错误。大多数默认1MB的虚拟机栈内存大小。一般如果方法正常递归也就几百，可以通过`-Xss256k`来节省内存。

- 本地方法栈：存储native本地方法的栈帧。

- 堆：创建出来的对象都在堆上，成员变量存放在堆中，局部变量在虚拟机栈中局部变量表中。JDK8前，静态变量放在方法区，JDK8之后，静态变量也在堆中。

  - 堆内存分为三类，max，uesd，total，默认max是系统内存1/4，total默认是系统内存的1/64，用`-Xmx`设置最大值，用`-Xms`设置原始值，如果原始值和最大值一样的话，这样就减少了申请或者收缩的情况。

- 方法区：方法区存放着类的元信息、运行时常量池、字符串常量池

  - 元信息是类的基本信息（结构信息、访问标识符、父类及接口），在类的加载阶段完成
  - 运行时常量池是在解析阶段将常量池中的符号引用替换为直接引用。
  - 字符串常量池存放着常量字符串内容。在**JDK8之后字符串常量池移到堆中**。

  JDK7之前方法区是在堆区域之中的永久代里，堆的到校由虚拟机参数来控制，而JDK8后方法区在元空间中，元空间位于操作系统的直接内存中。**元空间的大小一定要设置**，`-XX:MaxMetaspaceSize`设置元空间大小，设置256M接口。

- 直接内存

### JVM中的堆和栈有什么区别

- 用途：虚拟机栈保存着局部变量表、操作数栈、帧数据（动态链接、方法出口、异常表）；堆负责保存创建出来的对象及字符串常量池。
- 声明周期：栈随着线程的创建而创建，线程的销毁而回收；堆则一直存在。
- 存储空间：栈空间一般只有1MB左右；堆空间最大默认为系统内存的1/4
- 是否共享：栈空间属于线程独有；堆空间线程共享。

###  栈中存的到底是指针还是对象？

所有实例化的对象都在堆中存储，栈只是保存着其引用。

### 在分代GC中堆的存储结构

在JDK8版本后，使用分代GC回收堆的内存对象，也同时对堆的存储结构进行了细分。

![2](C:/Users/93752/Desktop/工作面试/小林coding/04-java虚拟机res/2.png)

将堆分为新生代和老年代。默认容量比例为1：2。

新生代又细分为伊甸园区、幸存区S0和幸存区S1。默认容量比例为8：1：1。

在JDK9后，老年代又细分出一个Humongous区，用于存放一些大对象。

### StringTable字符串常量池案例

- 案例一：new出来的和直接常量

  ```java
  String s1=new String("abc");
  String s2="abc";
  System.out.println(s1==s2);//false
  ```

  s1因为是被new出来的，所以s1保存在堆中，s2是指向字符串常量池的。

- 案例二：隐式new和直接常量

  ```java
  String a="1";
  String b="2";
  String c="12";
  String d=a+b;
  System.out.println(c==d);//false
  ```

  这里为什么会出现隐式new呢？我们查看字节码文件

  ![6](C:/Users/93752/Desktop/工作面试/小林coding/04-java虚拟机res/6.png)

  `String d=a+b;`相当于new出来一个StringBuilder，然后分别把a和b指向的字符串常量`append`进去了。所以c和d是不一样的。

- 案例三：隐式常量相加

  ```java
  String a="1";
  String b="2";
  String c="12";
  String d="1"+"2";
  System.out.println(c==d);//true
  ```

  这里怎么又一样了？我们还是看字节码

  ![7](C:/Users/93752/Desktop/工作面试/小林coding/04-java虚拟机res/7.png)

  取的都是字符串常量，常量字符串相加使用的是常量，而常量字符串的引用的相加是创建StringBuilder对象

### 引用类型

- 强引用：通常为GCRoot对象对普通对象有引用关系，也就是指new这种方式，只要不主动设置为null，都不会被回收。

- 软引用：软引用是一种比较弱的引用关系，如果一个对象只有软引用关联到它，当内存不足时，会回收软引用中的数据。

  - 软引用使用：将对象使用软引用包装起来，`new SoftReference<对象类型>(对象)`

    ```java
    public class SoftReferenceDemo2 {
        public static void main(String[] args) throws IOException {
            byte[] bytes = new byte[1024 * 1024 * 100];
            SoftReference<byte[]> softReference = new SoftReference<byte[]>(bytes);
            bytes = null;
            System.out.println(softReference.get());
            byte[] bytes2 = new byte[1024 * 1024 * 100];
            System.out.println(softReference.get());
    //      byte[] bytes3 = new byte[1024 * 1024 * 100];
    //      softReference = null;
            System.gc();
            System.in.read();
        }
    }
    ```

    ```
    [B@7ba4f24f
    null
    ```

    设置最大堆内存-`Xmx200m`，启动，总共200M实际能用不到200M，自然第一个放进去第二个放不进去，然后软引用就被释放回收了，打印null

    设置最大堆内存`-Xmx400m`，都能放进去，并且指向同一个软引用空间

    ```
    [B@7ba4f24f
    [B@7ba4f24f
    ```

    修改回200m，添加`byte[] bytes3 = new byte[1024 * 1024 * 100];`

    已经把软引用释放了，放进bytes2之后就不能添加了，自然报`OutOfMemoryError`。

  - 软引用回收：软引用使用引用队列进行依次回收

    ```java
    public class SoftReferenceDemo3 {
        public static void main(String[] args) throws IOException {
            ArrayList<SoftReference> softReferences = new ArrayList<>();
            ReferenceQueue<byte[]> queues = new ReferenceQueue<byte[]>();
            for (int i = 0; i < 10; i++) {
                byte[] bytes = new byte[1024 * 1024 * 100];
                SoftReference studentRef = new SoftReference<byte[]>(bytes,queues);
                softReferences.add(studentRef);
            }
            SoftReference<byte[]> ref = null;
            int count = 0;
            while ((ref = (SoftReference<byte[]>) queues.poll()) != null) {
                count++;
            }
            System.out.println(count);
        }
    }
    
    ```

    设置堆内存`200M`，只够存一个，后一个盒子把前一个盒子覆盖了，并且把前一个盒子放进queue中。所以最后一个存在，没有被回收，因此输出9

  - 软引用使用场景：缓存。例如缓存的对象是 `Student` 类的实例。通过使用软引用，缓存中的学生对象会在内存不足时被自动回收，从而避免内存溢出。每个缓存的 `Student` 对象被包装在 `StudentRef` 软引用中，并存储在一个 `HashMap` 中，软引用的回收会通过 `ReferenceQueue` 进行追踪。当垃圾回收发生时，回收的软引用会被放入队列，程序定期检查这些软引用并清理掉缓存中已被回收的对象，保证缓存中的数据始终是有效的。

- 弱引用：弱引用与软引用的整体机制与软引用类似，只不过内存够不够弱引用都会被回收。主要在ThreadLocal中使用。

- 虚引用：当对象被垃圾回收器回收的适合可以接收到对应的通知。

### 内存溢出与内存泄漏

- 内存泄漏是一个不再使用的对象仍然在GCRoot的引用链上，无法被垃圾回收器回收。绝大多数是由堆内存泄漏引起的。

- 内存泄漏原因：

  - 代码中：

    - **非正确的hashCode()和equals()方法编写**：导致多个相同id的对象保存多份。

      **解决方法：**

      1、在实体类中始终重写hashCode()和equals()方法。并使用唯一标识符区分id

      2、hashmap尽量使用id等数据作为key，避免使用实体对象。

      3、使用lombok注解@Data标注实体类。

    - **非静态的内部类引用外部类**：非静态的内部类或匿名内部类都会持有是由哪个外部类对象创建出来的，即使外部类不再被使用，这些非静态的内部类或匿名内部类也不会被垃圾回收器回收。

      **解决办法**：使用静态内部类或静态方法。

    - **线程池对ThreadLocal的使用**：使用线程池未使用remove对ThreadLocal清理对象，导致无法被回收。

      **解决办法**：单线程无需关心ThreadLocal，一旦使用线程池，应当使用remove对ThreadLocal中的对象进行手动删除。

    - **String的intern方法**：大量随机字符串被放入堆中

      **解决办法**：注意逻辑或增加堆空间

    - **静态变量的使用**：大量的数据在静态变量中被长期引用，长期不使用的话导致内存泄漏。

      **解决办法**：

      1、尽量减少对象长时间保存在静态变量中，如果不再使用将对象或静态变量设为null。

      2、使用单例模式时，使用懒汉模式而不是而饿汉模式。

      3、Spring中尽量不要长期存放大对象，缓存要设置过期时间定期失效。

    - **资源未正常关闭：**连接和流未关闭会占用资源。

      **解决办法**：使用finally块关闭资源。

  - 并发请求中：

    - 正常情况下，java会将一部分数据释放掉，但需要一些时间释放资源，但是如果用户请求并发量很大的的话，处理时间很长，就会一直存在于内存中，最终导致内存溢出，可能的原因为上述的任何一种都有可能。

## 类初始化及类加载

### 类的生命周期（类加载的过程）

类的生命周期分为加载、连接、初始化、使用、卸载

- 加载：
  - 类加载器根据类的全限定名通过不同的渠道以二进制流的方式获取字节码信息
  - 类加载器在加载完类后会将字节码信息放到方法区中，同时在堆中生成一份与方法区的数据类似的java.lang.Class的对象，包含静态字段的数据及方法。
- 连接：
  - 验证：验证内容是否满足jvm规范
  - 准备：给静态变量赋初值
  - 解析：将常量池中的符号引用（cp_info###1）替换成指向内存的直接引用（内存地址）
- 初始化：
  - 初始化阶段会执行静态代码中的代码，并为静态变量赋值
  - 初始化阶段会执行字节码文件中的clinit部分的字节码指令。
- 使用
- 卸载：垃圾回收器进行回收

### 类加载器有那些

类加载器是jvm提供给应用程序去实现获取类和接口字节码数据的技术。负责在类加载过程中的字节码获取并且加载到内存这一部分。通过加载字节码数据放入内存转换成byte[]，接下来调用虚拟机底层的方法将byte[]转换成方法区和堆中的数据。

- JDK8之前的分类

  ![8](C:/Users/93752/Desktop/工作面试/小林coding/04-java虚拟机res/8.png)

  - 引导类加载器 Bootstrap，加载属于JVM的一部分，由C++代码实现，负责加载`<JAVA_HOME\>\jre\lib`路径下的核心类库
  - 扩展类加载器 ExtClassLoader，扩展类加载器负责加载`<JAVA_HOME>\jre\lib\ext`目录下的类库。
  - 应用程序类加载器 AppClassLoader，应用程序类加载器负责加载 `classpath`环境变量所指定的类库，是用户自定义类的默认类加载器。既可以加载当前项目中创建的类，也可以加载maven依赖中包含的类。

### 类加载器的双亲委派机制

jvm中有多个类加载器，双亲委派机制的核心就是解决一个类到底由谁加载的问题。

- 双亲委派机制的作用：

  - 保证类加载的安全性：通过双亲委派机制避免恶意代码替换JDK的核心类库，比如Java.lang.String，确保核心类库的完整及安全性。
  - 避免重复加载，自下而上查找是否加载过，自上而下尝试加载，避免同一个类被重复加载。

  ![9](C:/Users/93752/Desktop/工作面试/小林coding/04-java虚拟机res/9.png)

### 打破双亲委派机制

#### 为什么？

因为如果一个应用程序要服务多个应用，那么在这些应用里可能出现相同限定名的类，但是又要加载成不同的类，比如tomcat要服务web应用中的Servlet类，那么就要打破双亲委派机制。

#### 怎么做？

- 设置自定义类加载器，修改ClassLoader的loadClass方法或findClass方法

  ```java
  @Override
  public Class<?> loadClass(String name) throws ClassNotFoundException {
      if(name.startsWith("java.")){
          return super.loadClass(name);
      }
      byte[] data = loadClassData(name);
      return defineClass(name, data, 0, data.length);
  }
  
  public static void main(String[] args) throws ClassNotFoundException, InstantiationException, IllegalAccessException, IOException {
      BreakClassLoader1 classLoader1 = new BreakClassLoader1();
      classLoader1.setBasePath("D:\\lib\\");
      Class<?> clazz1 = classLoader1.loadClass("com.itheima.my.A");
      BreakClassLoader1 classLoader2 = new BreakClassLoader1();
      classLoader2.setBasePath("D:\\lib\\");
      Class<?> clazz2 = classLoader2.loadClass("com.itheima.my.A");
      System.out.println(clazz1 == clazz2);
      Thread.currentThread().setContextClassLoader(classLoader1);
      System.out.println(Thread.currentThread().getContextClassLoader());
      System.in.read();
   }
  ```

  重写loadClass方法，删除双亲委派机制，如果是java开头的jar包，就交给原先父类的loadClass，如果是自定义的，就自己直接加载。

  自定义类加载器没指定双亲的话，默认双亲为应用程序类加载器

- 线程上下文类加载器

  JDBC使用DriverManager来管理数据库驱动，DriverManager类位于rt.jar包中，由启动类加载器加载。但我们引入的maven依赖中的jar包中的驱动应由应用类加载器加载，这俩明显违反双亲委派机制。

  - SPI机制，全程Service Provider Interface，是JDK内置的一种服务提供发现的机制

    需要在resources目录下新建META-INF/services目录，并且在这个目录下新建一个与上述接口的全限定名一致的文件java.sql.Driver的接口，在这个文件中写入接口的实现类的全限定名com.mysql.cj.jdbc.Driver。

  - 工作原理：

    - 在ClassPath的路径下的META-INF/services文件夹中以接口的全限定名来命名文件名，对应文件里面写该接口的实现。

    - 使用ServiceLoadder加载实现类

      ![10](C:/Users/93752/Desktop/工作面试/小林coding/04-java虚拟机res/10.png)

    - SPI中的ServiceLoadder中使用线程上下文类加载器进行加载，而这个类加载器一般是应用程序类。

      ```java
      public static <S> ServiceLoader<S> load(Class<S> service){
      	ClassLoader c1=Thread.currentThread().getContextClassLoader();
      	return ServiceLoader.load(service,c1);
      }
      ```

      

## 垃圾回收

### 什么是垃圾回收

Java为了简化对象释放及回收，引入自动垃圾回收机制（Garbage Collection GC）。通过垃圾回收器来对不再使用的对象进行自动回收，垃圾回收器主要负责对堆上的内存进行回收。

### 什么时候触发GC

- 内存不足：堆内存不足触发GC
- 手动触发：使用`System.gc()`，在不同的JVM表现出不同的情况，早期版本立刻触发Full GC，高阶JVM可能会触发Mixed GC或者Full GC，现在`System.gc()`只是建议JVM进行垃圾回收，并不能保证立刻进行。
- 元空间拓展：`-XX:MetaspaceSize`设置元空间初始阈值，到达这个值会触发FullGC，初始20M，后续JVM会自行计算阈值。`-XX:MaxMetaspaceSize`最大元空间值默认256M。
- JVM参数：`-Xms`设置初始堆大小，`-Xmx`设置最大堆内存，`-Xss`设置虚拟机栈大小，默认1M。到达这些阈值都会触发GC

### 判断垃圾回收的方法

- 引用计数法：C++使用引用计数法为每个对象维护一个引用计数器，当对象被引用时加1，取消引用减1。

  **缺点：**

  - 每次引用和取消都要维护计数器，对性能有影响
  - 存在循环引用问题。

- 可达性分析法：Java使用可达性分析法来判断对象是否可以被回收。可达性分析法将对象分为两类：垃圾回收的根对象（GC Root）和普通对象，GC Root不可被回收。

  GC Root对象：

  - 线程Thread对象
  - 系统类加载器（默认应用程序类加载器）所加载的java.lang.Class对象
  - 监视器对象，用来保存同步锁synchronized关键字持有的对象
  - 本地方法调用时使用的全局对象

### 垃圾回收算法的标准

因为垃圾回收通常是由一个单独的GC线程来完成的，所以无论那种GC算法，都会有部分阶段需要停止所有的用户线程。这个过程称之为Stop The Word，简称STW，如果STW时间过长则会影响用户的使用。

标准为：

- 最大吞吐量
- 最大暂停时间STW
- 堆使用效率

### 垃圾回收算法有哪些

- 标记清除算法：
  - 核心思想：
    - 标记阶段：使用可达性分析从GC Root开始通过引用链遍历出所有存活的对象。
    - 清除阶段：从内存中删除没有被标记的所有对象。
  - 优缺点：
    - 优点：实现简单，在一阶段维护一个标志位，二阶段删除即可
    - 缺点：会有大量外部碎片，因为要扫描空闲链表，所以分配速度也慢
- 复制算法：
  - 核心思想：将堆内存分为两块，from和to空间，GC开始阶段将GC Root搬运到to空间，再把GC Root关联的对象搬运到to空间。然后清理from空间，并且把俩名字交换。
  - 优缺点：
    - 优点：吞吐量高，只需要把活得复制到to空间即可；不会有碎片化问题相当于按顺序整理了一遍
    - 缺点：每次只能使用一半的空间来创建对象使用。
- 标记整理算法：
  - 核心思想：相当于对标记清除算法的一种优化
    - 标记阶段：同标记清除一阶段
    - 整理阶段：将存活对象移动到堆的另一端，清理掉存活对象的内存空间
  - 优缺点：
    - 优点：内存使用率高，没有碎片化问题
    - 缺点：整理阶段的性能不高。
- 分代GC。这个我们单独挑出来着重说。

#### 分代垃圾回收算法

- 最初创建出来的对象默认会放在伊甸园区，如果伊甸园区放满，则会触发在新生代的GC回收，成为Minor GC。Minor GC会将伊甸园区和幸存区S0的对象进行回收，没回收的对象会放入幸存区S1。
- 接下来，幸存区S0和幸存区S1会交换名字，S0变为To区，也就是说，下一次触发Minor GC会回收的是伊甸园区和幸存区S1。每次Minor GC都会为对象记录他的年龄，初始为0，发生一次Minor GC年龄加1。
- 如果年龄超过最大阈值（一般为15），那么该对象就会送到老年代中。
- 如果老年代空间也不足，无法放入新的对象时，先尝试新一轮的Minor GC，如果还是不足，则会触发Full GC，Full GC会对整个堆进行垃圾回收，如果依然被不足，则会抛出`Out Of Memory`的异常。

#### 为什么分代GC要把堆分成年轻代和老年代

系统中大部分对象都是创建出来后很快就可以释放的，比如获取到订单信息返回给用户就可以把订单信息回收了。老年代中存放长期存活的对象，比如Spring的大部分Bean对象，启动之后就不会回收了。新生代和老年代的默认比为1：2。

- 可以调整新生代老年代的比例来适应不同的应用程序，提高内存使用率及性能。
- 新生代和老年代可以使用不同的垃圾回收算法，新生代一般是复制算法，老年代可以选择标记清除或标记整理算法，灵活度高。
- 分代设计可以只回收新生代也就是触发Minor GC，减少对整个堆的Full GC次数，减少STW时间。

### 垃圾回收器有哪些

垃圾回收器是垃圾回收算法的具体实现。由于垃圾回收器分为年轻代和老年代，除了G1外的其他垃圾回收器必须成对组合进行使用。

![11](C:/Users/93752/Desktop/工作面试/小林coding/04-java虚拟机res/11.png)

- Serial---Serial Old：

  - 新生代：Serial采用复制算法。

  - 老年代：Serial Old采用**标记整理**算法，均是单线程串行回收。
    - 两个垃圾回收器类似优缺点也一样
    - 优点：单CPU处理下吞吐量优秀
    - 缺点：多CPU下性能差，STW时间长
  - 适用场景：客户端程序或硬件配置有限的情况。

- ParNew---CMS：

  - 新生代：ParNew是针对Serial在多CPU下的优化版本，依旧采用复制算法，使用多线程进行垃圾回收。
    - 优点：多CPU下停顿时间短
    - 缺点：吞吐量和停顿时间不如G1，JDK9之后不建议使用

  - 老年代：CMS（Concurrent Mask Sweep）采用**标记清除**算法，可以进行并发标记，用户线程和垃圾回收线程在某些步骤可以同时执行，减少用户线程等待时间。
    - 优点：停顿时间短，用户体验好。
    - 缺点：
      - 内存碎片问题：因为采用标记清除算法，会产生大量内存碎片，CMS会在Full GC进行整理，会导致用户线程暂停
      - 浮动垃圾问题：无法处理并发清理过程的“浮动垃圾”；
      - 退化问题：如果老年代内存不足，会退化为Serial Old。
  - 使用场景：大型互联网系统中用户请求数据量大，频率高的场景，比如订单接口，商品接口。

- Parallel Scavenge---Parallel Old
  - 新生代：Parallel Scavenge采用复制算法，可以多线程并行回收，具备自动调整堆内存的能力。**JDK8默认**。
    - 优点：吞吐量高，手动可控，并且会动态调整堆参数。
    - 缺点：不能保证单次停顿时间。
  - 老年代：Parallel Old采用**标记整理**算法
    - 优点：可以并发收集，多核CPU效率高
    - 缺点：整理算法固有毛病暂停时间较长。
  - 使用场景：后台任务，不需要交互的，如大数据处理，大文件导出。
- **JDK9之后强烈要求使用的G1垃圾回收器**。我们单独跳出来说。‘

### G1垃圾回收器

JDK9之后的垃圾回收期默认是G1。

- 新生代：采用复制算法。
- 老年代：采用标记整理算法。

具有以下优点：

- 支持巨大的堆空间回收，有较高的吞吐量
- 支持多CPU并行回收
- 允许用户据设置最大暂停时间

#### G1垃圾回收策略

G1之前的垃圾回收器内存结构一般都是连续的。类似于我们上一个图。但是G1垃圾回收器将整个堆分为多个大小相等的区域，成为Region，区域不要求是连续的，还是分为三类：伊甸园区、幸存者区、老年代。一般Region的个数为2048个，大小为堆内存/2048。

![3](C:/Users/93752/Desktop/工作面试/小林coding/04-java虚拟机res/3.png)

- 新创建的对象会放在伊甸园区，当G1判断年轻代不足时，默认为max的60%，则会触发Young GC，标记出伊甸园区和幸存区存活的对象，在最大暂停时间内选择这些对象放入新的幸存区并年龄加一。

- G1会在YoungGC的过程中记录每次垃圾回收时每个伊甸园区和幸存区的平均耗时，作为下次回收的参考依据，这样就能根据最大暂停时间计算出本次能回收多少个Region区域了。比如最大暂停时间200ms，平均耗时40ms，这样每次回收回收4个Region。

- 后序也是一样，未回收的对象不断地在幸存区里转移，直到年龄超过15，将被放进老年代。

- 如果要放进老年代的对象的大小超过Region的一半，会被直接放入老年代，这种直接放进老年代的叫Humongous区，比如堆内存是4G，除2048得到一个Region大小是2M，如果一个对象超过了1M就会被直接放入Humongous区，并且如果这个对象过大，允许该对象横跨多个Humongous区。

  ![4](C:/Users/93752/Desktop/工作面试/小林coding/04-java虚拟机res/4.png)

- 多次回收后，会出现很多的Old区，如果总堆的占有率超过默认45%，就会触发混合回收MixedGC，回收所有年轻代和部分老年代的对象及大对象区。G1对老年代的清理会选择存活度最低的区域来进行回收，这样可以保证回收效率最高，清理阶段使用复制算法，不会产生内存碎片。

  ![5](C:/Users/93752/Desktop/工作面试/小林coding/04-java虚拟机res/5.png)

- 若清理过程中，没有足够的的空Region存放转移的对象，会发生FullGC，单线程执行标记-整理算法，会导致用户线程的暂停，所以尽量保证使用的堆内存有一定的多余空间。

### minorGC、majorGC、fullGC的区别，什么场景触发full GC

- minorGC是针对年轻代的回收，发生的非常频繁。
- majorGC是针对老年代的回收，发生的频率较低。
- fullGC是针对整个堆内存及**元空间**进行回收。
  - 触发条件：
    - 调用`System.gc()`后，JVM会尝试进行FullGC。
    - 当新生代存活对象无法放入老年代或老年代不足会进行FullGC。
    - 元空间不足也会发生FullGC。

## GC调优

### 核心流程

使用jmap及mat分析内存快照

- 监控是否出现连续的Full GC或者单次GC时间过长
- 诊断并解决：
  - 优化基础JVM参数。
    - `-Xms`设置初始堆大小，`-Xmx`设置最大堆内存，建议初始堆大小和最大堆大小一致。
    - `-Xss`设置虚拟机栈大小，默认1M，建议256K。
    - `-XX:MetaspaceSize`设置元空间初始阈值。初始20M，`-XX:MaxMetaspaceSize`最大元空间值默认256M。建议都设置256M。
  - 减少对象的产生：防止超大对象的复制等等
  - 更换垃圾回收器
  - 优化垃圾回收器参数
- 在测试环境验证问题是否解决再上线

# Spring面试篇

## Spring

### Spring框架的核心特性

- **IOC（控制反转，Inversion of Control）**

  IOC 是 Spring 最核心的思想，它的作用是将对象的创建和管理交给 Spring 容器，而不是由应用程序代码自行控制。这通过依赖注入（DI，Dependency Injection）来实现。

  - 核心机制：
    - Bean管理：Spring负责创建、初始化、管理Bean对象。
    - 依赖注入（DI）：Spring 通过构造函数、Setter 方法、字段注入方式管理对象的依赖关系。
    - 降低耦合：组件之间不直接依赖，而是通过Spring容器注入。

- **AOP（面向切面编程，Aspect-Oriented Programming）**

  AOP 允许我们在不修改业务逻辑的情况下添加额外功能，如日志记录、事务管理、安全控制等。

  - 核心机制：
    - 横切关注点（如日志、事务、安全控制），不需要修改业务代码即可添加功能。
    - 通过动态代理（JDK 动态代理或 CGLIB）拦截方法调用，在执行前后添加逻辑。

- **事务管理（Transaction Management）**

  Spring 通过声明式事务（`@Transactional`）和编程式事务提供强大的事务管理能力，支持多种事务管理器（JDBC、JPA、Hibernate、MyBatis等）。

- **Spring MVC（Web 框架）**

  Spring MVC 是一个基于 Servlet 的轻量级 Web 框架，提供了前后端分离、RESTful API、视图解析等功能。

  - 核心机制：
    - 采用前端控制器模式（DispatcherServlet）处理请求
    - 通过 `@Controller`、`@RequestMapping` 构建 RESTful API。

### 介绍一下IOC

Spring IOC 通过反射+工厂模式实现对象创建、管理和依赖注入。

其核心流程为**启动-->扫描-->依赖注入-->初始化-->后置处理器**

- 启动（IOC 容器启动）

  Spring IOC 容器的启动和创建主要由 `ApplicationContext` 负责，常见的启动方式分为根据XML配置`ClassPathXmlApplicationContext`或Java配置`AnnotationConfigApplicationContext`。

  ```java
  ApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class);
  ```

- 扫描（ComponentScan 组件扫描）

  Spring 通过 `@ComponentScan` 或XML 配置扫描指定的包，找到所有的 `@Component`、`@Service`、`@Repository`、`@Controller` 注解的类，并注册成BeanDefinition。

  - 扫描指定包路径（`ClassPathBeanDefinitionScanner`）。
  - 解析 `@Component` 注解的类，生成 `BeanDefinition`（但此时 Bean 还未实例化）。
  - 注册 `BeanDefinition` 到 `BeanFactory`，等待后续实例化。

- 依赖注入（Dependency Injection, DI）

  Spring 在实例化 Bean 时，会自动注入它的依赖对象，主要有三种方式：构造注入、Setter方法注入、字段注入。通常使用字段注入。

  - 依赖注入流程：
    - 获取 `BeanDefinition`，确定要创建的 Bean。
    - 实例化 Bean（反射创建对象）。
    - 解析 `@Autowired`，查找依赖对象：先从单例池找，单例池没有就递归创建依赖对象。**（循环依赖会在这里被解决）**
    - 完成依赖注入（调用构造方法或 setter 方法）。

- 初始化（InitializingBean 和 @PostConstruct）

  Bean实例化完成并完成依赖注入后，Spring 会执行 Bean 的初始化逻辑。先执行`@PostConstruct`注解的方法再执行重写`afterPropertiesSet()`方法。

- 后置处理器（BeanPostProcessor）

  Spring 提供 `BeanPostProcessor` 在 Bean 初始化前后进行增强，比如 AOP、事务管理等都依赖它。

  - Bean 创建完成后，调用 `postProcessBeforeInitialization()`（初始化前增强）。
  - 执行Bean的初始化方法，如`@PostConstruct`注解的方法再执行重写`afterPropertiesSet()`方法。
  - Bean 初始化完成后，调用 `postProcessAfterInitialization()`（初始化后增强）。
  - 如果有 AOP 代理，此时会生成代理对象，并返回代理对象，而不是原始 Bean。

### 介绍一下AOP

AOP 允许我们在不修改业务逻辑的情况下，添加额外功能，如日志记录、事务管理、安全控制等。

#### AOP核心概念

- **切面（Aspect）**：封装一组增强逻辑（例如日志、事务），是 AOP 的核心。

- **切点（Pointcut）**：定义哪些方法需要被增强。

- **通知（Advice）**：定义增强逻辑的具体执行时机，如 前置通知（Before）、后置通知（After）、环绕通知（Around）等。

- **代理对象（Proxy）**：Spring AOP 通过 JDK 动态代理或 CGLIB 代理 生成代理对象，并在代理对象的方法调用前后插入增强逻辑。

- **目标对象（Target）**：实际被代理的业务对象。

#### 静态代理和动态代理

- **静态代理**：适用于代理对象不多、代理关系在编译时就能确定的简单场景。
  - 代理类与目标类之间有直接的关系，需要在编译时就确定。
  - 每个类都需要一个代理类，无法在运行时动态生成。
- **动态代理**：适用于代理对象较多，且在运行时需要动态生成代理类的复杂场景，如 AOP、事务管理、缓存等。

#### AOP代理方式

Spring AOP基于代理对象实现，而代理对象的生成方式取决于目标对象是否实现了接口：

- JDK动态代理（基于接口）：适用于目标类实现了接口的情况。
  - 使用 `java.lang.reflect.Proxy` 生成代理对象。
  - 要求目标对象必须实现接口。
  - 代理对象会实现相同的接口，并在方法调用时拦截增强逻辑。
- CGLIB动态代理（基于子类继承）：适用于目标类没有实现接口的情况。
  - 使用 CGLIB（Code Generation Library）生成代理对象。
  - 适用于目标对象没有实现接口的情况。
  - 通过创建目标类的子类，并在方法调用时插入 AOP 逻辑。

#### AOP执行流程

当我们从 Spring 容器中获取一个 Bean 时，实际返回的是一个代理对象。代理对象会在方法调用的开始、执行过程和结束时，插入相应的回调函数进行增强。

- Spring容器启动，读取所有切面配置中的切入点
- 初始化Bean，判定bean对应的类中的方法是否匹配到任意的切入点
  - 匹配失败，创建原始对象
  - 匹配成功，创建原始对象的代理对象
- 获取bean执行方法
  - 获取的bean如果是原始对象，调用方法并执行，完成操作
  - 获取的bean如果是代理对象，根据代理对象的运行模式运行原始方法与增强的内容，完成操作。

### 什么是循环依赖

- 构造方法循环依赖（Spring 无法解决）

  当两个 Bean 通过构造方法互相依赖时，Spring 无法解决，会抛出 `BeanCurrentlyInCreationException`。

  ```java
  @Component
  public class A {
      private final B b;
      public A(B b) { this.b = b; }
  }
  @Component
  public class B {
      private final A a;
      public B(A a) { this.a = a; }
  }
  ```

- Setter / 字段注入循环依赖（Spring 通过三级缓存解决）

  如果 `A` 和 `B` 之间是 setter 方法 或 字段注入，Spring可以解决

  ```java
  @Component
  public class A {
      @Autowired private B b;
  }
  @Component
  public class B {
      @Autowired private A a;
  }
  ```

### Spring是如何解决循环依赖的？

Spring 采用"三级缓存"机制来解决Setter / 字段注入形式的循环依赖。

| 三级缓存                              | 作用                                         |
| ------------------------------------- | -------------------------------------------- |
| **一级缓存（singletonObjects）**      | 存放完全初始化好的单例 Bean（可直接使用）    |
| **二级缓存（earlySingletonObjects）** | 存放提前曝光的 Bean 实例（未初始化完成）     |
| **三级缓存（singletonFactories）**    | 存放可以创建 Bean 的工厂（用于代理对象创建） |

解决的过程：

- 创建Bean实例（但不初始化）

  - 通过反射创建Bean的原始实例，但未进行依赖注入
  - 将创建Bean的工厂存入三级缓存

- 提前暴露Bean（放入二级缓存）

  - Spring允许其他Bean对象访问这个还未初始化的对象，从而解决循环依赖的问题。

- 依赖注入

  - A需要B，Spring发现B正在创建中，就会尝试从缓存获取，一级缓存找不到找二级缓存。在二级缓存找到提前暴露的B，就可以直接使用。如果B需要A，但A也提前暴露了，所以B也能拿到A。

- 完成初始化，存入一级缓存。

  A和B完成初始化，分别从二级缓存移除，存入一级缓存。

### Spring框架中都用到了那些设计模式

- 工厂设计模式：Spring使用工厂模式通过BeanFactory、ApplicationContext 创建 bean 对象。
- 代理设计模式：AOP
- 单例设计模式：单例池及单例池里的Bean对象。
- 包装器设计模式：针对不同的数据库可以动态的切换数据源
- 观察者模式：Spring 中“订阅-通知”的事件驱动模型就是观察者模式很经典的一个应用。

### Spring常用注解

- IOC中：
  - `@Component`：标记一个类为 Spring 容器的 Bean。
  - `@Autowired`：自动装配 Bean（依赖注入），Spring 会自动为该字段、构造器或方法注入匹配的 Bean。
- AOP中：
  - `@Transactional`：声明事务管理。标记方法或类，支持事务的自动管理。

- MVC中：
  - `@Controller`, `@RequestMapping`, `@GetMapping`等

### Spring的事务什么情况下会失效？

- 未捕获异常：如果一个事务方法中发生了未捕获的异常，并且异常未被处理或传播到事务边界之外，那么事务会失效，所有的数据库操作会回滚。
- 事务传播属性设置不当：非事务方法调用事务方法会导致事务失效。
- 多数据源的事务管理
- 事务在非公开方法中失效：`@Transactional`如果标注在私有方法和非public方法上，事务也会失效。

## SpringMVC

### MVC分层

MVC全名是Model View Controller，是模型(model)－视图(view)－控制器(controller)的缩写，一种软件设计典范。

- **视图(view)：** 为用户提供使用界面，与用户直接进行交互。
- **模型(model)：** 代表一个存取数据的对象或 JAVA POJO。分为两类
  - 数据承载类Bean：实体类
  - 业务处理类Bean：Service或Dao对象，专门处理用户提交请求的。
- **控制器(controller)：**用于将用户请求转发给相应的 Model 进行处理，并根据 Model 的计算结果向用户提供相应响应。它使视图与模型分离。

### SpringMVC的工作流程

SpringMVC 是基于 前端控制器模式（Front Controller Pattern）的 Web 框架，它的工作流程是通过一系列的步骤来完成请求的处理和响应的返回。

流程：

- 用户通过view页面向服务端提出请求，包括表单请求、超链接请求、ajax请求等等。
- 请求到达 DispatcherServlet（前端控制器）
  - `DispatcherServlet` 是 SpringMVC 的核心控制器，负责接收所有请求，并进行分发。每个请求都会经过 `DispatcherServlet`。
  - 该控制器从 web.xml 或 Spring 配置文件中获取映射信息，知道如何分发请求。
- 请求解析和处理及选择处理器
  - `DispatcherServlet` 将请求传递给 HandlerMapping，它根据请求的 URL 查找与之匹配的处理器（Controller）。
  - `HandlerMapping`：根据请求的 URL 和配置的映射规则（比如 `@RequestMapping` 注解）找到对应的处理器（即 Controller 中的方法）。
  - `DispatcherServlet` 将请求传递给处理器方法。
- 执行控制器方法（Handler）
  - 控制器方法开始执行，处理客户端请求。控制器方法可以根据业务逻辑获取数据、处理请求、并返回 Model 数据。

- Controller再将处理结果向客户端发回响应View页面，渲染后发送给客户端。

### Handlermapping 和 handleradapter有了解吗？

- **HandlerMapping**：负责根据请求的 URL 找到匹配的Controller方法。

- **HandlerAdapter**：负责适配不同类型的 Handler，调用对应的处理方法来执行请求。

具体来说

```java
@Controller
public class MyController {
    @RequestMapping("/test")
    public String handleRequest() {
        // 处理请求的业务逻辑
        return "viewName";  // 返回视图名
    }
}
```

**`HandlerMapping`**：

- 负责根据请求的 URL 找到对应的 **Handler**（即 `MyController` 中的 `handleRequest()` 方法）。
- 当用户访问 `/test` 路径时，`HandlerMapping` 会查找配置的请求映射，发现 `/test` 映射到 `MyController` 类中的 `handleRequest()` 方法。
- `HandlerMapping` 返回一个 **Handler**（即方法 `handleRequest()`）和相关的拦截器（如果有的话）。

**`HandlerAdapter`**：

- 负责执行 **Handler**（即 `handleRequest()` 方法）。
- `HandlerAdapter` 是根据 `Handler` 的类型选择合适的执行方式。`HandlerAdapter` 会调用 `handleRequest()` 方法来处理请求。

## SpringBoot

### 为什么使用springboot

- **自动配置（Auto Configuration）**
  - Spring：在Spring，员需要手动配置大量的 XML 配置文件，或者使用 Java 配置类进行大量的配置。
  - Spring Boot：通过自动配置，Spring Boot 会根据项目的依赖和环境自动配置相关组件，减少了大量的配置工作。例如，Spring Boot 可以自动配置数据库连接池、嵌入式服务器（如 Tomcat）、JPA 等，开发者不需要显式地定义它们。

- **内嵌Web服务器**

  - Spring：需要在外部部署 Web 服务器（如 Tomcat）并将应用打包成 WAR 文件进行部署。

  - Spring Boot：Spring Boot 默认支持嵌入式的 Web 服务器，可以将 Spring Boot 应用打包为一个 可执行 JAR 文件，不依赖外部服务器。这样应用可以直接运行，减少了配置和部署复杂性。

- **快速开发与启动（Starter POMs）**

  - Spring：在传统的 Spring 中，开发者需要自己管理所有依赖，可能会遇到版本冲突、依赖缺失等问题。
  - Spring Boot：通过 `starter` 模块，Spring Boot 提供了一些预先配置好的常用依赖（例如 `spring-boot-starter-web`、`spring-boot-starter-data-jpa` 等）。

### SpringBoot自动装配原理

Springboot的启动类都会有`@SpringBootApplication`这个注解，这个注解包含：

- @SpringBootConfiguration

  - 这个注解表示该类是 Spring Boot 应用的配置类，功能类似于传统 Spring 中的 `@Configuration` 注解，表示该类可以包含 Bean 定义。

- @EnableAutoConfiguration

  - 这个注解告诉 Spring Boot 启动自动配置功能。它会根据项目的类路径、配置文件等信息，自动配置应用程序所需的组件和服务。

- @ComponentScan

  - 这个注解让 Spring Boot 扫描当前包及其子包中的所有组件（如 `@Component`、`@Service`、`@Repository`、`@Controller` 等），并将它们自动注册到 Spring 容器中。默认会扫描 `@SpringBootApplication` 注解所在类的包及其子包。

  **自动配置原理：**

  - `SpringApplication` 启动：应用启动时，`SpringApplication.run()` 方法会启动 Spring 上下文。
  - `@EnableAutoConfiguration` 触发自动配置：`@EnableAutoConfiguration` 会通过 `spring.factories` 加载自动配置类。
  - 条件判断：自动配置类会使用 `@Conditional` 系列注解来判断是否需要进行配置。
  - Bean 注册：符合条件的自动配置类会向 Spring 容器注册相关的 Bean。

### 说几个启动器（starter)？

- **spring-boot-starter-web**：含了Spring MVC和Tomcat嵌入式服务器
- **spring-boot-starter-test**：用于单元测试和集成测试，包含常用的测试框架和工具
- **mybatis-spring-boot-starter**：由MyBatis团队提供的，用于简化在Spring Boot应用中集成MyBatis的过程。
- **spring-boot-starter-jdbc**：可以让你轻松地与MySQL数据库进行交互。
- **spring-boot-starter-data-redis**：用于集成Redis缓存和数据存储服务。

### 自己实现过SpringBoot demo吗？

- **模拟依赖**

  为了实现SpringBoot starter，因为springboot starter自带mvc和tomcat，所以我们创建自己的SpringBoot starter第一步也要先引入一下依赖，包括springmvc、web、tomcat等

  ```xml
  <dependencies>
  	<dependency>
              <groupId>org.springframework</groupId>
              <artifactId>spring-context</artifactId>
              <version>5.3.18</version>
          </dependency>
          <dependency>
              <groupId>org.springframework</groupId>
              <artifactId>spring-web</artifactId>
              <version>5.3.18</version>
          </dependency>
          <dependency>
              <groupId>org.springframework</groupId>
              <artifactId>spring-webmvc</artifactId>
              <version>5.3.18</version>
          </dependency>
  
          <dependency>
              <groupId>javax.servlet</groupId>
              <artifactId>javax.servlet-api</artifactId>
              <version>4.0.1</version>
          </dependency>
  
          <dependency>
              <groupId>org.apache.tomcat.embed</groupId>
              <artifactId>tomcat-embed-core</artifactId>
              <version>9.0.60</version>
          </dependency>
  </dependencies>
  ```

- **模拟 `@SpringBootApplication` 注解**

  Spring Boot 中的 `@SpringBootApplication` 是一个组合注解，其中包含 `@Configuration`、`@ComponentScan` 和 `@EnableAutoConfiguration`。我们可以模拟它，创建一个自定义注解 `@SJBSpringBootApplication`，其功能类似。

  ```java
  @Target(ElementType.TYPE)
  @Retention(RetentionPolicy.RUNTIME)
  @Configuration
  @ComponentScan
  public @interface SJBSpringBootApplication {
  }
  ```

- **模拟 `SpringApplication.run()`**

  Spring Boot 的启动是通过 `SpringApplication.run()` 方法来完成的。在模拟的实现中，创建了一个 `SJBSpringApplication` 类，模仿 `SpringApplication`，并实现了 `run()` 方法。

  ```java
  public class SJBSpringApplication {
      public static void run(Class<?> primarySource, String... args) {
          // 创建并刷新 Spring 容器
          AnnotationConfigWebApplicationContext applicationContext = new AnnotationConfigWebApplicationContext();
          applicationContext.register(primarySource);
          applicationContext.refresh();
  
          // 启动 Tomcat
          startTomcat(applicationContext);
      }
  }
  ```

- **模拟启动内嵌Tomcat**

  内嵌 Tomcat 是 Spring Boot 的一个核心特性

  ```java
  public static void startTomcat(WebApplicationContext applicationContext) {
      Tomcat tomcat = new Tomcat();
      Server server = tomcat.getServer();
      Service service = server.findService("Tomcat");
  
      Connector connector = new Connector();
      connector.setPort(8081);
  
      Engine engine = new StandardEngine();
      engine.setDefaultHost("localhost");
  
      Host host = new StandardHost();
      host.setName("localhost");
  
      Context context = new StandardContext();
      context.setPath("");
      context.addLifecycleListener(new Tomcat.FixContextListener());
  
      host.addChild(context);
      engine.addChild(host);
      service.setContainer(engine);
      service.addConnector(connector);
  
      tomcat.addServlet("", "dispatcher", new DispatcherServlet(applicationContext));
      context.addServletMappingDecoded("/*", "dispatcher");
  
      try {
          tomcat.start();
      } catch (LifecycleException e) {
          e.printStackTrace();
      }
  }
  ```

  这里通过 `Tomcat` 类实例化并配置了一个内嵌的 Tomcat 服务器：

  - **配置端口**：将 Tomcat 端口设置为 8081。
  - **绑定 `DispatcherServlet`**：将 `DispatcherServlet` 添加到 Tomcat，并映射到 `"/*"` 路径，处理所有的请求。
  - **启动 Tomcat**：通过 `tomcat.start()` 启动服务器。

- **启动整个应用**

  在 `UserApplication` 类中，使用 `@SJBSpringBootApplication` 注解来标记启动类，并通过 `SJBSpringApplication.run()` 启动应用。

  ```java
  @SJBSpringBootApplication
  public class UserApplication {
      public static void main(String[] args) {
          SJBSpringApplication.run(UserApplication.class, args);
      }
  }
  ```

  `UserApplication` 类作为配置类，使用 `@ComponentScan` 自动扫描并加载 `UserController`。

- **定义业务逻辑**

  `UserController` 是一个典型的 Spring MVC Controller，它处理 HTTP 请求并返回响应。使用 `@RestController` 注解使得每个方法都直接返回 JSON 格式的响应。

  ```java
  @RestController
  public class UserController {
      @GetMapping("/test")
      public String test() {
          return "sjb123";
      }
  }
  ```

  `/test` 路径的请求将由 `UserController` 的 `test()` 方法处理，返回 `"sjb123"`。

### Springboot中的注解

- **@SpringBootApplication**：用于标注主应用程序类，标识一个Spring Boot应用程序的入口点，同时启用自动配置和组件扫描。**默认情况下，Spring 会扫描启动类所在包及其子包中的所有组件。**
- **@Controller**：标识控制器类，处理HTTP请求。
- **@RestController**：结合@Controller和@ResponseBody，返回RESTful风格的数据。
- **@Service**：标识服务类，通常用于标记业务逻辑层。
- **@Component**：通用的Spring组件注解，表示一个受Spring管理的组件。
- **@Autowired**：用于自动装配Spring Bean。
- **@RequestMapping**：用于映射HTTP请求路径到Controller的处理方法。
- **@Configuration**：用于指定一个类为配置类，其中定义的bean会被Spring容器管理。通常与@Bean配合使用，@Bean用于声明一个Bean实例，由Spring容器进行管理。
- **@GetMapping、@PostMapping、@PutMapping、@DeleteMapping：**简化@RequestMapping的GET、POST、PUT和DELETE请求。

## MyBatis

### 与传统的JDBC相比，MyBatis的优点？

- 代码简洁
  - JDBC要写大量sql语句，容易出错。
  - mybatis只要编写SQL映射文件（XML或者注解）。可以直接通过Mapper直接调用。提高开发效率。

- SQL与代码解耦
  - JDBC与代码捆绑，修改sql要改代码，难以维护。
  - mybatis：SQL 语句可以放在XML 配置文件或注解中。
- 提供 ORM（对象关系映射）支持、
  - JDBC需要手动将 `ResultSet` 转换为 Java 对象，代码冗长。
  - mybatis：可以自动映射查询结果到 Java 对象，减少手动转换的工作。
- 更灵活的SQL
  - JDBC的sql语句无法动态修改
  - mybatis：支持 动态 SQL（`<if>、<choose>、<foreach>` 等标签），可以根据条件动态生成 SQL。

- 安全性更高
  - JDBC的sql语句容易被SQL注入
  - MyBatis：使用 `###{}` 绑定参数，防止 SQL 注入

### JDBC连接数据库的步骤

- 加载数据库驱动程序，使用 `Class.forName()` 加载数据库驱动。
- 建立数据库连接，使用 `DriverManager.getConnection()` 获取数据库连接。
- 创建Statement或PreparedStatement对象，用于执行 SQL 查询或更新操作。
- 执行sql语句
  - `executeQuery()`（查询）
  - `executeUpdate()`（增、删、改）
- 处理查询结果`ResultSet`
- 关闭数据库资源（`ResultSet` → `Statement` → `Connection`）

### 如果项目中要用到原生的mybatis去查询，该怎样写？

- 配置mybatis
- 创建实体类
- 编写sql映射文件
- 编写dao接口
- 在xml文件中编写具体的sql查询语句
- 在dao层调用查询方法

### Mybatis里的 ### 和 $ 的区别？

- mybatis中在处理###{}会将其替换为？，之后调用PreparedStatement 的 set 方法来赋值，执行效率更高，并且防止sql注入，提高安全性。
- 在处理${}只是简单的参数拼接，不能防止sql注入，会导致安全问题。

### MybatisPlus和Mybatis的区别？

MybatisPlus是一个基于MyBatis的增强工具库，旨在简化开发并提高效率。

| **对比项**        | **MyBatis**                           | **MyBatis-Plus (MP)**                    |
| ----------------- | ------------------------------------- | ---------------------------------------- |
| **开发效率**      | 需要手写 SQL 语句                     | 内置 CRUD 方法，大大减少 SQL 书写        |
| **代码量**        | 需要编写 `Mapper.xml` 和 `Mapper接口` | 只需继承 `BaseMapper<T>`，省去大量 `xml` |
| **SQL 复杂度**    | 适合复杂 SQL，自定义灵活              | 适合简单增删改查，复杂 SQL 仍需自定义    |
| **分页**          | 需要手写 SQL 或自己封装               | **内置分页插件**，支持 `Page<T>`         |
| **SQL 语句**      | 需要手动写 SQL，控制 SQL 执行         | **内置 CRUD 方法**，默认 SQL 生成        |
| **Lambda 表达式** | 需要自己拼接 SQL                      | **提供 LambdaQueryWrapper**，代码更优雅  |
| **事务管理**      | 依赖 Spring 事务                      | 依赖 Spring 事务                         |
| **动态 SQL**      | 通过 `if`、`choose` 等 XML 方式实现   | 通过 `Wrapper` 方式更简洁                |
| **适用场景**      | 适用于复杂 SQL、自定义查询            | 适用于**CRUD 快速开发**，减少 SQL 工作量 |

## SpringCloud

### 了解SpringCloud吗，说一下他和SpringBoot的区别

Spring Boot是用于构建单个Spring应用的框架，而Spring Cloud则是用于构建分布式系统中的微服务架构的工具，Spring Cloud提供了服务注册与发现、负载均衡、断路器、网关等功能。

两者可以结合使用，通过Spring Boot构建微服务应用，然后用Spring Cloud来实现微服务架构中的各种功能。

### 用过那些微服务组件？

- **Nacos**：注册中心 & 配置中心，负责服务注册与发现，以及动态配置管理。

- **Feign**：基于 HTTP 的声明式 RPC 客户端，简化微服务之间的调用。

- **Spring Cloud Gateway**：API 网关，负责路由、负载均衡、权限控制等功能。

- **Sentinel**：流量控制 & 熔断限流，提供高可用保护，防止雪崩效应。

### 负载均衡算法

Nginx 负载均衡算法主要用于将流量分发到多个后端服务器，提高系统的吞吐量和可用性。

- **轮询（默认）**

  - 优点：简单
  - 缺点：无法动态感知服务器的实时负载情况。

- **加权轮询**：为每台服务器分配权重

  - 使用场景：性能不同的服务器，分配不同的流量
  - 优点：适用于不同性能的服务器，可以让高性能服务器处理更多请求。
  - 缺点：无法动态感知服务器的实时负载情况。

- **IP哈希**：根据客户端 IP 地址计算哈希值，每个 IP 地址始终访问同一台服务器。

  - 使用场景：需要会话保持的应用，如购物车、支付系统等。
  - 优点：确保相同的用户请求始终被路由到相同的服务器，适用于状态依赖型应用。
  - 缺点：服务器变更会影响映射，如果一台服务器下线，原本映射到该服务器的请求不会自动转移。可能会导致流量不均衡，如果某些 IP 地址的请求特别多，会给对应服务器造成过载。

- **最少连接**：Nginx 将请求转发给当前**活跃连接数最少**的服务器，以保证负载均衡。

  - 使用场景：服务器处理能力不同，或请求处理时间长短不一的情况下。
  - 优点：适用于长连接、慢请求的场景，如 WebSocket、数据库连接池等。
  - 缺点：需要 Nginx 维护服务器连接状态，有一定计算开销。

- **健康检查**：除了负载均衡策略，Nginx 还可以通过 `max_fails` 和 `fail_timeout` 设置服务器的健康检查和故障转移。

  - 优点：服务器异常时，自动切换到健康的服务器，提高系统可用性。
  - 缺点：需要配合 `proxy_next_upstream` 进一步优化容错机制。


### SpringCloudAlibaba主要包括那些

- **Nacos**（服务注册与配置管理）
- **RocketMQ**（消息队列）
- **Sentinel**（流量控制与熔断降级）
- **Dubbo**（高性能 RPC 框架）
- **Seata**（分布式事务）

# MySQL面试篇

##  SQL基础

### NOSQL和SQL的区别？

| 特性     | SQL                        | NoSQL                              |
| -------- | -------------------------- | ---------------------------------- |
| 数据模型 | 关系型模型（表）           | 键值对、文档、列族、图等           |
| 扩展性   | 垂直扩展（不易横向扩展）   | 水平扩展（容易扩展）               |
| 一致性   | ACID（事务支持）           | 最终一致性（CAP 定理）             |
| 查询语言 | SQL（结构化查询语言）      | 根据数据库不同有不同查询方式       |
| 数据结构 | 固定模式（表结构）         | 灵活（JSON、XML、BSON等）          |
| 事务支持 | 强事务支持                 | 限制事务支持（大多数为最终一致性） |
| 使用场景 | 适合关系型数据，事务要求高 | 高并发、大数据量、非关系型数据     |

### NOSQL和SQL的代表数据库

- SQL代表数据库
  - MySQL
  - Oracle Database
  - Microsoft SQL Server
- NOSQL代表数据库
  - MongoDB：基于文档的数据库，使用 BSON 格式存储数据。非常适合存储半结构化和非结构化数据。
  - Redis：基于内存的键值存储数据库，支持高性能读写，广泛用于缓存和实时数据处理。
  - HBase：基于列族的分布式 NoSQL 数据库，运行在 Hadoop 生态系统中，适合处理海量数据。

### 数据库范式是什么

- **1NF第一范式**：关系中的每一个分量必须是不可分的数据项。

  ![1](C:/Users/93752/Desktop/工作面试/小林coding/06-MySQLres/1.png)

  家庭信息和学校信息可以再分为（家庭信息，户籍）和（学历，所在年级），这样才能符合1NF

  ![2](C:/Users/93752/Desktop/工作面试/小林coding/06-MySQLres/2.png)

- **2NF第二范式：**在1NF的基础上，每一个非主属性完全函数依赖于任何一个候选码，也就是说第二范式是消除了非主属性对码的部分依赖。主要针对**联合主键**。

  ![3](C:/Users/93752/Desktop/工作面试/小林coding/06-MySQLres/3.png)

​	这里面一个订单号里可能有多个产品号，而一个产品号可能对应多个订单号，那么这俩就是联合主键。

​	产品数量、产品折扣、产品价格都与联合主键有关，而订单金额和订单时间只与订单号有关。不符合2NF范式，所以需要拆成两张表。

![4](C:/Users/93752/Desktop/工作面试/小林coding/06-MySQLres/4.png)

![5](C:/Users/93752/Desktop/工作面试/小林coding/06-MySQLres/5.png)

- **3NF第三范式**：每一个非主属性既不传递依赖于码，也不部分依赖于码，第三范式消除了非主属性对码的传递依赖。

  ![6](C:/Users/93752/Desktop/工作面试/小林coding/06-MySQLres/6.png)

​	以上满足第二范式，明显这里面的主键也就是码是学号，但班主任性别和年龄只跟班主任姓名有关，所以为了消除这些非主属性通过班主任姓名间接对学号的传递依赖，需要拆成两张表。

![7](C:/Users/93752/Desktop/工作面试/小林coding/06-MySQLres/7.png)

![8](C:/Users/93752/Desktop/工作面试/小林coding/06-MySQLres/8.png)

这样就消除班主任性别和年龄这些非主属性通过班主任姓名间接对学号的传递依赖，从而符合3NF。

### MySQL连表查询

- 内连接（INNER JOIN）：`INNER JOIN` 返回两个表中满足连接条件的记录。如果没有满足条件的记录，查询结果将为空。

  - 查询学生信息和他们所在班级的名称：

    ```mysql
    SELECT students.name, classes.class_name
    FROM students
    INNER JOIN classes
    ON students.class_id = classes.id;
    ```

- 左连接（LEFT JOIN）：`LEFT JOIN` 返回左表中的所有记录，即使右表中没有匹配的记录。如果右表没有匹配的记录，结果中右表的列将为 `NULL`。

  - 查询所有学生的姓名和他们所在班级的名称，如果学生没有班级，则班级名称为 `NULL`：

    ```mysql
    SELECT students.name, classes.class_name
    FROM students
    LEFT JOIN classes
    ON students.class_id = classes.id;
    ```

- 右连接（RIGHT JOIN）：`RIGHT JOIN` 返回右表中的所有记录，即使左表中没有匹配的记录。如果左表没有匹配的记录，结果中左表的列将为 `NULL`。

  - 查询所有班级的名称和属于该班级的学生姓名，如果班级没有学生，则学生姓名为 `NULL`：

    ```mysql
    SELECT students.name, classes.class_name
    FROM students
    RIGHT JOIN classes
    ON students.class_id = classes.id;
    ```

- 全连接（FULL JOIN）：MySQL 不直接支持 `FULL JOIN`，但是可以通过组合 `LEFT JOIN` 和 `RIGHT JOIN` 来实现类似的效果。`FULL JOIN` 返回两个表中所有的记录，匹配的部分返回实际数据，不匹配的部分返回 `NULL`。结合`UNION`来实现。

  - 查询所有班级和所有部门

    ```mysql
    SELECT students.name, classes.class_name
    FROM students
    LEFT JOIN classes
    ON students.class_id = classes.id;
    
    UNION
    
    SELECT students.name, classes.class_name
    FROM students
    RIGHT JOIN classes
    ON students.class_id = classes.id;
    ```

- 自连接（SELF JOIN）：自连接是表与表自身进行连接的情况，即同一个表与它自己连接。MySQL 语法要求我们使用 `INNER JOIN` 或 `LEFT JOIN` 来进行自连接。

  - 假设 `employees`（员工表）中有 `id`, `name`, `manager_id` 字段，`manager_id` 表示该员工的经理（也是员工）。查询每个员工和他们经理的名字：

    ```mysql
    SELECT e.name AS employee, m.name AS manager
    FROM employees e
    LEFT JOIN employees m
    ON e.manager_id = m.id;
    ```

### MySQL如何避免重复插入数据？

- **使用 `UNIQUE` 约束**:在表结构的相关列使用UNIQUE约束。例如：

  ```mysql
  CREATE TABLE users (
      id INT PRIMARY KEY,
      email VARCHAR(255) UNIQUE,
      username VARCHAR(255) UNIQUE
  );
  ```

- **使用 `ON DUPLICATE KEY UPDATE`**: 如果你希望在插入数据时，遇到重复主键或唯一索引时进行更新而不是插入新记录，可以使用 `ON DUPLICATE KEY UPDATE` 语法。例如：

  ```mysql
  INSERT INTO users (id, email, username)
  VALUES (1, 'user@example.com', 'username1')
  ON DUPLICATE KEY UPDATE email = VALUES(email), username = VALUES(username);
  ```

- **使用 `INSERT IGNORE`**: 使用 `INSERT IGNORE` 语句插入数据时，如果插入的记录违反了唯一约束（比如主键或唯一索引），MySQL 会忽略插入，而不会抛出错误。例如：

  ```mysql
  INSERT IGNORE INTO users (id, email, username) VALUES (1, 'user@example.com', 'username1');
  ```

### MySQL的关键字in和exist

在 MySQL 中，`IN` 和 `EXISTS` 都是用于子查询的操作符，它们的作用有些相似，但也有关键的区别。

- `IN` 子查询：`IN` 用于在查询中检查某个值是否存在于子查询返回的结果集中。它通常用于匹配某个字段的值是否在一组可能的值中。

  - 假设有两个表：`employees` 和 `departments`，我们想查询那些属于特定部门的员工。

    ```mysql
    SELECT name
    FROM employees
    WHERE department_id IN (SELECT id FROM departments WHERE name = 'Sales');
    ```

  - 适用场景：用于检查字段值是否在一组静态值或者子查询结果中。

  - 性能：当子查询结果集较大时，`IN` 可能效率较低，因为它需要加载整个子查询结果集到内存中。

- `EXISTS` 子查询：`EXISTS` 用于检查子查询是否返回至少一行结果。它是一个布尔值表达式，只有在子查询至少返回一行记录时才为 `TRUE`，否则为 `FALSE`。

  - 假设同样有 `employees` 和 `departments` 表，我们想查找那些所在的部门至少有一个员工的部门。

    ```mysql
    SELECT name
    FROM departments
    WHERE EXISTS (SELECT 1 FROM employees WHERE employees.department_id = departments.id);
    ```

  - 适用场景：用于判断子查询是否返回至少一行记录。常用于与主查询中的列关联。

  - 性能：当子查询中有 `EXISTS` 时，MySQL 通常会停止执行子查询一旦找到第一行匹配的记录，这通常会比 `IN` 更高效，特别是在子查询返回大量数据时。

### SQL查询语句的执行顺序是怎么样的？

1. `FROM` / `JOIN`：从表中获取数据。

2. `ON`（仅在 `JOIN` 时使用）

3. `WHERE`：筛选出符合条件的记录。
4. `GROUP BY`：将结果按指定列进行分组。
5. `HAVING`：在分组后的数据上进行过滤，可以使用聚合函数（例如 `COUNT(*)`）来过滤分组后的数据。
6. `SELECT`：选择需要的列，聚合函数计算的结果会出现在这里。
7. `DISTINCT`（如果有的话）：去重（如果使用）。
8. `ORDER BY`：排序。
9. `LIMIT`：限制返回的记录数。

举例：

```mysql
SELECT department, COUNT(*) AS num_employees
FROM employees
WHERE salary > 3000
GROUP BY department
HAVING num_employees > 5
ORDER BY num_employees DESC
LIMIT 10;
```

1. **`FROM employees`**：首先从 `employees` 表中获取数据。

2. **`WHERE salary > 3000`**：接着筛选出工资大于 3000 的员工。

3. **`GROUP BY department`**：然后将员工按 `department` 列进行分组。

4. **`HAVING num_employees > 5`**：接下来，筛选出每个部门员工数大于 5 的记录（`HAVING` 用于在分组后的数据中进行过滤）。

5. **`SELECT department, COUNT(\*) AS num_employees`**：然后选择部门和每个部门的员工数。

6. **`DISTINCT`**：如果使用了 `DISTINCT`，此时会去重。

7. **`ORDER BY num_employees DESC`**：接下来根据员工数降序排序。

8. **`LIMIT 10`**：最后返回前 10 条记录。

### SQL题

#### 给学生表、课程成绩表，求不存在01课程但存在02课程的学生的成绩

假设我们有以下两张表：

1. `Student` 表，其中包含学生的`sid`（学生编号）和其他相关信息。
2. `Score` 表，其中包含`sid`（学生编号），`cid`（课程编号）和`score`（分数）。

```mysql
SELECT s1.sid, sc1.score
FROM Student s1
JOIN Score sc1 ON s1.sid = sc1.sid
WHERE sc1.cid = '02'
AND NOT EXISTS (
    SELECT 1
    FROM Score sc2
    WHERE sc2.sid = s1.sid
    AND sc2.cid = '01'
);
```

#### 给定一个学生表 student_score（stu_id，subject_id，score），查询总分排名在5-10名的学生id及对应的总分

```mysql
SELECT RankedScores.stu_id, RankedScores.total_score
FROM (
    SELECT 
        stu_id, 
        SUM(score) AS total_score
    FROM student_score
    GROUP BY stu_id
    ORDER BY total_score DESC
    LIMIT 4, 6
) AS RankedScores;
```

### MySQL 的 NULL 值是怎么存放的？

![11](C:/Users/93752/Desktop/工作面试/小林coding/06-MySQLres/11.png)

MySQL 的 Compact 行格式中会用「NULL值列表」来标记值为NULL 的列，NULL 值并不会存储在行格式中的真实数据部分。

NULL值列表会占用 1 字节空间，当表中所有字段都定义成 NOT NULL，行格式中就不会有 NULL值列表，这样可节省 1 字节的空间。

### varchar(n)中n最大取值是多少

如上图，我们的Compact行中变长字段长度列表，一行最多存储65535的字节数，但是：

- 如果变长字段最大字节数小于等于255字节，就会用1字节表示变长字段长度。
- 如果变长字段最大字节数大于255字节，就会用2字节表示变长字段长度。

所以我们要根据变长字节的字节长度来选择变长字段的长度。

- 如果数据库使用字符集ascii并且只有一个varchar(n)，那么n的最大长度应该减去[变长字段长度列表]和[NULL值列表]所占用的字节数。也就是65535-2-1=65532。
- 如果采用utf-8字符集，一个字符需要三个字节。那么n的取值就是65532/3=21844

### 行溢出后，MySQL 是怎么处理的？

mysql的表空间由段（segment）、区（extent）、页（page）、行（row）组成。InnoDB的数据页是按照页为单位来读写的。一个页的默认大小是16KB，也就是16384字节。

而如果一个varchar(n)的类型最多可以存储65532字节，那么一个页就存不了一条记录，会发生行一处，多的数据就会存到另外的溢出页中。

Compact 行格式针对行溢出的处理是这样的：当发生行溢出时，在记录的真实数据处只会保存该列的一部分数据，而把剩余的数据放在「溢出页」中，然后真实数据处用**20 字节**存储指向溢出页的地址，从而可以找到剩余数据所在的页。

## 存储引擎

### 一条SQL执行过程

![9](C:/Users/93752/Desktop/工作面试/小林coding/06-MySQLres/9.png)

- **解析阶段**：
  - 语法分析：检查查询是否符合 SQL 语法，如果正确转化为抽象语法树
  - 语义分析：检查查询的语义，确保表、列、函数等元素存在并且符合预期。防止元素不存在。
- **优化阶段**：
  - 查询重现：优化器可能会对查询进行重写
  - 查询计划生成：检测是否有索引等等，从而制定执行计划
  - 选择执行策略：根据表的大小、索引选择从而选择最佳的执行路径
- **执行阶段**：
  - 根据执行计划执行sql预计，从存储引擎读取记录，返回给客户端

### mysql的引擎

- **InnoDB**（默认引擎）

  - 特点：
    - 事务支持（ACID）
    - 行级锁
    - 支持崩溃恢复
    - B-tree（聚集索引和非聚集索引）：在聚集索引中，叶子节点存储的是表的数据行，每个表只能有一个聚集索引。非聚集索引的叶子节点存储的是主键值（而不是数据行本身），通过主键值指向实际数据。双向链表为了方便倒序遍历或排序。

  - 适用场景：大多数场景，如电商、银行等

- MyISAM
  - 特点：
    - 不支持事务
    - 表级锁
    - B-tree（非聚集索引）：MyISAM 使用非聚集索引，数据的存储顺序与索引顺序无关。每个表可以有多个非聚集索引，索引的叶子节点存储的是数据的地址（指针），通过指针访问数据行。
  - 适用场景：适合读多写少的情景，如网站日志、数据仓库等。
- MEMORY
  - 特点：
    - 存储在内存中，速度快
    - 不持久化
    - B-tree（非聚集索引）+哈希索引（Hash Index）：哈希索引通过哈希算法将数据映射到内存中，以加快查找速度。
  - 适用场景：适用于临时表、缓存数据等。

### MySQL中innoDB的三层B+树能存多少数据

**计算思路：**

1. 计算叶节点的大小
2. 计算子节点的个数，由此算出第三层叶子节点的个数（n*n）
3. 一个节点的子节点个数=页大小/（索引键大小+指向该子节点的指针大小）
4. 计算每个叶子节点存储的数据行个数（多少条数据）
5. 算出第三层能存储的总的数据行数

每个节点就是一个页，页是MySql InnoDB引擎中最小的存储单元，一个**页里面有一个页目录，存储的是多个 索引键和子节点指针的组合**。

- **页大小**（也就是B+树的一个节点）：**默认16KB**
- **索引键和指针大小**：**假设索引键是BigInt类型即8字节，指针大小通常是6字节**。
- 数据大小1KB

**root**能存16384/（8+4）=1170

**第二层：**每个根节点的指针可以指向一个第二层的节点，所以第二层最多有1170个节点。第二层最多有1170个指针。

**第三层节点（叶子节点）**：每个第二层节点的指针可以指向多个第三层节点（叶子节点）。因此，第三层的节点数是第二层节点数乘以每个第二层节点的指针数，即1170 * 1170。

再乘每个节点存储的数据行数，一页放16行。



## 索引

### 讲讲索引的分类是什么？

在数据库中，**索引**是一种加速查询操作的技术。它可以帮助数据库系统快速找到和检索特定的数据行，而不需要对整个表进行全表扫描。索引提高了数据检索的效率，但会牺牲一些写入操作的性能，因为每次对表进行插入、删除或更新时，索引也需要被更新。

- 按数据结构分类：
  - B+tree索引、Hash索引、Full-text索引。
- 按物理存储分类：
  - 聚簇索引（主键索引）、二级索引（辅助索引）。
- 按字段特性分类：
  - 主键索引、唯一索引、普通索引、前缀索引。
- 按字段个数分类：
  - 单列索引、联合索引

#### 按数据结构分类

- B+ Tree 索引

  **定义**：B+ Tree 是一种自平衡的树形数据结构，每个节点都存储键值和指向其他节点的指针。它是B 树的一种变体，叶节点之间通过指针相连，可以更高效地执行范围查询。

  ![10](C:\Users\93752\Desktop\工作面试\小林coding\06-MySQLres\10.png)

  - 特点：
    - 支持范围查询
    - 因为平衡树的原因，所以无论查询还是删除的时间复杂度都是O(log N)

- hash索引

  **定义**：Hash 索引通过哈希函数对键值进行映射，生成一个哈希值，然后根据哈希值定位存储位置。Memory引擎支持。

  - 特点：
    - 等值查询效率高
    - 不支持范围查询

- Full-text 索引

  **定义**：全文索引是对文本进行分词处理，针对每个单词建立索引。适用于大文本字段的快速搜索。Memory引擎不支持。

  - 特点：
    - 支持模糊查询不支持精确查询
    - 适用于大文本字段

#### 按物理存储分类

- 聚簇索引（Clustered Index）

  **定义**：聚簇索引是表中数据的存储顺序和索引顺序一致的索引。数据行本身按索引的顺序存储在磁盘中。

  - 特点
    - 每个表只能有一个聚簇索引。
    - 数据的物理存储顺序与索引的顺序相同。
    - **主键索引通常是聚簇索引**，也可以通过其他字段创建聚簇索引。

- 二级索引（Secondary Index）（或非聚簇索引）

  **定义**：二级索引（或非聚簇索引）是将索引结构与数据的存储顺序分离的索引。索引中保存了数据记录的指针，而不是数据本身。

  - 特点：
    - 一个表可以有多个二级索引。
    - 数据的物理存储顺序与索引的顺序无关。
    - 查询时，首先通过索引找到记录的指针，然后访问数据。

#### 按字段特性分类

- 主键索引（Primary Key Index）

  **定义**：主键索引是由表的主键字段建立的索引。主键索引要求字段值唯一且不能为空。

  - 特点
    - 每个表只能有一个主键索引。
    - 自动创建聚簇索引（如果没有手动指定其他聚簇索引）。

- 唯一索引（Unique Index）

  **定义**：唯一索引保证索引列的值是唯一的，但允许 NULL 值。通常作用在UNIQUE字段上。

  - 特点：
    - 不允许重复的值，但允许多个 NULL 值。
    - 可以创建在非主键字段上，用于确保数据的唯一性。
    - 索引列可以是单列或多列。

- 普通索引（Regular Index）

- 前缀索引（Prefix Index）

  **定义**：前缀索引是只对字段的前几位创建索引，而不是整个字段值。适用于长文本字段。

  - 特点：
    - 节省空间：对于字符串类型的字段，通过前缀索引，可以节省存储空间。
    - 常用于处理大文本字段（如 URL、长字符串等）。

#### 按字段个数分类

- 单列索引（Single-column Index）

- 联合索引（Composite Index）

  **定义**：联合索引是由多个列组成的索引，适用于包含多个字段的查询条件。

  - 特点：

    - 列的顺序很重要，索引的有效性通常依赖于查询条件的顺序。遵从**最左匹配原则**，也就是按照最左优先的方式进行索引的匹配。如果创建了一个 `(a, b, c)` 联合索引。`where a=1`、`where a=1 and b=2`是可以正常索引的。但如果是`where b=1`、`where b=1 and c=2`索引则会失效。

- 联合索引的最左匹配原则，在遇到范围查询（如 >、<）的时候，就会停止匹配，也就是范围查询的字段可以用到联合索引，但是在范围查询字段的后面的字段无法用到联合索引。注意，对于 >=、<=、BETWEEN、like 前缀匹配的范围查询，并不会停止匹配，前面我也用了四个例子说明了。

### 索引失效的情况

- 使用左或者左右模糊匹配的时候，也就是 `like %xx` 或者 `like %xx%`这两种方式都会造成索引失效；而右模糊匹配则不会失效，类似`like xx%`。
- 在查询条件中对索引列做了计算、函数、类型转换操作，这些情况下都会造成索引失效。
- 联合索引要能正确使用需要遵循最左匹配原则，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效。
- 在 WHERE 子句中，如果在 OR 前的条件列是索引列，而在 OR后的条件列不是索引列，那么索引会失效。

以上只是常见的索引失效的场景，现实中我们用explain分析来判断查询语句是否使用了索引。

### 优化索引的方法

- **前缀优化：**将一些大字符串使用前缀索引来减小索引项的大小。
  - order by 无法使用前缀索引
  - 无法把前缀索引用作覆盖索引
- **覆盖索引优化：**如果从二级索引中查询到记录就可以避免通过聚簇索引回表查询。
  - 可以建立一个联合索引，即「商品ID、名称、价格」作为一个联合索引。如果索引中存在这些数据，查询将不会再次检索主键索引，从而避免回表。

- **主键索引最好是自增的：**使用自增主键我们的每一步在b+树上的操作都是追加。并且会使索引结构紧凑，减少页分裂的次数。并且主键的字段最好越短越好，相应的二级索引的空间也就越小。
- **索引列最好设置为NOT NULL**：如果索引列中有NULL，会占用物理空间且难以优化。
- **防止索引失效**

### MySQL 使用 like “%x“，索引一定会失效吗？

- 如果一个表有多个字段，name是索引字段，id是自增主键，其他为非索引
  - `name like "xxx"`会走name二级索引，type 是 `range`
  - `name like "xx%"`会走name二级索引，type 是 `range`
  - `name like "%xx"`左模糊匹配，索引失效
  - `name like "%xx%"`左右模糊匹配，索引失效

- 如果一个表有2个字段，name是索引字段，id是自增主键，没有非索引字段
  - `name like "xxx"`会走name二级索引，type 是 `range`
  - `name like "xx%"`会走name二级索引，type 是 `range`
  - `name like "%xx"`会走name二级索引，type 是 `index`
  - `name like "%xx%"`会走name二级索引，type 是 `index`

为什么都走name的二级索引呢？这张表的字段没有「非索引」字段，所以 `select *` 相当于 `select id,name`，然后这个查询的数据都在二级索引的 B+树，因为二级索引的 B+ 树的叶子节点包含「索引值+主键值」，所以查二级索引的 B+ 树就能查到全部结果了，这个就是覆盖索引。

但是第三第四条type 是 `index`，这代表着是通过全扫描二级索引的 B+ 树的方式查询到数据的，也就是遍历了整颗索引树。

而第一和第二条查询语句的执行计划中 type 是 `range`，表示对索引列进行范围查询，也就是利用了索引树的有序性的特点，通过查询比较的方式，快速定位到了数据行。

### 哪种 count 性能最好？

**count(*)=count(1)>count(主键字段)>count(字段)**

count() 是一个聚合函数，函数的参数不仅可以是字段名，也可以是其他任意表达式，该函数作用是统计符合查询条件的记录中，函数指定的参数不为 NULL 的记录有多少个。

- count(*)和count(1)的操作过程一样，如果没有二级索引，采用主键索引来统计，读到一条记录count+1。如果有二级索引，则会优先采用代价最小的二级索引来扫描
- count(id)也是一样，一级二级索引的判别和count(*)和count(1)一样，只不过多了一步，他会对id字段进行判断，如果不为 NULL，就将 count 变量加 1。所以性能差一点。
- count(字段)如果字段不是索引话，只能全表扫描，效率最差。

### 如何优化count(*)

-  使用`show table status`或者`explain`命令来表进行估算。
-  采用额外表保存计数值。当我们在数据表插入一条记录的同时，将计数表中的计数字段 +1。也就是说，在新增和删除操作时，我们需要额外维护这个计数表。

## 事务

### 事务特性ACID

- **原子性（Atomicity）：**要么全部成功，要么全部失败。
- **一致性（Consistency）：**事务执行前后，数据库必须从一个一致性状态转变到另一个一致性状态。
- **隔离性（Isolation）：**一个事务的执行不应受到其他事务的干扰。
- **持久性（Durability）：**一旦事务提交，其对数据库的修改是永久性的，即使系统崩溃或发生其他故障，数据也不会丢失。

### 并行事务会引发什么问题？

- **脏读：**读取到其他事务未提交的数据
- **不可重复读：**在同一个事务内，执行相同查询时，查询的单个数据（或行）发生了变化。
  - 读钱先读100，然后钱被别人改了，变成了200，然后读出来是200。侧重数据。
- **幻读：**在同一个事务内，执行相同查询时，查询条件对应的**结果集**发生了变化。即查询的行数或满足条件的数据发生了变化。
  - 先查钱大于100的个数有100个，然后再查变成了150个。侧重结果集。

### mysql是怎么解决并发问题的？

- **锁机制：**Mysql提供了多种锁机制来保证数据的一致性，包括行级锁、表级锁、页级锁等。通过锁机制，可以在读写操作时对数据进行加锁，确保同时只有一个操作能够访问或修改数据。
- **事务隔离级别：**Mysql提供了多种事务隔离级别，包括读未提交、读已提交、可重复读和串行化。设置合适的事务隔离级别，可以在多个事务并发执行时，控制事务之间的隔离程度，以避免数据不一致的问题。
- **MVCC（Multi-Version Concurrency Control 多版本并发控制）**，Mysql使用MVCC来管理并发访问，它通过在数据库中保存不同版本的数据来实现不同事务之间的隔离。在读取数据时，Mysql会根据事务的隔离级别来选择合适的数据版本，从而保证数据的一致性。

### 隔离级别

SQL 标准提出了四种隔离级别来规避这些现象，隔离级别越高，性能效率就越低，这四个隔离级别如下：

- 读未提交：一个事务可以读取到另一个事务未提交的数据。
- 读已提交：一个事务只能读取到另一个事务已经提交的数据。
- 可重复读：指一个事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，**MySQL InnoDB 引擎的默认隔离级别**。可能会幻读。
- 串行化：事务完全隔离，事务之间按顺序执行，防止了“脏读”、“不可重复读”和“幻读”，但性能较差。

### MySQL InnoDB如何最大程度避免幻读？

- 针对于快照读（普通 select 语句），是通过 **MVCC** 方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。
- 针对于当前读（select ... for update）是通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。
- 在 MySQL 的可重复读隔离级别下，针对**当前读**的语句会对**索引**加记录锁+间隙锁，这样可以避免其他事务执行增、删、改时导致幻读的问题。
- 有一点要注意的是，在执行 update、delete、select ... for update 等具有加锁性质的语句，一定要检查语句是否走了索引，如果是全表扫描的话，会对每一个索引加 next-key 锁，相当于把整个表锁住了，这是挺严重的问题。

### 可重复读隔离级别下，A事务提交的数据，在B事务能看见吗？

可重复读隔离级是由 MVCC（多版本并发控制）实现的，实现的方式是开始事务后（执行 begin语句后），在执行第一个查询语句后，会创建一个 Read View，后续的查询语句利用这个 Read View，通过这个 Read View 就可以在 undo log 版本链找到事务开始时的数据，所以事务过程中每次查询的数据都是一样的，即使中途有其他事务插入了新纪录，是查询不出来这条数据的。

### 串行化隔离级别是通过什么实现的？

串行化隔离级别是通过 强制加锁 来实现的，确保事务按照顺序执行，避免并发访问造成的数据问题。具体来说：

- **行锁**：对每一行数据加锁，确保其他事务无法同时访问或修改相同的数据行。
- **范围锁（Gap Lock）**：锁定数据行之间的“间隙”，防止其他事务插入满足查询条件的新数据行，避免 幻读。
- **强制顺序执行**：事务必须一个接一个地执行，避免并发访问，确保事务之间不会相互影响。

### 介绍MVCC实现原理

**MVCC（多版本并发控制，Multiversion Concurrency Control）** 是一种用于数据库系统的并发控制机制，通过为每个数据项保留多个版本来允许多个事务并发执行，同时避免事务间的冲突。MVCC 的核心目标是提高数据库的并发性，同时保持数据一致性。

1. **InnoDB的MVCC实现**：
   - InnoDB 使用 **Undo Log**（回滚日志）和 **双重写（Double Write）** 日志来实现 MVCC。
   - 每行记录会有两个隐藏的字段：
     - **TRX_ID**：记录创建该版本的事务ID。
     - **ROLL_PTR**：指向上一版本的指针，通常为空，表示没有版本链。
2. **事务开始时**，InnoDB 会记录事务的 **事务ID**。
3. **更新时**，InnoDB 会生成新的记录，并标记旧记录为过期。
   - 在查询时，InnoDB 会检查每一行的事务ID，确保**只返回对当前事务可见的记录版本**。
4. **清理过时版本**：通过 **后台清理线程**（通常称为 **Purging**）清理不再需要的旧版本。

### 可重复读是如何实现的

**可重复读隔离级别是启动事务时生成一个 Read View，然后整个事务期间都在用这个 Read View**。

假设事务 A （事务 id 为51）启动后，紧接着事务 B （事务 id 为52）也启动了，那这两个事务创建的 Read View 如下：

![12](C:\Users\93752\Desktop\工作面试\小林coding\06-MySQLres\12.png)

事务 A 和 事务 B 的 Read View 具体内容如下：

- 在事务 A 的 Read View 中，它的事务 id 是 51，由于它是第一个启动的事务，在事务 A 的 Read View 中，它的事务 id 是 51，由于它是第一个启动的事务，下一个事务 id 则是 52。
- 在事务 B 的 Read View 中，它的事务 id 是 52，由于事务 A 是活跃的，在事务 B 的 Read View 中，它的事务 id 是 52，由于事务 A 是活跃的，下一个事务 id 应该是 53。

接着，在可重复读隔离级别下，事务 A 和事务 B 按顺序执行了以下操作：

- 事务 B 读取小林的账户余额记录，读到余额是 100 万；
- 事务 A 将小林的账户余额记录修改成 200 万，并没有提交事务；
- 事务 B 读取小林的账户余额记录，读到余额还是 100 万；
- 事务 A 提交事务；
- 事务 B 读取小林的账户余额记录，读到余额依然还是 100 万；

事务 B 第一次读小林的账户余额记录，在找到记录后，它会先看这条记录的 trx_id，此时发现 trx_id 为 50，比事务 B 的 Read View 中的 min_trx_id 值（51）还小，这意味着修改这条记录的事务早就在事务 B 启动前提交过了，所以该版本的记录对事务 B 可见的，也就是事务 B 可以获取到这条记录。

接着，事务 A 通过 update 语句将这条记录修改了（还未提交事务），将小林的余额改成 200 万，这时 MySQL 会记录相应的 undo log，并以链表的方式串联起来，形成**版本链**，如下图：

![13](C:\Users\93752\Desktop\工作面试\小林coding\06-MySQLres\13.png)

你可以在上图的「记录的字段」看到，由于事务 A 修改了该记录，以前的记录就变成旧版本记录了，于是最新记录和旧版本记录通过链表的方式串起来，而且最新记录的 trx_id 是事务 A 的事务 id（trx_id = 51）。

然后事务 B 第二次去读取该记录，发现这条记录的 trx_id 值为 51，在事务 B 的 Read View 的 min_trx_id 和 max_trx_id 之间，则需要判断 trx_id 值是否在 m_ids 范围内，判断的结果是在的，那么说明这条记录是被还未提交的事务修改的，这时事务 B 并不会读取这个版本的记录。而是沿着 undo log 链条往下找旧版本的记录，直到找到 trx_id 「小于」事务 B 的 Read View 中的 min_trx_id 值的第一条记录，所以事务B 能读取到的是 trx_id 为 50 的记录，也就是小林余额是 100 万的这条记录。

最后，当事物 A 提交事务后，由于隔离级别时「可重复读」，所以事务 B 再次读取记录时，还是基于启动事务时创建的 Read View 来判断当前版本的记录是否可见。所以，即使事物 A 将小林余额修改为 200万并提交了事务， 事务 B 第三次读取记录时，读到的记录都是小林余额是 100 万的这条记录。

就是通过这样的方式实现了，「可重复读」隔离级别下在事务期间读到的记录都是事务启动前的记录。

### 读已提交是如何实现的

**读提交隔离级别是在每次读取数据时，都会生成一个新的 Read View**。

与可重复提交的前两次读取的Read View都一样，唯一区别的是第三次读取，在A事务提交后，B事务再次读取时，会重新生成一个Read View

![14](C:\Users\93752\Desktop\工作面试\小林coding\06-MySQLres\14.png)

因为A事务已经提交，所以A事务会从m_ids中移除，这样min_trx_id就是52，而undo_log记录的trx_id是51，比事务 B 的 Read View 中的 min_trx_id 值（52）还小，这意味着修改这条记录的事务早就在创建 Read View 前提交过了，所以该版本的记录对事务 B 是可见的。

所以第三次读时会变成200W，与第一次第二次读的完全不一样。这就是读已提交所存在的隐患，不可重复读。

### 一条update是不是原子性的？为什么？

是原子性，主要通过锁+undolog 日志保证原子性的

- 执行 update 的时候，会加行级别锁，保证了一个事务更新一条记录的时候，不会被其他事务干扰。
- 事务执行过程中，会生成 undo log，如果事务执行失败，就可以通过 undolog 日志进行回滚。

### 滥用事务，或者一个事务里有特别多sql的弊端？

事务的资源在事务提交之后才会释放的，比如存储资源、锁。

- 如果一个事务特别多 sql，锁定的数据太多，容易造成大量的死锁和锁超时。
- 回滚记录会占用大量存储空间，事务回滚时间长。在MySQL中，实际上每条记录在更新的时

- 执行时间长，容易造成主从延迟，主库上必须等事务执行完成才会写入binlog，再传给备库。所以，如果一个主库上的语句执行10分钟，那这个事务很可能就会导致从库延迟10分钟。

## 锁

###  mysql有哪些锁

- **全局锁 (Global Lock)**：会锁定整个 MySQL 服务的所有数据库，通常用于备份（如 `FLUSH TABLES WITH READ LOCK`）等操作。这种锁在数据库级别生效，对所有数据库都有效。

  - 既然备份数据库数据的时候，使用全局锁会影响业务，那有什么其他方式可以避免？？

    InnoDB引擎支持的事务支持可重复读的隔离级别，那么在备份数据库之前先开启事务，会先创建 Read View，然后整个事务执行期间都在用这个 Read View，而且由于 MVCC 的支持，，备份期间业务依然可以对数据进行更新操作。

- **表级锁 (Table Lock)**：锁定整张表，其他事务无法对该表进行任何操作，直到锁释放。

  - 表锁：通过lock tables 语句可以对表加表锁，表锁除了会限制别的线程的读写外，也会限制本线程接下来的读写操作。
  - 元数据锁：当我们对数据库表进行操作时，会自动给这个表加上MDL，对一张表进行 CRUD 操作时，加的是 **MDL 读锁**；对一张表做结构变更操作的时候，加的是 **MDL 写锁**；MDL 是为了保证当用户对表执行 CRUD 操作时，防止其他线程对这个表结构做了变更。
  - 意向锁：当执行插入、更新、删除操作，需要先对表加上「意向独占锁」，然后对该记录加独占锁。意向锁的目的是为了快速判断表里是否有记录被加锁。

- **行级锁**：InnoDB 引擎是支持行级锁的，而 MyISAM 引擎并不支持行级锁。

- 记录锁（Record Lock）：锁住的是一条记录。而且记录锁是有 S 锁和 X 锁之分的，满足读写互斥，写写互斥

- 间隙锁（Gap Lock）：只存在于可重复读隔离级别，目的是为了解决可重复读隔离级别下幻读的现象。

- Next-Key Lock 称为临键锁，是 Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。

### update处理是否会锁住全表

- **MySQL两个线程的update语句同时处理一条数据，会不会有阻塞？**

  处理时next-key 锁会退化成记录锁，会锁住当前记录，会阻塞

- **两条update语句处理一张表的不同的主键范围的记录，一个<10，一个>15，会不会遇到阻塞？底层是为什么的？**

  不会，因为next-key锁住的范围不一样，不会形成冲突。

- **如果2个范围不是主键或索引？还会阻塞吗？**

  如果2个范围查询的字段不是索引的话，那就代表 update 没有用到索引，这时候触发了全表扫描，全部索引都会加行级锁，这时候第二条 update 执行的时候，就会阻塞了。

  因为如果 update 没有用到索引，在扫描过程中会对索引加锁，所以全表扫描的场景下，所有记录都会被加锁，也就是这条 update 语句产生了 4 个记录锁和 5 个间隙锁，相当于锁住了全表。

### MySQL死锁了怎么办？

死锁（Deadlock）是指在多线程或多进程的环境中，两个或多个事务或进程在执行过程中，因相互争夺资源而导致相互等待的情况，最终使得这些事务或进程都无法继续执行。

死锁的四个必要条件：**互斥、占有且等待、非剥夺、循环等待**。破坏一个即可。

- **设置事务等待锁的超时时间**。超过时间自动回滚。
- **开启主动死锁检测**。发现死锁后主动回滚死锁链上某一事务。

### 死锁案例

![15](C:/Users/93752/Desktop/工作面试/小林coding/06-MySQLres/15.png)

会发生什么？

本次案例中，事务 A 和事务 B 在执行完后 update 语句后都持有范围为`(20, 30）`的间隙锁，而接下来的插入操作为了获取到插入意向锁，都在等待对方事务的间隙锁释放，于是就造成了循环等待，满足了死锁的四个条件：互斥、占有且等待、不可强占用、循环等待，因此发生了死锁。

### insert怎么加行级锁

- **记录之间有间隙锁**

  举例，现在 t_order 表中，只有这些数据，**order_no 是二级索引**

  ![16](C:/Users/93752/Desktop/工作面试/小林coding/06-MySQLres/16.png)

  事务A：

  ```mysql
  select * from t_order where order_no = 1006 for update;
  ```

  事务B：

  ```mysql
  insert into t_order(order_no, create_date) values(1010,now());
  ```

  事务 B向事务 A 生成的 next-key 锁（记录锁+间隙锁）范围`（1005, +∞]` 中插入了一条记录，所以事务 B 的插入操作生成了一个插入意向锁（`LOCK_MODE: X,INSERT_INTENTION`），锁的状态是等待状态，意味着事务 B 并没有成功获取到插入意向锁，因此事务 B 发生阻塞。

- **遇到唯一键冲突**

  如果在插入新记录时，插入了一个与「已有的记录的主键或者唯一二级索引列值相同」的记录，此时插入就会失败，然后对于这条记录加上了 **S 型的锁**。

  - 如果主键索引重复，插入新记录的事务会给已存在的主键值重复的聚簇索引记录**添加 S 型记录锁**。
  - 如果唯一二级索引重复，插入新记录的事务都会给已存在的二级索引列值重复的二级索引记录**添加 S 型 next-key 锁**。

## 日志

### 日志文件分为几种

- **Redo Log (重做日志)**
  - **用途**：Redo Log 是InnoDB存储引擎的一个关键日志，用于保证事务的持久性（Durability，ACID特性之一）。它记录了对数据的所有修改操作，这些操作是按顺序写入Redo Log的。Redo Log 关注的是事务数据修改的物理记录。是一组**循环写**的日志文件。
  - **工作原理**：在事务提交之前，所有的修改操作先记录到Redo Log中，然后再写入到数据文件（例如 `.ibd` 文件）。如果系统崩溃，MySQL可以通过Redo Log重新执行未写入数据文件的操作，从而保证数据不会丢失。记录数据变更并保存**在内存中**，在提交事务时会刷写到磁盘，确保事务的持久性。

- **Undo Log (撤销日志)**
  - **用途**：Undo Log 用于实现事务的原子性（Atomicity），当事务发生回滚（Rollback）时，通过Undo Log撤销已执行的操作。Undo Log 记录了对数据的反向操作。
  - **工作原理**：每当对数据进行修改时，Undo Log会记录下该数据修改前的值。这样，如果事务回滚，系统会使用Undo Log将数据恢复到事务开始之前的状态。
- **Bin Log (二进制日志)**：是 server 层的日志，用于数据备份和主从辅助，保存这是逻辑日志。Binlog 是持久化到磁盘的，它是一个**顺序追加**的日志文件。Binlog 关注的是**数据库操作**事件的记录，全量日志。
  - **用途**：Bin Log 记录了所有改变数据库状态的操作（如INSERT、UPDATE、DELETE等），并用于数据的复制（主从复制）和恢复。
- **relay Log (中继日志)**
  - **用途**：Relay Log 主要用于MySQL的主从复制架构中。从服务器会把主服务器的Bin Log事件写入到自己的Relay Log中，然后从服务器通过应用这些日志来更新自己的数据。
  - **工作原理**：在复制过程中，主服务器的Bin Log会被发送到从服务器并写入到Relay Log。从服务器再根据这些日志执行相应的数据库操作来保持和主服务器的数据同步。
- **慢查询日志 (Slow Query Log)**
  - **用途**：Slow Query Log 用于记录执行时间超过指定阈值的SQL查询。通过分析这些查询，可以找出数据库性能瓶颈，进行优化。
  - **工作原理**：MySQL会记录所有执行时间超过`long_query_time`参数指定阈值的查询。这个日志文件可以帮助数据库管理员发现并优化慢查询，以提高系统性能。

### redo log怎么保证持久性的？

- **Write-Ahead Logging（WAL）：**所有的修改（如插入、更新、删除）都会首先记录在Redo Log中，而后再写入到数据文件（例如InnoDB表空间）。
- **顺序写入：**Redo Log 是顺序写入的日志文件，因此写入速度较快。它保证所有的修改操作都能按顺序保存在磁盘中，防止数据丢失。

- **Checkpoint机制：**InnoDB 将 Redo Log 写入到内存中的缓冲区（即 **Log Buffer**）后，再异步写入磁盘。即使在内存中，它仍然保证数据不会丢失。

### redo log满了怎么办

Redo Log 文件写满了，InnoDB 会自动切换到下一个日志文件，并且按顺序回绕（即覆盖最早的日志文件）。回绕时，已经提交的事务日志不会丢失，因为它们已经被写入磁盘。未提交的事务日志会通过 Undo Log回滚，保持数据一致性。

### update更新的过程

当优化器分析出成本最小的执行计划后，执行器就按照执行计划开始进行更新操作。

具体更新一条记录`UPDATE t_user SET name = 'xiaolin' WHERE id = 1;`的流程如下：

1. 执行器负责具体执行，会调用存储引擎的接口，通过主键索引树搜索获取 id = 1这一行记录：
   - 如果 id=1 这一行所在的数据页本来就在 buffer pool 中，就直接返回给执行器更新；
   - 如果记录不在 buffer pool，将数据页从磁盘读入到 buffer pool，返回记录给执行器。
2. 执行器得到聚簇索引记录后，会看一下更新前的记录和更新后的记录是否一样：
   - 如果一样的话就不进行后续更新流程；
   - 如果不一样的话就把更新前的记录和更新后的记录都当作参数传给 InnoDB层，让 InnoDB 真正的执行更新记录的操作；
3. 开启事务， InnoDB 层更新记录前，首先要记录相应的 undo log，因为这是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log。
4. InnoDB 层开始更新记录，会先更新内存（同时标记为脏页），然后将记录写到 redo log 里面，这个时候更新就算完成了。为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。
5. 至此，一条记录更新完了。
6. 在一条更新语句执行完成后，然后开始记录该语句对应的 binlog，此时记录的 binlog 会被保存到 binlog cache，并没有刷新到硬盘上的 binlog 文件，在事务提交时才会统一将该事务运行过程中的所有 binlog 刷新到硬盘。
7. 事务提交，接下来进行redo log和bin log的两阶段提交。

### binlog两阶段提交过程是怎样的？

在 MySQL 的 InnoDB 存储引擎中，开启 binlog 的情况下，MySQL 会同时维护 binlog 日志与 InnoDB 的 redo log，为了保证这两个日志的一致性，MySQL 使用了内部 XA 事务。

将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入binlog

- **prepare 阶段**：将 XID（内部 XA 事务的 ID） 写入到 redo log，同时将 redo log对应的事务状态设置为 prepare，然后将 redo log 持久化到磁盘。
- **commit 阶段**：把 XID 写入到 binlog，然后将 binlog 持久化到磁盘，将 redo log 状态设置为 commit。只要 binlog 写磁盘成功，就算 redo log 的状态还是 prepare 也没有关系，一样会被认为事务已经执行成功。

**异常状态：**

不管是时刻 A（redo log 已经写入磁盘， binlog 还没写入磁盘），不管是时刻 A（redo log 已经写入磁盘， binlog 还没写入磁盘），**此时的 redo log 都处于 prepare 状态**。

- 如果 binlog 中没有当前内部 XA 事务的 XID，说明 redolog 完成刷盘，但是 binlog 还没有刷盘，则回滚事务。对应时刻 A 崩溃恢复的情况。
- 如果 binlog 中有当前内部 XA 事务的 XID，说明 redolog 和 binlog 都已经完成了刷盘，则提交事务。

所以说，两阶段提交是以 binlog 写成功为事务提交成功的标识，因为 binlog 写成功了，就意味着能在 binlog 中查找到与 redo log 相同的 XID。

### MySQL 磁盘 I/O 很高，有什么优化的方法？

总体的方向是延迟刷盘时机，减少刷盘次数

- 增加**InnoDB Buffer Pool**大小，让更多数据缓存在内存中，减少磁盘访问。比如设置成物理内存的60%-80%。
- 增加 **InnoDB Log Buffer** 的大小，避免频繁地将日志刷到磁盘。比如设置成64M。

## 性能调优

### 给你张表，发现查询速度很慢，你有那些解决方案？

- **查看执行计划（EXPLAIN）：**使用explain查看执行计划，确定优化方向
- **优化索引**：检查是否使用索引、是否全表扫描
- **优化查询语句**：
  - 避免使用 `SELECT *`，只查询需要的列。
  - 如果只需要部分数据，使用 `LIMIT` 限制返回的记录数。
  - 避免复杂的子查询，使用 `JOIN` 替代。
- **调整数据库配置**：增加InnoDB Buffer Pool大小
- **更换硬件：**将数据库迁移到 SSD 上，提升磁盘读写速度。
- **分库分表和读写分离**：
  - 对于非常大的表，考虑分库分表，将数据分散到多个表或数据库中。
  - 使用读写分离，将读操作分发到从库，减轻主库负担。

### 分库分表

- **水平分表**：把一个表的数据按某些规则（比如 ID 范围）分到多个表里。比如把用户表按用户 ID 分成 `users_1`, `users_2` 等表。
- **垂直分表**：把一个表的不同列拆成多个表，通常是频繁查询的列和不常用的列分开。比如，把用户表分成 `users_basic`（存储基本信息）和 `users_contact`（存储联系方式）。
- **水平分库**：把数据按某些规则分到多个数据库中。比如把用户数据分到 `db1`, `db2` 等多个数据库实例里。
- **垂直分库**：把不同业务模块的数据分到不同的数据库里。比如把用户数据放到 `user_db`，商品数据放到 `product_db`。

## 安全

### 如何防止SQL注入

- **输入验证和转义：**在将用户输入用作SQL查询的一部分之前，对输入进行验证和转义。
- **使用参数化查询：**使用参数化查询可以避免直接将用户输入嵌入到SQL查询中。
- **限制数据库权限：**限制数据库用户的权限，只授予他们执行所需操作所需的最低权限。
- **实施输入过滤：**在某些情况下，实施输入过滤可以进一步减少SQL注入的风险。

# Redis面试篇

## 认识redis

Redis 是一个高性能的内存数据库，它支持多种数据结构，在底层实现中通过多种精心设计的数据结构来实现其高效的操作。

### 为什么用 Redis 作为 MySQL 的缓存？

**redis特点：**

- **高性能**：数据存储在内存里，读取和写入都非常快，比硬盘上的 MySQL 快很多。响应速度在毫秒级别。
- **高并发**：Redis 能同时处理大量请求，支持高并发场景，避免了多线程的性能瓶颈。

**为什么用 Redis 缓存 MySQL：**

- **减轻 MySQL 压力**：频繁查询的数据放在 Redis 中，减少 MySQL 的负担。
- **提升读取速度**：热点数据可以直接从 Redis 获取，不用每次都查询数据库。
- **防止缓存问题**：Redis 有很多机制可以避免缓存穿透、击穿等问题，确保数据稳定。

## 数据结构

### 讲一下Redis底层的数据结构

- 字符串（String）：缓存对象、常规计数、分布式锁、共享 session 信息等。
  - 底层是一个简单的 **动态字符串（SDS）**。支持动态增长，可以容纳任意类型的数据（字符串、整数、浮点数、二进制数据等）。
- 哈希（Hash）：存储对象类型数据，如用户信息、商品属性等。
  - 底层是 **哈希表**，用来存储键值对。
- 列表（List）：队列、消息队列、任务调度等。
  - 底层是 **双向链表**
- 集合（Set）：去重、标签系统、好友关系等。
- 有序集合（ZSet）：排行榜、延迟队列、定时任务等。

### ZSet底层是由什么实现的

Zset 类型的底层数据结构是由**跳表**实现的：

![1](C:/Users/93752/Desktop/工作面试/小林coding/07-Redisres/1.png)

跳表（SkipList）是一种用来保持有序元素的数据结构，它通过多个层次的链表加速查找操作。简单来说，跳表是一个由多个链表构成的结构，每一层链表都比上一层少一些元素，允许通过跳跃的方式快速查找。

- **多层结构**：通过多层链表，跳表减少了查找时需要遍历的节点数量，尤其是当数据量很大时，可以大大提高查找效率。
- **随机化**：跳表的随机化机制保证了插入和查找的时间复杂度平均为 **O(log N)**，即使数据量很大，查询效率仍然非常高。

## 线程模型

### Redis 是单线程吗？

Redis 单线程指的是「接收客户端请求->解析请求 ->进行数据读写等操作->发送数据给客户端」这个过程是由一个线程（主线程）来完成的，这也是我们常说 Redis 是单线程的原因。

但是，Redis 程序并不是单线程的，Redis 在启动的时候，为「关闭文件、AOF 刷盘、释放内存」这些任务创建单独的线程来处理。是因为这些任务的操作都是很耗时的，很容易发生阻塞，这样就无法处理后续的请求了。

并且在后续发展中，网络I/O也会限制Redis的性能，为了提高网络 I/O 的并行度，Redis 6.0 对于网络 I/O 采用多线程来处理。

因此在Redis6.0后，Redis 在启动的时候，默认情况下会**额外创建 6 个线程**（*这里的线程数不包括主线程*）：

- Redis-server ： Redis的主线程，主要负责执行命令。
- bio_close_file、bio_aof_fsync、bio_lazy_free：三个后台线程，分别异步处理关闭文件任务、AOF刷盘任务、释放内存任务；
- io_thd_1、io_thd_2、io_thd_3：三个 I/O 线程，io-threads 默认是 4 ，所以会启动 3（4-1）个 I/O 多线程，用来分担 Redis 网络 I/O 的压力。

### Redis怎么进行I/O多路复用的？

Redis 采用了 **事件驱动模型**，即使用一个单线程处理所有客户端连接的请求。这个线程会 **循环监听** 事件并响应相应的请求。

Redis 初始化的时候，会做下面这几件事情：

- 首先，调用 epoll_create() 创建一个 epoll 对象和调用 socket() 创建一个服务端 socket
- 然后，调用 bind() 绑定端口和调用 listen() 监听该 socket；
- 然后，将调用 epoll_ctl() 将 listen socket加入到 epoll，同时注册「连接事件」处理函数。

初始化完后，主线程就进入到一个**事件循环函数**，主要会做以下事情：

- **调用 `epoll_wait()`** 阻塞，等待事件。
- **处理连接事件**：接受新连接并将其加入到 `epoll` 中，监听该连接的读写事件。
- **处理数据读写**：如果有客户端的数据到达，读取并解析请求，然后返回响应数据。
  - **读取数据**：当客户端发送请求时，Redis 会使用非阻塞方式读取数据。如果没有数据，`recv()` 会立即返回。
  - **写入数据**：当 Redis 要返回响应时，它会使用非阻塞方式写数据。如果连接不可写，`send()` 会立即返回。
- **继续阻塞等待**：处理完当前的请求后，继续等待新的事件发生。

### Redis 采用单线程为什么还这么快？

- Redis 的大部分操作都在内存中完成，并且采用了高效的数据结构，redis的瓶颈一般是内存或者网络带宽，所以可以采用单线程。
- 单线程省去了多线程之间来回切换的性能开销。
- Redis采用I/O多路复用机制处理大量的客户端 Socket 请求。内核会一直监听这些 Socket 上的连接请求或数据请求。一旦有请求到达，就会交给 Redis 线程处理。

## 事务

### Redis锁有哪几种实现方式

- **基于 SETNX 实现分布式锁**
  - 通常称为**分布式锁**。这种方式是基于 Redis 的原子操作来确保一个客户端可以在 Redis 中创建一个独占的锁。
- **基于 `SET` 命令实现的带有超时的分布式锁**
  - Redis 5.0 版本引入了 `SET` 命令的 **NX** 和 **EX** 参数，可以用来在一个命令中同时实现 **设置键值**、**设置锁** 和 **设置超时** 的功能。
- **基于 Redisson 的分布式锁。**底层依然使用set的。
- **基于 Lua 脚本实现的分布式锁**

###  如何实现redis 原子性？

redis 执行一条命令的时候是具备原子性的，因为 redis 执行命令的时候是单线程来处理的，不存在多线程安全的问题。

**如果要保证 2 条命令的原子性的话，如何实现？**

- **可以考虑用 lua 脚本**

  将多个操作写到一个 Lua 脚本中，Redis 会把整个 Lua 脚本作为一个整体执行，在执行的过程中不会被其他命令打断，从而保证了 Lua 脚本中操作的原子性。

- **redis 事务**

  - 如果 redis 事务正常执行，使用 MULTI 和 EXEC 配合使用就可以保证多个操作都完成。
  - 如果 redis 事务发生错误了，就没办法保证原子性了。事务中执行失败的操作不会回滚，因为**redis没有回滚机制。**

### Redis中incr命令保证原子性的原理是什么

- Redis 是一个**单线程**的系统，所有操作（包括 `INCR`）都是通过单线程执行的。这意味着当 Redis 处理一个客户端的请求时，它不会同时处理其他客户端的请求。这就天然保证了在执行 `INCR` 命令时不会被其他命令打断，从而实现原子性。
- Redis 使用**事件循环机制来处理客户端的请求**。每一个客户端的请求都会被放入一个队列中，Redis 按照队列中的顺序逐一处理请求。

## 持久化及日志

### Redis有哪些持久化方式？分别的优缺点是什么？

- **RDB（快照存储）**

  RDB 持久化是 Redis 默认的持久化机制，它会定期将 Redis 内存中的数据 **快照** 保存到磁盘上的一个 `.rdb` 文件中。

  - **原理：**Redis 会根据配置的规则（例如每隔多少时间或者多少次写操作）定期生成 RDB 快照。生成快照时，Redis 会 fork 出一个子进程，将内存中的数据存储到磁盘文件中。
  - 优点：
    - **性能高**：由于 RDB 是基于快照的持久化方式，它通常不会影响 Redis 的正常读写性能。
    - **适用于大规模部署**：由于 RDB 文件是一个二进制的快照，它非常适合用于数据备份和恢复。
    - **较少磁盘 I/O 操作**：RDB 的数据写入磁盘是周期性发生的，不像 AOF 会每次写操作都执行 I/O。
  - 缺点：
    - **数据丢失**：RDB 的持久化是周期性的，因此在上次持久化之后到 Redis 崩溃期间的数据会丢失。
    - **持久化时间较长**：当 Redis 数据集很大时，生成 RDB 快照的过程可能会导致较长的阻塞时间。
  - **适用场景：**对数据丢失有一定容忍的场景，比如缓存系统。

- **AOF（追加日志文件）**

  AOF 持久化方式通过将每个写操作都记录到一个日志文件中来实现持久化。每次客户端执行写操作时，Redis 会把操作记录在 AOF 文件中。

  - **原理：**AOF 文件是一个日志文件，其中记录了所有的写操作。Redis 会把写操作追加到文件的末尾。AOF 文件在每次写操作后会被写入磁盘，可以根据配置的 `fsync` 策略来控制何时同步到磁盘。
  - 优点：
    - **数据持久性更强**：AOF 记录的是所有的写操作，因此可以在崩溃后恢复到上次写操作的状态
    - **更细粒度的控制**：通过 `fsync` 配置，AOF 可以实现多种持久化方式，包括“每次写操作后同步”（最强一致性），以及“每秒同步一次”等。
  - 缺点：
    - **写入性能差**：每次写操作都会产生磁盘 I/O。
    - **AOF 文件较大**：随着时间的推移，AOF 文件会不断增大。
  - **适用场景：**对数据持久化要求较高的领域，如金融应用等。

- **RDB 和 AOF 结合使用**

  - 优点：结合了 RDB 的快速恢复和 AOF 的高持久性。在恢复时，Redis 会先加载 RDB 文件，然后应用 AOF 中的操作。
  - 缺点：Redis 会额外占用磁盘空间，且可能会影响性能。

## 缓存淘汰及过期删除

###  过期删除策略和内存淘汰策略有什么区别？

- 内存淘汰策略是在内存满了的时候，redis 会触发内存淘汰策略，淘汰一些不必要的内存资源。
  - 常用淘汰策略：淘汰最近最少使用的数据LRU和随机选择。
  - 当 Redis 内存达到 `maxmemory` 限制时，就会触发内存淘汰机制，根据配置的策略删除一些数据。
- 过期键删除策略是将已过期的键值对进行删除，Redis 采用的删除策略是惰性删除+定期删除。
  - Redis 会定期扫描所有设置了过期时间的键（默认每 100 毫秒扫描一次）。在扫描过程中，它会检查每个键是否过期，如果过期，则删除该键。
  - Redis 在访问一个键时，会检查该键是否过期。如果过期，就删除它。这种方式被称为惰性删除。也就是说，Redis 只会删除那些被访问过的过期键。

### Redis 持久化时，对过期键会如何处理的？

Redis 持久化文件有两种格式：RDB（Redis Database）和 AOF（Append Only File）

- RDB模式
  - 在生成 RDB 文件时，**过期的键不会被保存**。
  - 加载 RDB 文件时，**主服务器不会加载过期键**，从服务器会加载所有键，但同步时会清除过期的键。
- AOF模式
  - 在写入 AOF 文件时，**过期的键会保存，当过期键删除时，但会有 DEL 命令删除它们**。
  - AOF 重写时，**过期的键不会被保存到新文件中**。

## Redis集群

### Redis 如何实现服务高可用？

Redis 通过 **主从复制** 实现读写分离和基本的数据冗余，通过 **哨兵机制** 实现自动故障转移，而通过 **集群模式** 实现更高规模的分布式数据存储与高可用性。结合这些机制，Redis 能确保服务在节点或主服务器故障时保持可用。

- 主从复制（Replication）

  **原理**：Redis 可以配置多个从服务器（Slave）来复制主服务器（Master）的数据。

  **高可用性**：从服务器实时同步主服务器的数据，当主服务器出现故障时，可以将一个从服务器提升为主服务器，保证服务继续提供。

  - **主从同步：**
    - **完全同步情况：**初次同步、从服务器请求全量同步、主从差异过大
    - **增量同步情况：**当主从偏移量位于环形缓冲区内，则是增量同步。

- 哨兵机制（Sentinel）

  **原理**：Redis Sentinel 是一种监控和故障转移机制，用于监控 Redis 实例的健康状态，并在主服务器宕机时自动进行故障转移

  **高可用性**：当 Sentinel 发现主服务器宕机时，它会自动选择一个从服务器提升为主服务器，并通知其他 Redis 实例进行更新。

- 集群模式（Cluster）

  **原理**：Redis 集群允许采用哈希槽（Hash Slot）将数据分布到多个节点上，每个节点负责存储数据的某一部分（通过分片）。Redis 集群具有自动分片和数据冗余功能。

  **高可用性**：集群模式支持每个数据分片有多个副本（主从复制），当某个节点发生故障时，集群会自动将该分片的副本提升为主节点。

### 集群脑裂导致数据丢失怎么办？

群脑裂（Split Brain）是指在分布式系统中，多个节点出现网络分区或失去联系，导致系统分为多个不相互通信的“脑”，这些“脑”可能独立地做出决策，造成数据不一致或丢失。

- **使用 Redis Sentinel**：监控主节点的健康，自动进行故障转移，确保主节点宕机时能自动选举新的主节点。

- **设置 `cluster-require-full-coverage` 为 `yes`**：这样在脑裂发生时，集群会停止接受写请求，避免数据不一致。

- **增加副本数量**：每个分片配置多个副本，即使发生脑裂，仍有副本可用来恢复数据。

- **定期备份（RDB 和 AOF）**：发生脑裂导致的数据丢失时，可以从备份中恢复。

## 缓存设计

### 如何避免缓存雪崩、缓存击穿、缓存穿透？

- **缓存雪崩**是指大量缓存同时失效，导致大量请求直接访问数据库，造成数据库压力过大，可能会引发系统宕机。
  - **避免办法：**
    - 设置缓存过期时间的随机性
    - 提前预热缓存
    - 使用多级缓存
- **缓存击穿**是指某个热点数据的缓存失效，而同时有大量请求访问该数据，导致请求直接打到数据库上，从而造成数据库负担过重。
  - **避免办法：**
    - **加锁机制**：当缓存失效时，使用分布式锁（如 Redis 锁）保证只有一个请求会去加载数据并更新缓存，其他请求等待加载完成后直接返回缓存数据。
    - 针对热点数据异步进行更新缓存
- **缓存穿透**是指请求的数据既不在缓存中，也不在数据库中（通常是恶意请求或无效数据），导致请求直接打到数据库上，造成数据库的无谓负担。
  - **避免办法：**
    - **缓存不存在的查询结果**：当查询的数据为空（例如数据库查询结果为 `null` 或空集合），也将该结果缓存一段时间，避免后续相同请求每次都查询数据库。
    - **使用 Bloom Filter**：在查询缓存前，先通过 Bloom Filter 判断请求的数据是否存在于数据库中。如果不存在，直接返回空数据或者错误，避免查询数据库。
    - **限制无效请求**：在接口层做合理的验证和过滤，避免无效请求（例如恶意攻击、过期数据）直接访问缓存和数据库。

### 布隆过滤器原理介绍一下

布隆过滤器（Bloom Filter）是一种空间效率极高的概率型数据结构，主要用于测试一个元素是否属于一个集合。它的核心特点是能够快速判断某个元素是否存在于集合中，但有一定的误判率（假阳性），即可能会错误地报告某个元素存在，但不会漏掉实际存在的元素。

布隆过滤器由「初始值都为 0 的位图数组」和「 N 个哈希函数」两部分组成。当我们在写入数据库数据时，在布隆过滤器里做个标记，这样下次查询数据是否在数据库时，只需要查询布隆过滤器，如果查询到数据没有被标记，说明不在数据库中。

- **添加元素**：通过多个哈希函数计算数组的位置，把对应的位设为 `1`。
- **查询元素**：用相同的哈希函数计算位置，如果所有位置都是 `1`，说明元素“可能”存在；如果有位置是 `0`，说明元素一定不存在。

### 常见的缓存更新策略

- Cache Aside（旁路缓存）策略

  - 读写策略
    - 写策略：先更新数据库再删除缓存
    - 读策略：先读缓存再读数据库

  - 适用场景：Cache Aside 策略适合读多写少的场景，不适合写多的场景
  - 缺点：存在缓存穿透的问题

- Read/Write Through（读穿 / 写穿）策略

  - 读写策略
    - 读穿（Read Through）：与旁路缓存一样
    - 写穿（Write Through）：如果缓存有，就缓存和数据库一块更新，如果缓存没有，直接更新数据。

  - 适用场景：Cache Aside 策略适合读多写少的场景，不适合写多的场景
  - 缺点：存在缓存穿透的问题

- Write Back（写回）策略

  - 写回策略是与写穿策略相反的，写操作仅仅写入缓存，不立即写入数据库。然后后台会有一个定时任务，将缓存中的数据批量写入数据库。
  - 适用场景：适合大规模写操作且可以容忍一定延迟的数据更新场景
  - 缺点：会有一致性和数据缺失的风险。

### 数据库和缓存如何保证一致性？

保证缓存一致性需要根据具体的需求来定：

- 对数据实时性有一定要求

  对数据实时性有一定要求即数据库数据更新需要近实时查询到最新的数据，针对这种情况可采用延迟双删、**Canal+MQ异步同步**的方式。

- 对数据实时性要求不高

  使用定时任务的方式定时更新缓存，或者直接用redis查也行。

- 对数据实时性要求非常高

  此类场景不适合用缓存，直接使用数据库即可

## Redis实战

### Redis 如何实现延迟队列？

**Redis 可以通过Zset来实现延迟队列**。这个方法利用了有序集合的 **按分数排序** 特性来实现任务的延迟执行。

- 当一个任务需要延迟执行时，我们将它插入到 Sorted Set 中，**分数** 设置为未来的时间戳，**成员** 设置为任务标识。
- 消费者端进行监听，如果当前时间匹配到延迟队列的毫秒值就立刻消费。

**ttl+死信交换机怎么实现延迟队列？**

**TTL (Time-To-Live)**：指定消息在队列中存活的最大时间。当消息的 **TTL** 超过设置的时间后，消息就会被自动删除或转发到一个 **死信交换机**（DLX）。

- 生产者将消息发送到 `delay_queue`。
- 消息在 `delay_queue` 存活一段时间（TTL 到期后）。
- 消息转发到 **死信交换机 (DLX)**，再进入 **延迟队列 (delayed_queue)**。
- 消费者从 `delayed_queue` 获取到的消息就是经过延迟处理的。

### Redis 的大 key 如何处理？

大 key 并不是指 key 的值很大，而是 key 对应的 value 很大。

- 对大Key进行拆分。例如将含有数万成员的一个HASH Key拆分为多个HASH Key，并确保每个Key的成员数量在合理范围。
- 对大Key进行清理。将不适用Redis能力的数据存至其它存储，并在Redis中删除此类数据。注意，要使用异步删除。
- 监控Redis的内存水位。
- 对过期数据进行定期清。

### Redis 的热 key 如何处理？

- 在Redis集群架构中对热Key进行复制。

- 使用读写分离架构。

###  如何设计秒杀场景处理高并发以及超卖现象？

用分布式锁、Redis等技术都可以防止超卖，**Redisson** 和 **Lua** 可以配合使用来增强 Redis 操作的性能和原子性。

- 使用Redis分布式锁Redisson解决超卖问题，控制多个Jvm进程去争抢同一个锁，将并发操作库存改为同步执行。
- 使用Redis原子操作解决超卖问题，Redis命令具有原子性，将库存放在Redis中，使用decr命令去扣减库存。增加使用incr。

### Redis 分布式锁的原理

现实使用Redisson实现分布式锁。使用多线程从同步队列查询并处理数据时，同一个队列只允许一个线程去处理，这里我们用到了分布式锁，锁的粒度是每个同步队列。

- **加锁**：客户端通过 Redis 的 `SET` 命令加锁，要求锁的键不存在时才能设置，并且设置一个过期时间。
- **释放锁**：客户端通过 `DEL lock_key` 删除锁，但为了避免误删其他客户端的锁，通常需要先确认锁的值是否和自己加锁时存的值一样，确保自己是持有锁的客户端。
- **锁超时**：如果客户端持有锁的时间超过了设置的过期时间，锁会自动释放，避免死锁问题。

### Redis 管道有什么用？

Redis 管道（Pipeline）是将多个命令打包一起发送到 Redis，避免每个命令都等待响应，从而减少网络延迟，提高性能。这样可以一次性执行多个命令，而不用每次等待一个命令的结果，适合批量操作。

### Redis 事务支持回滚吗？

不支持，要么全部成功要么全部失败，即使是lua脚本，也只是回到运行脚本之前的情况。

- 如果 redis 事务正常执行，使用 MULTI 和 EXEC 配合使用就可以保证多个操作都完成。
- 如果 redis 事务发生错误了，就没办法保证原子性了。事务中执行失败的操作不会回滚，因为**redis没有回滚机制。**

# 计算机网络面试篇

## 网络模型

### 网络OSI模型和TCP/IP模型分别介绍一下

- OSI分为：应用层、表示层、会话层、传输层、网络层、、数据链路层以及物理层。

  - 应用层，负责给应用程序提供统一的接口；
  - 表示层，负责把数据转换成兼容另一个系统能识别的格式；
  - 会话层，负责建立、管理和终止表示层实体之间的通信会话；
  - 传输层，负责端到端的数据传输；
  - 网络层，负责数据的路由、转发、分片；
  - 数据链路层，负责数据的封帧和差错检测，以及 MAC 寻址；

  - 物理层，负责在物理网络中传输数据帧；

- TCP/IP分为：应用层、传输层、网络层、链路层

  - 应用层 支持 HTTP、SMTP 等最终用户进程
  - 传输层 处理主机到主机的通信（TCP、UDP）
  - 网络层 寻址和路由数据包（IP 协议）

  - 链路层 通过网络的物理电线、电缆或无线信道移动比特

### 键入网址到网页显示，期间发生了什么？

- **DNS 查找**：浏览器使用 DNS 协议查找你输入的网址url对应的服务器 IP 地址。
- **建立 TCP 连接**：浏览器通过 TCP 协议与服务器建立连接，确保数据能可靠地传输。
- **发送 HTTP 请求**：浏览器使用 HTTP 或 HTTPS 协议向服务器发送请求，请求网页数据。
- **获取数据**：服务器通过 HTTP 或 HTTPS 响应浏览器，返回网页内容（如 HTML、CSS、图片等资源）。
- **渲染页面**：浏览器解析并渲染网页，将内容显示在屏幕上。

## 应用层- HTTP

### 应用层有哪些协议？

HTTP、HTTPS、CDN、DNS、FTP、SMTP

### HTTP是什么及HTTP报文有哪些部分？

HTTP 是超文本传输协议。

报文分为：

- 请求报文：
  - **请求行**：包含请求方法、请求的资源路径和 HTTP 协议版本。`GET /index.html HTTP/1.1`

  - **请求头**：一组键值对，包含客户端和请求相关的附加信息。

    - `Host`: 请求的目标主机地址。

    - 客户端能够处理的内容类型`Accept`：`application/json`等
    - 认证信息`Authorization`
    - `Range`指定字节范围。

  - **空行**：请求头和请求体之间必须有一个空行。

  - **请求体**：可选部分，包含发送到服务器的数据（仅在一些请求方法如 `POST`、`PUT` 中存在）。

- 响应报文：
  - **状态行**：包含 HTTP 协议版本、状态码和状态说明。`HTTP/1.1 200 OK`
  - **响应头**：包含与响应相关的附加信息
    - 返回内容的类型`Content-Type`：`application/json`等
    - `Content-Length`：响应体的长度。
  - **空行**：响应头和响应体之间也有一个空行。
  - **响应体**：实际的内容部分，即服务器返回的资源（如 HTML 页面、图片、JSON 数据等）。

### HTTP是怎么传输数据的

1. **客户端发起请求：**当用户在浏览器中输入网址或点击链接时，浏览器会发起一个 HTTP 请求，向目标服务器请求数据，构建请求报文。
2. 浏览器首先将请求的域名通过 DNS（Domain Name System）解析为 IP 地址。
3. 建立 TCP 连接
4. 发送 HTTP 请求
5. **服务器处理请求**：服务器接收到请求后，根据请求的路径、方法等进行处理。例如，如果请求的是一个静态资源（如 HTML 文件、图片），服务器直接返回该资源。如果是动态内容（如数据库查询），服务器则可能会执行一些脚本（如 PHP、Node.js 或 Java）来生成响应内容。
6. **服务器响应请求**：构建响应报文。
7. 客户端接受响应，解析报文并渲染在浏览器。
8. 如果是长连接，则等待，如果连接不需要则四次挥手拜拜。

### HTTP常用的状态码？

- 2xx 类状态码表示服务器**成功**处理了客户端的请求，也是我们最愿意看到的状态。
- 3xx 类状态码表示客户端请求的资源发生了变动，需要客户端用新的 URL 重新发送请求获取资源，也就是**重定向**。
- 4xx 类状态码表示客户端发送的**报文有误**，服务器无法处理，也就是错误码的含义。
- 5xx 类状态码表示客户端请求报文正确，但是**服务器处理时内部发生了错误**，属于服务器端的错误码。

**常见的包括：**

- 200：请求成功；
- 301：永久重定向；302：临时重定向；
- 404：无法找到此页面；405：请求的方法类型不支持；
- 500：服务器内部出错。502 Bad Gateway（收到无效的响应）。504 Gateway Time-out（超时）。

### GET和POST的使用场景，有哪些区别？

- **幂等性**
  - GET：多次发送相同的 GET 请求，结果是相同的，不会对服务器的数据产生副作用。
  - POST：POST 请求可能会修改服务器上的资源，每次请求可能会产生不同的效果。

- **参数传递**
  - GET：参数通过 URL 传递，通常附加在 `?` 后面。
  - POST：参数放在请求体中，而不是 URL 中，可以传递大量数据
- **可缓存**：
  - GET：浏览器和中间代理服务器可以缓存 GET 请求的结果，以提高性能。
  - POST：POST 请求的结果通常不可缓存。
- **用途**：
  - GET：**适用于查询操作**：当请求只涉及查询数据时，应该使用 GET。
  - POST：**适用于提交操作**：当请求涉及创建或修改数据时，应该使用 POST。

### HTTP的长连接是什么（KeepAlive）

**HTTP 长连接**（也称为持久连接）指的是在一个 TCP 连接中，客户端和服务器可以多次发送和接收 HTTP 请求和响应，而不需要为每个请求都建立新的连接。这与传统的 **短连接**（每次请求都建立一个新的 TCP 连接）不同，长连接能够减少连接建立和关闭的开销，提高性能。

- **HTTP/1.1**：
  - 引入了持久连接的概念，默认情况下，HTTP/1.1 的连接是持久的，不再像 HTTP/1.0 中那样每次请求都关闭连接。
- **HTTP/2：**
  - 在长连接的基础上进行了优化，引入了多路复用机制。在一个连接上，多个请求和响应可以并行传输，而无需等待前一个请求完成。这进一步提升了效率。

HTTP/1.1默认使用长连接，也可以使用在请求或响应头中显示的使用`Connection: keep-alive`来开启长连接。

### HTTP和HTTPS的区别

- **安全性：**
  - HTTP是超文本传输协议，信息是明文传输，存在安全风险。
  - HTTPS在HTTP引入**SSL/TLS**安全协议，使得报文可以加密传输。
- **连接建立：**
  - HTTP建立相对简单，TCP三次握手即可进行HTTP报文传输。
  - HTTPS不仅需要TCP三次握手，还需要 SSL/TLS 的握手过程，才可进入加密报文传输。
- **默认端口：**
  - HTTP采用80，HTTPS采用443
- **安全验证：**
  - **HTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的。**CA 通过验证证书持有者的身份并签发证书，从而保证了服务器的身份。只有在 **证书由受信任的 CA 签发**，客户端（如浏览器）才会信任该证书，从而建立安全的 HTTPS 连接。**自签名证书**（即自己签发的证书）不需要 CA，但浏览器通常会显示警告，因为它不能验证该证书的合法性。

### HTTPS握手过程

HTTPS除了TCP三次握手外，还有TLS四次通信，我们这里着重讲一下tls四次握手

- **TLS 第一次握手：客户端Hello（Client Hello）**

  客户端向服务器发送一个**ClientHello**消息，消息中包含：

  - 支持的TLS版本（如TLS 1.2或TLS 1.3）
  - 支持的加密套件（如AES、RSA、ECDSA等）
  - 客户端随机数**Client Random**（用于后续的加密密钥生成）

- **TLS 第二次握手：服务器Hello（Server Hello）**

  服务器收到**ClientHello**消息后，选择一个TLS版本和加密套件，并向客户端发送**ServerHello**消息，内容包括：

  - 选择的TLS版本及加密套件
  - 服务器随机数**Server Random**（用于后续的加密密钥生成）
  - 服务器数字证书（包括公钥等信息）

- **TLS 第三次握手：客户端回应** 

  客户端收到服务器的回应之后，首先通过浏览器或者操作系统中的 CA 公钥，确认服务器的数字证书的真实性。如果证书没有问题，客户端会**从数字证书中取出服务器的公钥**，然后使用它加密报文，向服务器发送如下信息：

  - 一个随机数**pre-master key**，该随机数会被服务器公钥加密。
  - 加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。
  - 客户端握手结束通知，表示客户端的握手阶段已经结束。

  服务器和客户端有了这三个随机数（Client Random、ServerRandom、pre-master key），接着就用双方协商的加密算法，各自生成本次通信的「会话秘钥」。

- **TLS 第四次握手：服务器的最后回应** 

  服务器收到客户端的第三个随机数（`pre-master key`）之后，通过协商的加密算法，计算出本次通信的「会话秘钥」。然后，向客户端发送最后的信息：

  - 加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。
  - 服务器握手结束通知，表示服务器的握手阶段已经结束。

至此，整个 TLS 的握手阶段全部结束。接下来，客户端与服务器进入加密通信，就完全是使用普通的 HTTP 协议，只不过用「会话秘钥」加密内容。

### HTTPS一定安全可靠吗

HTTPS本身是没有任何漏洞的，即使成功被中间人进行网络攻击，本质上是利用了客户端的漏洞（用户点击继续访问或被恶意导入伪造的根证书），而这并不是HTTPS的问题。

抓包工具也是同理，也是一个中间人的角色。

- 中间人与真实服务端建立连接是正常的，因为服务端并不会校验客户端身份。
- 而中间人与客户端建立连接，这里会有信任的问题，也就是中间人作为服务端要有对应域名的私钥。而中间人能拿到对应域名的私钥只能通过：
  - 去网站服务端拿到私钥；
  - 去CA处拿域名签发私钥；
  - 自己签发证书，但是会被浏览器拦截，需要被信任；
- 很明显，抓包工具对HTTPS抓包使用私钥是第三种方式，需要在客户端安装 Fiddler 的根证书，这里实际上起认证中心（CA）的作用。
- 抓包工具会往系统受信任的根证书列表中导入抓包工具生成的证书，而这个证书会被浏览器信任，也就是抓包工具给自己创建了一个认证中心 CA，客户端拿着中间人签发的证书去中间人自己的 CA 去认证，当然认为这个证书是有效的。

### 平时使用什么来抓包

- **tcpdump**

​	linux使用tcpdump抓取和分析网络流量

- **fiddler**

  用于 Web 流量的抓包与分析。它提供图形化界面，适合开发者和测试人员使用。

### HTTP/1.1性能及特点

HTTP 最突出的优点是「简单、灵活和易于扩展、应用广泛和跨平台」。

HTTP 协议里有优缺点一体的**双刃剑**，分别是「无状态、明文传输」，同时还有一大缺点「不安全」。

- 无状态
  - 好处：不需要记录状态信息，服务器压力小
  - 坏处：完成关联操作需要其他组件帮助，比如cookie
- 明文：这个就显而易见了，方便调试但不安全。安全这个问题在HTTPS中引入**SSL/TLS** 层才得已解决。

**HTTP/1.1**

- 特点：
  - **持久连接（Keep-Alive）：**HTTP/1.1 默认支持持久连接
  - **管道化（Pipelining）**：允许客户端在发送请求时不必等待前一个请求的响应，可以同时发送多个请求。但是**HTTP/1.1 管道解决了请求的队头阻塞，但是没有解决响应的队头阻塞**。**但HTTP/1.1默认并没有使用管道化技术**。

###  /2 /3分别做了什么优化

**HTTP/1.1**

- 特点：
  - **持久连接（Keep-Alive）：**HTTP/1.1 默认支持持久连接
  - **管道化（Pipelining）**：允许客户端在发送请求时不必等待前一个请求的响应，可以同时发送多个请求。但是**HTTP/1.1 管道解决了请求的队头阻塞，但是没有解决响应的队头阻塞**。**但HTTP/1.1默认并没有使用管道化技术**。

**HTTP/2**

- 特点：
  - **二进制协议**：HTTP/2 将 HTTP/1.x 的文本协议改为二进制协议。二进制协议在解析和传输时更加高效。比如将`status: 200 OK `的编码改为`1000 1000`。
  - **头部压缩（Header Compression）**：HTTP/2 使用 HPACK 压缩头部信息，减少了由于重复头部数据（如 cookies、用户代理等）导致的带宽浪费。使用静态表来减少重复传输，静态表的目的是通过使用预定义的常见头部来减少重复传输。
  - **多路复用（Multiplexing）**：HTTP/2 通过允许在同一个连接中并行发送多个请求和响应，解决了 HTTP/1.x 中的队头阻塞问题。多个请求的响应可以交错地返回，不会相互阻塞。虽然解决了HTTP的队头阻塞问题，但还是**存在TCP的队头阻塞问题**，因为TCP是字节流协议，所以必须是完整且连续的，一旦发生丢包现象，会触发TCP的重传机制，那么这个TCP连接中的所有HTTP请求都会等待这个丢了的包被传回来。
  - **服务器推送（Server Push）**：HTTP/2 允许服务器在客户端请求之前主动推送资源。例如，当浏览器请求一个 HTML 页面时，服务器可以提前推送该页面依赖的 CSS 和 JavaScript 文件，减少客户端的请求次数。

**HTTP/3** 

- 特点：
  - **基于 QUIC 协议**：HTTP/3 放弃了传统的 TCP 协议，改为使用 QUIC（Quick UDP Internet Connections）协议，基于 UDP 传输。QUIC 本身解决了 TCP 中的许多问题，如连接建立时间长、丢包恢复慢等。
  - **减少连接建立延迟**：QUIC 实现了 0-RTT（零延迟连接），意味着客户端和服务器可以在一次往返（RTT）内完成握手，降低了延迟。
  - **多路复用不再受限于队头阻塞**：由于 QUIC 是基于 UDP 的，数据包的丢失不会导致其他请求的阻塞，因此解决了 HTTP/2 中可能出现的队头阻塞问题。
  - **内置加密**：QUIC 协议强制使用加密（TLS 1.3），提高了数据传输的安全性和隐私性。

### 如何优化HTTP/1.1

- **使用缓存技术：**保存相同请求的响应数据，如果缓存没有过期就直接读取本地缓存的响应数据。如果缓存过期，客户端发送请求的时候带上响应数据的摘要，服务器比对后发现资源没有变化就发出不带包体的 304 响应，告诉客户端缓存的响应仍然有效。
- **减少HTTP请求次数：**
  - 将交由客户端处理的重定向请求交由代理服务器，减少重定向次数。
  - 多个小资源合并成一个大资源减少HTTP请求次数。
  - 按需访问资源，只访问看得到的资源。
- **压缩响应资源，减少传输资源大小。**仿照HTTP2进行优化。

### 如何优化HTTPS

- **协议优化：**
  - 密钥交换算法选择安全性、性能更高的ECDHE算法，而不是RSA算法。
  - 升级TLS
- **证书优化：**
  - 服务器使用ECDSA证书，而不是RSA证书。
- **会话重用：**会话可以在1RTT的时间内恢复会话。会话虽好，但有风险，需要一个合理的而过期时间
  - Session ID
  - Session Ticket

### HTTP进行TCP连接之后，在什么情况下会中断

- 当服务端或者客户端执行 close 系统调用的时候，会发送FIN报文，就会进行四次挥手的过程
- 当发送方发送了数据之后，接收方超过一段时间没有响应ACK报文，发送方重传数据达到最大次数的时候，就会断开TCP连接
- 当HTTP长时间没有进行请求和响应的时候，超过一定的时间，就会释放连接

### HTTP、SOCKET和TCP的区别

- HTTP是一种用于传输超文本数据的应用层协议，用于在客户端和服务器之间传输和显示Web页面。
- Socket是计算机网络中的一种抽象，用于描述通信链路的一端，提供了底层的通信接口，可实现不同计算机之间的数据交换。
- TCP是一种面向连接的、可靠的传输层协议，负责在通信的两端之间建立可靠的数据传输连接。

### 既然有 HTTP 协议，为什么还要有 RPC？

早期的HTTP和RPC（远程过程调用）都来自于TCP协议，只不过是各式各样加了消息头消息体的TCP。

那为什么会有两个不同的协议呢？因为早期的网络结构，自己公司的服务器只用服务自己的客户即可，用RPC就可以满足这样的需求，也就是CS的架构。但是随着互联网的发展，浏览器出现了，要求不仅能访问自家服务器，还要能访问别家服务器，这时候BS架构出现了。

- HTTP主要用于B/S架构，RPC更多用于C/S架构，现在提倡多端融合，所以C/S，B/S架构也在慢慢融合。对外HTTP，对内RPC协议。
- RPC 本质上不算是协议，而是一种调用方式，而像 gRPC 和 Thrift 这样的具体实现，才是协议，它们是实现了 RPC 调用的协议。
- RPC出现早，比主流的HTTP/1.1性能还好，历史遗留问题。

### 既然有 HTTP 协议，为什么还要有 WebSocket？

- TCP 协议本身是**全双工**的，但我们最常用的HTTP/1.1，虽然是基于 TCP 的协议，但它是**半双工**的，对于大部分需要服务器主动推送数据到客户端的场景，都不太友好，因此我们需要使用支持全双工的 WebSocket 协议。
- 在 HTTP/1.1 里，只要客户端不问，服务端就不答。基于这样的特点，对于登录页面这样的简单场景，可以使用**定时轮询或者长轮询**的方式实现**服务器推送**(comet)的效果。
- 对于客户端和服务端之间需要频繁交互的复杂场景，比如网页游戏，都可以考虑使用WebSocket 协议。
- WebSocket 和 socket 几乎没有任何关系，只是叫法相似。
- 正因为各个浏览器都支持 HTTP协议，所以WebSocket 会先利用HTTP协议加上一些特殊的header头进行握手升级操作，升级成功后就跟HTTP 没有任何关系了，之后就用 WebSocket的数据格式进行收发数据。

### HTTPS 中 TLS 和 TCP 能同时握手吗？

**HTTPS 是先进行 TCP 三次握手，再进行 TLSv1.2 四次握手**

HTTPS 中的 TLS 握手过程可以同时进行三次握手只有在以下两个情景都满足的情况下才可以：

- **客户端和服务端都开启了 TCP Fast Open 功能，且 TLS 版本是 1.3**
- **客户端和服务端已经完成过一次通信**

### Nginx位于哪一层，负载均衡有哪些算法

Nginx位于应用层。其负载均衡算法包括：轮询、加权轮询、IP哈希、URL哈希、最少连接等等。
虽然Nginx工作在应用层，但进行TCP/UDP负载均衡时也会涉及到传输层。

## 应用层-安全

### 对称加密与非对称加密？

- **对称加密：**加密和解密用同一个密钥。
  - 速度快，但需要安全地交换密钥。
  - 如果密钥泄露，安全性就没保障。
- 非对称加密
  - 非对称加密使用**一对密钥**：一个是**公钥**（public key），另一个是**私钥**（private key）。**公钥**用于加密，**私钥**用于解密。公钥可以公开，任何人都可以使用公钥加密数据，但只有私钥持有者才能解密。
  - 反之，私钥加密的数据只能用公钥解密。
    - 私钥加密通常不是用于保密消息，而是用于**数字签名**。
    - **私钥加密**并不是加密消息内容，而是对消息的**数字签名**。**公钥解密**用于验证签名是否来自持有该私钥的人。

### cookie和session有什么区别？

- 存储位置：
  - Cookie：存储在客户端浏览器中。
  - Session：存储在服务器端。浏览器会存储一个唯一的 **Session ID**，这个 ID 会在每次请求时被发送到服务器，服务器根据这个 ID 来找到与之对应的会话数据。
- 生命周期：
  - Cookie：可以设置过期时间，过期后自动删除。也可以设置为会话Cookie，即浏览器关闭时自动删除。
  - Session：在默认情况下，当用户关闭浏览器时，Session结束。如果用户长时间没有活动，服务器通常会自动销毁 Session 或设置一个过期时间。
- 安全及存储：
  - Cookie 数据不安全，每个 Cookie 存储的内容是有限的。
  - Session 数据存储在服务器端，存储容量几乎没有限制。

### token，session，cookie的区别？

- session存储于服务器，可以理解为一个状态列表，拥有一个唯一识别符号sessionId，通常存放于cookie中。服务器收到cookie后解析出sessionId，再去session列表中查找，才能找到相应session，依赖cookie。
- cookie类似一个令牌，装有sessionId，存储在客户端，浏览器通常会自动添加。
- token也类似一个令牌，无状态，用户信息都被加密到token中，服务器收到token后解密就可知道是哪个用户，需要开发者手动添加。

### 什么数据应该存在到cookie，什么数据存放到 Localstorage

Cookie 适合用于在客户端和服务器之间传递数据、跨域访问和设置过期时间，而 LocalStorage适合用于在同一域名下的不同页面之间共享数据、存储大量数据和永久存储数据。

- 存储容量:
  - Cookie 的存储容量通常较小,每个 Cookie 的大小限制在几 KB 左右。
  - LocalStorage 的存储容量通常较大,一般限制在几 MB 左右。
- 数据发送:
  - Cookie 在每次 HTTP 请求中都会自动发送到服务器
  - localStorage 的数据不会自动发送到服务器,它仅在浏览器端存储数据,因此 LocalStorage 适合用于在同一域名下的不同页面之间共享数据;
- 生命周期：
  - Cookie 可以设置一个过期时间,使得数据在指定时间后自动过期。
  - LocalStorage 的数据将永久存储在浏览器中,除非通过JavaScript 代码手动删除
- 安全性：
  - Cookie 的安全性较低,因为 Cookie 在每次 HTTP 请求中都会自动发送到服务器,存在被窃取或篡改的风险。
  - LocalStorage 的数据仅在浏览器端存储，相对而言更安全一些。

### http的cookie有安全泄漏吗？

- **未加密的传输（不使用 HTTPS）**
  - 如果网站仅使用 HTTP（而不是 HTTPS）传输数据，Cookie 信息在网络中是明文传输的，容易被中间人攻击（MITM，Man-in-the-Middle Attack）。
- **跨站脚本攻击（XSS）**
  - XSS（Cross-Site Scripting）攻击使得恶意脚本能够注入到网站页面中。当攻击者能够将恶意 JavaScript 注入到页面时，恶意脚本可能访问存储在 Cookie 中的敏感信息，进而窃取用户的身份验证信息或其他重要数据。

一般**使用 Cookie 并设置 `Secure` 和 `HttpOnly` 标志**是一个较好的选择。

- 可以使用 `Secure` 标志确保 Cookie 仅通过 HTTPS 传输，防止中间人攻击。
- 可以使用 `HttpOnly` 标志，防止 JavaScript 访问 Cookie，提高安全性，减少 XSS 攻击风险。

### JWT 令牌和传统方式有什么区别?

- 无状态性：JWT是无状态的令牌，不需要在服务器端存储会话信息。JWT令牌中包含了所有必要的信息，如用户身份、权限等。这使得JWT在分布式系统中更加适用，可以方便地进行扩展和跨域访问。
- 安全性：JWT使用密钥对令牌进行签名，确保令牌的完整性和真实性。只有持有正确密钥的服务器才能对令牌进行验证和解析。
- 跨域支持：JWT令牌可以在不同域之间传递，适用于跨域访问的场景。通过在请求的头部或参数中携带JWT令牌，可以实现无需Cookie的跨域身份验证。

### JWT 令牌都有哪些字段？

JWT令牌由三个部分组成：头部（Header）、载荷（Payload）和签名（Signature）。头部和载荷均为JSON格式，使用Base64编码进行序列化，而签名部分是对头部、载荷和密钥进行签名后的结果。

### JWT 令牌为什么能解决集群部署，什么是集群部署？

在传统的基于会话和Cookie的身份验证方式中，会话信息通常存储在服务器的内存或数据库中。但在集群部署中，不同服务器之间没有共享的会话信息，这会导致用户在不同服务器之间切换时需要重新登录，或者需要引入额外的共享机制（如Redis），增加了复杂性和性能开销。

当用户进行登录认证后，服务器将生成一个JWT令牌并返回给客户端。客户端在后续的请求中携带该令牌，服务器可以通过对令牌进行验证和解析来获取用户身份和权限信息而无需访问共享的会话存储。

由于JWT令牌是自包含的，服务器可以独立地对令牌进行验证，而不需要依赖其他服务器或共享存储。这使得集群中的每个服务器都可以独立处理请求，提高了系统的可伸缩性和容错性。

### JWT 令牌如果泄露了，怎么解决，JWT是怎么做的？

- 黑名单制度或失效制度：因为JWT令牌派发后无法撤回，所以可以将泄露的令牌添加到黑名单或失效状态，在验证时先检查是否存在于黑名单或者是否失效。
- 刷新令牌：JWT令牌通常具有一定的有效期，过期后需要重新获取新的令牌。

### 前端是如何存储JWT的？

前端存储 JWT 令牌有几种常见的方式，主要包括 **localStorage**、**sessionStorage** 和 **cookie**。

一般**使用 Cookie 并设置 `Secure` 和 `HttpOnly` 标志**是一个较好的选择。

- 自动随每个请求发送，便于服务器进行身份验证。
- 可以使用 `Secure` 标志确保 Cookie 仅通过 HTTPS 传输，防止中间人攻击。
- 可以使用 `HttpOnly` 标志，防止 JavaScript 访问 Cookie，提高安全性，减少 XSS 攻击风险。

因为每次请求都携带Cookie，所以会导致额外的性能开销。

### 建立连接后，服务端怎么确认客户端发送的消息有没有被篡改过，恶意篡改怎么办

- **使用哈希校验：**对消息进行哈希处理，然后将哈希值发送给服务端。服务端收到消息后，重新计算消息的哈希值，并与客户端发送的哈希值进行比对。
  - 只能检测数据是否被篡改，但无法防止恶意用户伪造消息及其哈希值。因此需要对这个哈希值进行加密。

- **使用消息认证码**
  - 客户端和服务端共享一个秘密密钥,客户端在发送消息时，使用该密钥和消息内容生成 MAC 值（如 HMAC-SHA256）。

- **使用数字签名：**数字签名是一种更加安全的方法，广泛应用于消息认证、身份验证和防篡改。在数字签名中，客户端使用自己的私钥对消息进行签名，服务端则使用公钥验证签名的有效性。
  - 客户端使用自己的私钥对消息进行签名，生成数字签名。
  - 客户端将消息和签名一起发送给服务端。
  - 服务端收到消息后，使用客户端的公钥验证签名。如果签名有效，说明消息是由客户端发送且未被篡改；如果签名无效，则说明消息被篡改或伪造。
- **加密通信（如 TLS/SSL）：**通过加密的方式（如 **TLS/SSL** 协议），可以确保消息的内容在传输过程中不被篡改，同时也能验证消息的完整性和来源。TLS/SSL 协议利用对称加密和公钥加密技术，在客户端与服务端之间建立一个安全的通道。
  - 客户端和服务端在建立连接时，通过 **握手协议** 协商加密算法和密钥。
  - 通过加密的通信通道传输消息，保证消息在传输过程中的机密性和完整性。
  - 如果消息在传输过程中被篡改，接收方会发现校验失败。

## 传输层-TCP连接

### TCP三次握手过程

![1](C:/Users/93752/Desktop/工作面试/小林coding/08-计网面试篇res/1.png)

- 一开始，客户端和服务端都处于 CLOSE 状态。先是服务端主动监听某个端口，处于 LISTEN 状态。
- 客户端会随机初始化序号（client_isn），将此序号置于 TCP 首部的「序号」字段中，同时把 SYN 标志位置为 1，表示 SYN 报文。接着把第一个 SYN报文发送给服务端，表示向服务端发起连接，该报文不包含应用层数据，之后客户端处于 SYN-SENT状态。
- 服务端收到客户端的 SYN 报文后，首先服务端也随机初始化自己的序号（server_isn）,将此序号填入 TCP 首部的「序号」字段中，其次把 TCP 首部的「确认应答号」字段填入 client_isn + 1, 接着把 SYN 和 ACK 标志位置为 1。最后把该报文发给客户端，该报文也不包含应用层数据，之后服务端处于 SYN-RCVD 状态。
- 客户端收到服务端报文后，还要向服务端回应最后一个应答报文，首先该应答报文 TCP 首部 ACK 标志位置为 1 ，其次「确认应答号」字段填入server_isn + 1 ，最后把报文发送给服务端，**这次报文可以携带客户到服务端的数据**，之后客户端处于 ESTABLISHED 状态。

从上面的过程可以发现**第三次握手是可以携带数据的，前两次握手是不可以携带数据的**，这也是面试常问的题。

### 客户端发送的第一个 SYN 报文，服务器没有收到怎么办？

- 当客户端想和服务端建立 TCP 连接的时候，首先第一个发的就是 SYN 报文，然后进入到 SYN_SENT 状态。

- 如果客户端迟迟收不到服务端的 SYN-ACK 报文（第二次握手）就会触发「超时重传」机制，重传 SYN 报文，而且**重传的 SYN 报文的序列号都是一样的**。**每次超时的时间是上一次的 2 倍**。

- 当客户端超时重传 3 次 SYN 报文后，如果还是没能收到服务端的第二次握手（SYN-ACK 报文），那么客户端就会断开连接。

### 服务器收到第一个 SYN 报文，回复的 SYN + ACK 报文丢失了怎么办？

- 当服务端收到客户端的第一次握手后，就会回 SYN-ACK 报文给客户端，这个就是第二次握手，此时服务端会进入 SYN_RCVD 状态。

- 因为第二次握手报文里是包含对客户端的第一次握手的 ACK 确认报文，所以，如果客户端迟迟没有收到第二次握手，那么客户端就觉得可能自己的 SYN 报文（第一次握手）丢失了，于是**客户端就会触发超时重传机制，重传 SYN 报文**。
- 因为第二次握手中包含服务端的 SYN 报文，所以当客户端收到后，需要给服务端发送 ACK 确认报文（第三次握手），服务端才会认为该 SYN 报文被客户端收到了。**服务端这边会触发超时重传机制，重传 SYN-ACK 报文**。

### TCP 三次握手，客户端第三次发送的确认包丢失了发生什么？

客户端收到服务端的 SYN-ACK 报文后，就会给服务端回一个 ACK 报文，也就是第三次握手，此时客户端状态进入到 ESTABLISH 状态。

因为这个第三次握手的 ACK 是对第二次握手的 SYN 的确认报文，所以当第三次握手丢失了，如果服务端那一方迟迟收不到这个确认报文，就会触发超时重传机制，重传 SYN-ACK 报文，直到收到第三次握手，或者达到最大重传次数。

注意，**ACK 报文是不会有重传的，当 ACK 丢失了，就由对方重传对应的报文**。

### 第一次握手，客户端发送SYN报后，服务端回复ACK报，那这个过程中服务端内部做了哪些工作？

- 服务端收到客户端发起的 SYN 请求后，**内核会把该连接存储到半连接队列**，并向客户端响应 SYN+ACK。

- 接着客户端会返回 ACK，服务端收到第三次握手的 ACK 后，**内核会把连接从半连接队列移除，然后创建新的完全的连接，并将其添加到 accept 队列**，等待进程调用 accept 函数时把连接取出来。

- 不管是半连接队列还是全连接队列，都有最大长度限制，超过限制时，内核会直接丢弃，或返回 RST 包。

### 如何防御 SYN 攻击？

- **增大半连接和全连接队列**
- **开启tcp_syncookies功能**
  - 客户端发来第一次握手SYN时，服务端不会将其放入半连接队列中，而是直接生成一个`cookies`，这个`cookies`会跟着第二次握手，发回客户端。客户端在发第三次握手的时候带上这个`cookies`，服务端验证到它就是当初发出去的那个，就会建立连接并放入到全连接队列中。可以看出整个过程不再需要半连接队列的参与。`cookies`并没有队列保存，而是实时计算的。同时因为cookie出现带来编码和解码的问题，比较耗CPU。
  - 也会带来ACK攻击，瞎编的`cookie`CPU也会进行解析，从而导致CPU资源耗尽。
- **减少 SYN+ACK 重传次数，限制只能重传一次**

### SYN报文什么时候会被丢弃

- 半连接队列满了（如果开启了tcp_syncookies，即使半连接队列满了也不会丢弃syn报文）
  - 解决方法：参考**如何防御SYN攻击**
- 全连接队伍满了
  - 解决方法：调大全连接队列。检查为啥调用accept()不及时。

### 已经建立连接的TCP，还收到SYN会发生什么？

客户端掉线了，服务器并不知道，客户端再上线时发起SYN握手，会发生什么？

**检查四元组（源 IP 地址、源端口号、目标 IP 地址、目标端口号）是否一样。**

- 如果端口号不同，会建立新的TCP连接，旧的TCP连接服务器发给客户端，客户端会回复RST报文，以让服务端断开旧的连接。
- 如果端口号相同
  - 判断SYN携带的序列号在不在窗口内，不在的话丢弃。
  - 如果 **RST** 标志被设置，则会丢弃并重置连接。
  - 如果 **SYN** 标志被设置且序列号合法，服务端可能会进入 **SYN Challenge** 状态，发送挑战 ACK 来进行验证。挑战成功则发送最终ACK完成三次握手，建立连接。

### 为什么 TCP 每次建立连接时，初始化序列号都要不一样呢？

**防止历史报文被下一个相同四元组的连接接收。**如果上一个连接中被网络阻塞的数据包正好抵达了服务端，刚好该数据包的序列号正好是在服务端的接收窗口内，所以该数据包会被服务端正常接收，就会造成数据错乱。

- 随机数是会基于时钟计时器递增的，基本不可能会随机成一样的初始化序列号。初始化序列号可被视为一个 32 位的计数器，该计数器的数值每 4 微秒加 1，循环一次需要 4.55 小时。
- 为了解决这个问题，就需要有 TCP 时间戳。

### TCP为什么需要三次握手建立连接？

- **防止旧的连接请求影响新的连接**：防止老的确认信息干扰新的连接，比如某些延迟的SYN报文。初始化序列的随机数是根据时间戳生成的，为了避免偶然事件的发生，可以把时间戳从32bit扩大到64bit。
- **三次握手才可以同步双方的初始序列号**
- **确保可靠性**

### 服务端没有 listen，客户端发起连接建立，会发生什么？

`listen` 是用于服务端监听指定端口的函数，告诉操作系统服务端将准备好接受连接请求。`listen` 还会启动一个 **队列** 来保存那些等待连接的请求，当应用程序通过 `accept` 来接受连接时，操作系统会从队列中取出一个请求。

如果服务端没有调用 `listen`，则操作系统不会将该端口标记为可接受连接的状态。即使该端口上有应用程序在运行，操作系统也不会将其分配给该应用程序进行后续的连接处理。在没有 `listen` 的情况下，当客户端发起连接时，服务端操作系统并不会处理这个连接请求。**客户端的 SYN 包会被丢弃，并会收到一个 RST（重置）包作为响应，表明服务端没有准备好接受连接。**

### 不使用 listen ，可以建立 TCP 连接吗？

不使用 listen **，客户端和服务端**是不可以建立 TCP 连接的。

但是**客户端与客户端是可以建立连接的**。

- **客户端是可以自己连自己的形成连接（TCP自连接）**：在没有调用 `listen` 的情况下，客户端程序仍然可以使用 `connect` 成功连接自己，因为操作系统在处理客户端请求时，会直接将请求转发到同一机器上的同一个端口。这里没有服务端监听，因此不会涉及 `accept`，但仍然能够通过操作系统的回环机制（即使用本地回环地址 `127.0.0.1`）完成连接。
- **两个客户端同时向对方发出请求建立连接（TCP同时打开）**：`listen` 函数是服务端监听连接请求时所必需的，它告诉操作系统该端口在等待接受客户端的连接。**客户端之间的连接**并不需要使用 `listen`，它们直接发起SYN连接，操作系统会在本地机器上处理这些请求，最终建立连接。

### TCP自连接或同时打开的情况下，那么客户端会有半连接队列吗？

没有，因为客户端没有执行listen，因为半连接队列和全连接队列都是在执行 listen 方法时，内核自动创建的。

但内核还有个全局 hash 表，可以用于存放 sock 连接的信息。

在 TCP 自连接的情况中，客户端在 connect 方法时，最后会将自己的连接信息放入到这个全局 hash 表中，然后将信息发出，消息在经过回环地址重新回到 TCP 传输层的时候，就会根据 IP + 端口信息，再一次从这个全局 hash 中取出信息。于是握手包一来一回，最后成功建立连接。

TCP 同时打开的情况也类似，只不过从一个客户端变成了两个客户端而已。

### 没有 accept，能建立 TCP 连接吗？

下面一段简化过的服务端伪代码。

```c
/*Step 1: 创建服务器端监听socket描述符listen_fd*/ 
listen_fd = socket(AF_INET, SOCK_STREAM, 0);
/*Step 2: bind绑定服务器端的IP和端口，所有客户端都向这个IP和端口发送和请求数据*/ 
bind(listen_fd, xxx);
/*Step 3: 服务端开启监听*/   
listen(listen_fd, 128);
/*Step 4: 服务器等待客户端的链接，返回值cfd为客户端的socket描述符*/  
cfd = accept(listen_fd, xxx);
/*Step 5: 读取客户端发来的数据*/
n = read(cfd, buf, sizeof(buf));
```

在执行`listen()`方法之后还会执行一个`accept()`方法。**一般情况**下，如果启动服务器，会发现最后程序会**阻塞在**`accept()`里。

- 对socket执行bind方法可以绑定监听端口，然后执行`listen方法`后，就会进入监听（`LISTEN`）状态。内核会为每一个处于`LISTEN`状态的`socket` **分配两个队列，分别叫半连接队列(哈希表)和全连接队列（链表）。**

**`accept方法`只是为了从全连接队列中拿出一条连接，本身跟三次握手几乎毫无关系。**

## 传输层-TCP断开

### TCP四次挥手过程

![2](C:/Users/93752/Desktop/工作面试/小林coding/08-计网面试篇res/2.png)

- 客户端主动调用关闭连接的函数，于是就会发送FIN 报文，这个 FIN 报文代表客户端不会再发送数据了，进入 FIN_WAIT_1 状态；
- 服务端收到了 FIN 报文，然后马上回复一个 ACK 确认报文，此时服务端进入 CLOSE_WAIT 状态。在收到 FIN 报文的时候，TCP 协议栈会为 FIN 包插入一个文件结束符 EOF 到接收缓冲区中，服务端应用程序可以通过 read 调用来感知这个 FIN 包，这个 EOF 会被放在已排队等候的其他已接收的数据之后，所以必须要得继续 read 接收缓冲区已接收的数据；
- 接着，当服务端在 read 数据的时候，最后自然就会读到 EOF，接着 read() 就会返回 0，这时服务端应用程序如果有数据要发送的话，就发完数据后才调用关闭连接的函数，如果服务端应用程序没有数据要发送的话，可以直接调用关闭连接的函数，这时服务端就会发一个 FIN 包，这个 FIN 报文代表服端不会再发送数据了，之后处于 LAST_ACK 状态；
- 客户端接收到服务端的 FIN 包，并发送 ACK 确认包给服务端，此时客户端将进入 TIME_WAIT 状态；
- 服务端收到 ACK 确认包后，就进入了最后的CLOSE 状态；
- 客户端经过 2MSL 时间之后，也进入 CLOSE 状态；

### 第一次挥手丢失了，会发生什么？

当客户端（主动关闭方）调用 close 函数后，就会向服务端发送 FIN 报文，试图与服务端断开连接，此时客户端的连接进入到 FIN_WAIT_1 状态。

正常情况下，如果能及时收到服务端（被动关闭方）的 ACK，则会很快变为 FIN_WAIT2状态。

- 如果第一次挥手丢失了，那么客户端迟迟收不到被动方的 ACK 的话，也就会触发超时重传机制，重传 FIN报文，重发次数由 tcp_orphan_retries 参数控制。

- 当客户端重传 FIN 报文的次数超过tcp_orphan_retries ，默认3次后，就不再发送 FIN 报文，则会在等待一段时间（时间为上一次超时时间的 2 倍），如果还是没能收到第二次挥手，那么客户端直接进入到 close 状态，而服务端还是ESTABLISHED状态。

### 第三次挥手一直没发，会发生什么？

当主动方收到 ACK 报文后，会处于 FIN_WAIT2 状态，就表示主动方的发送通道已经关闭，接下来将等待对方发送 FIN 报文，关闭对方的发送通道。

- 如果连接是用 shutdown 函数关闭的，连接可以一直处于 FIN_WAIT2 状态，因为它可能还可以发送或接收数据。。
- 对于 close 函数关闭的孤儿连接，由于无法再发送和接收数据，所以这个状态不可以持续太久，默认值是 60 秒。**如果在 60 秒后还没有收到 FIN 报文，连接就会直接关闭。**

### 第二次和第三次挥手能合并嘛

当被动关闭方在 TCP 挥手过程中，「**没有数据要发送**」并且「**开启了 TCP 延迟确认机制**」，那么第二和第三次挥手就会合并传输，这样就出现了三次挥手。

- 当有响应数据要发送时，ACK 会随着响应数据一起立刻发送给对方
- 当没有响应数据要发送时，ACK 将会延迟一段时间，以等待是否有响应数据可以一起发送
- 如果在延迟等待发送 ACK 期间，对方的第二个数据报文又到达了，这时就会立刻发送 ACK

### 为什么需要 TIME_WAIT 状态？

**tcp_tw_reuse 的作用是让客户端快速复用处于 TIME_WAIT 状态的端口，相当于跳过了 TIME_WAIT 状态。**好处是可以快速复用，但坏处也很多，所以默认是关闭的。

主动发起关闭连接的一方，才会有 `TIME-WAIT` 状态。需要 TIME-WAIT 状态，主要是两个原因：

- **防止历史连接中的数据，被后面相同四元组的连接错误的接收；**序列号和初试序列号不是无限递增的，会绕回初始值。不能单单依靠序列号来判断新老数据。TIME_WAIT 状态，状态会持续 `2MSL` 时长，足以让两个方向上的数据包都被丢弃，使得原来连接的数据包在网络中都自然消失，再出现的数据包一定都是新建立连接所产生的。
- **保证「被动关闭连接」的一方，能被正确的关闭；**如果第四次挥手的ACK丢失，那么被动关闭连接就无法进入close状态，而如果 TIME_WAIT 就等不到丢失一次的第四次挥手的ACK报文之前的第三次挥手的FIN报文的重传，那么就只能发RST报文来断开连接了，这属于非正常关闭。

### 为什么四次挥手之后的TIME_WAIT要等2MSL?

MSL 是 Maximum Segment Lifetime，**报文最大生存时间**，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为 TCP 报文基于是 IP 协议的，而 IP 头中有一个 TTL 字段，是 IP 数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减 1，当此值为 0 则数据报将被丢弃，同时发送ICMP 报文通知源主机。

- MSL 与 TTL 的区别： MSL 的单位是时间，而 TTL 是经过路由跳数。所以 MSL 应该要大于等于 TTL 消耗为 0 的时间，以确保报文已被自然消亡。
  - TTL 的值一般是 64，Linux 将 MSL 设置为 30 秒，意味着 Linux 认为数据报文经过 64 个路由器的时间不会超过 30 秒，如果超过了，就认为报文已经消失在网络中了。

- TIME_WAIT 等待 2 倍的 MSL，比较合理的解释是：网络中可能存在来自发送方的数据包，当这些发送方的数据包被接收方处理后又会向对方发送响应，所以一来一回需要等待 2 倍的时间。

- 2MSL时长 这其实是相当于至少允许报文丢失一次。比如，若 ACK 在一个 MSL 内丢失，这样被动方重发的 FIN 会在第 2 个 MSL 内到达，TIME_WAIT 状态的连接可以应对。

- **防止历史连接中的数据，被后面相同四元组的连接错误的接收；**

- **保证「被动关闭连接」的一方，能被正确的关闭；**


### 四次挥手中收到乱序的FIN包会如何处理

- 在四次挥手的过程中，乱序的 **FIN** 包会被 TCP 协议栈缓存并重新排序，确保按正确的顺序完成握手和连接关闭。
- 即使 **FIN** 包乱序，TCP 也能通过序列号和缓存机制确保连接的正常关闭，避免丢失数据或错误关闭连接。
- 如果超时或收不到必需的报文，TCP 可能会超时并终止连接。

### TIME_WAIT 过多有什么危害？

- 第一是占用系统资源，比如文件描述符、内存资源、CPU 资源、线程资源等；、
- 第二是占用端口资源，端口资源也是有限的

### 服务器出现大量 TIME_WAIT 状态的原因有哪些？

- **HTTP的短连接：**无论客户端还是服务端哪一方禁用了 HTTP Keep-Alive，都是由服务端主动关闭连接，那么此时服务端上就会出现 TIME_WAIT 状态的连接。
- **HTTP 长连接超时：** HTTP 长连接的超时时间是 60 秒，如果客户端在完后一个 HTTP 请求后，在 60 秒内都没有再发起新的请求，定时器的时间一到，nginx 就会触发回调函数来关闭该连接，那么此时服务端上就会出现 TIME_WAIT 状态的连接。
- **HTTP 长连接的请求数量达到上限：**如果达到这个参数设置的最大值时，则 nginx 会主动关闭这个长连接

### 服务器出现大量 CLOSE_WAIT 状态的原因有哪些？

CLOSE_WAIT 状态是「被动关闭方」才会有的状态，而且如果「被动关闭方」没有调用 close 函数关闭连接，那么就无法发出 FIN 报文，从而无法使得CLOSE_WAIT 状态的连接转变为 LAST_ACK 状态。

当服务端出现大量 CLOSE_WAIT 状态的连接的时候，说明服务端的程序**没有调用 close 函数关闭连接**。

### 在 TIME_WAIT 状态的 TCP 连接，收到 SYN 后会发生什么？

- **TIME_WAIT** 状态下，如果收到来自相同四元组且合法（时间戳大于期望的序列值）的 **SYN** 包，TCP 协议会视为一个新的连接请求，创建新的连接，并使连接从 **TIME_WAIT** 状态转换为 **SYN_RECV**。
  - 如果不合法，服务端会再发一次第四次挥手的ACK，这时候客户端收到的ACK并不是期望的ACK，则会发RST给服务端。
- 如果接收到的 **SYN** 包来自不同的四元组，**TIME_WAIT** 状态将继续保持不变，不会产生影响。

### 在 TIME_WAIT 状态，收到 RST 会断开连接吗？

- 如果 `net.ipv4.tcp_rfc1337` 参数为 0，则提前结束 TIME_WAIT 状态，释放连接。
- 如果 `net.ipv4.tcp_rfc1337` 参数为 1，则会丢掉该 RST 报文。

尽量让该参数为1，保证对应的TIME_WAIT时间足够。

### 客户端 TCP 连接 TIME_WAIT 状态过多，会导致端口资源耗尽而无法建立新的连接吗？

要看客户端是否都是与同一个服务器（目标地址和目标端口一样）建立连接。

- 如果客户端都是与同一个服务器（目标地址和目标端口一样）建立连接，那么如果客户端 TIME_WAIT 状态的连接过多，当端口资源被耗尽，就无法与这个服务器再建立连接了。
- 如果不是一个服务器，那么端口是重复使用的。

### 如果已经建立了连接，但是客户端突然出现故障了怎么办？

客户端出现故障指的是客户端的主机发生了宕机，或者断电的场景。服务端为了探测客户端是否存活，TCP 搞了个**保活机制**。这个机制的原理是这样的：

定义一个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作用，每隔一个时间间隔，发送一个探测报文，该探测报文包含的数据非常少，如果连续几个探测报文都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息通知给上层应用程序。

包括：**保活时间（默认两小时）、保活探测的次数（默认9次）、保活探测的时间间隔（每次间隔75秒）。**

### 在没有开启 TCP keepalive，且双方一直没有数据交互的情况下，主机崩溃（主机宕机）或进程崩溃分别会发生什么？

**TCP 连接的管理**直接依赖于操作系统内核，内核会管理连接的状态，包括连接的创建、数据传输、以及连接的关闭过程。

**主机崩溃**是操作系统是意外的主机的操作系统或硬件发生故障，**主机宕机**是操作系统是可能是有计划的停机。大多数情况下两者可以互换使用。

- **没有数据交互的情况下主机崩溃**
  - 当主机崩溃了，进程无法正常调用`close()`关闭TCP连接，另外一段端是无法感知到的，在加上并没有开启 TCP keepalive，又没有数据交互的情况下，**另外一段端的 TCP 连接将会一直处于ESTABLISHED 连接状态**，直到另外一段端重启进程。
  - 所以，在没有使用 TCP 保活机制且双方不传输数据的情况下，一方的 TCP 连接处在ESTABLISHED 状态，并不代表另一方的连接还一定正常。
- **没有数据交互的情况下进程崩溃**
  - TCP 的连接信息是由内核维护的，所以当**某一端端进程**崩溃后，内核需要回收该进程的所有 TCP 连接资源，于是内核会发送第一次挥手 FIN 报文，后续的挥手过程也都是在内核完成，并不需要进程的参与，所以即使**这一端**的进程退出了，还是能与对方完成TCP 四次挥手的过程。

### 有数据传输的场景，客户端主机宕机，又迅速重启，会发生什么？

**注意，客户端重启会丢失内存中的TCP相关的结构体。**

- **如果客户端主机宕机，又迅速重启**
  - 服务端重传报文的过程中，客户端主机重启完成后，因为丢失了内存中的TCP连接的结构体，所以客户端会回复RST报文，重置该连接。**只要有一方重启完成后，收到之前 TCP 连接的报文，都会回复 RST 报文，用来断开连接后重置该 TCP 连接。**
- **如果客户端一直宕机**
  - 如果服务端向客户端发送的报文会得不到任何的响应，在一定时长后，服务端就会触发**超时重传**机制，重传未得到响应的报文，当重传总间隔时长达到一定阈值后，会断开 TCP 连接。
  - 如果服务端一直不会发送数据，再看服务端有没有开启 TCP keepalive 机制？
    - 如果有开启，服务端在一段时间没有进行数据交互时，会触发 TCP keepalive 机制，探测对方是否存在，如果探测到对方已经消亡，则会断开自身的 TCP 连接；
    - 如果没有开启，服务端的 TCP 连接会一直存在，并且一直保持在 ESTABLISHED 状态。

### 拔掉网线后， 原本的 TCP 连接还存在吗？

**注意，拔网线并不会丢失内存中的TCP相关的结构体。**

- **没有数据传输的情况：**
  - 如果双方都没有开启 TCP keepalive 机制，那么在客户端拔掉网线后，如果客户端一直不插回网线，那么客户端和服务端的 TCP 连接状态将会一直保持存在。
  - 如果双方都开启了 TCP keepalive 机制，那么在客户端拔掉网线后，如果客户端一直不插回网线，TCP keepalive 机制会探测到对方的 TCP 连接没有存活，于是就会断开 TCP 连接。而如果在 TCP 探测期间，客户端插回了网线，那么双方原本的 TCP连接还是能正常存在。

- **有数据传输的情况：**
  - 在客户端拔掉网线后，如果服务端发送了数据报文，那么在服务端**重传次数没有达到最大值之前**，客户端就插回了网线，那么双方原本的 TCP 连接还是能正常存在，就好像什么**事情都没有发生**。
  - 在客户端拔掉网线后，如果服务端发送了数据报文，在客户端插回网线之前，服务端**重传次数达到了最大值时**，服务端就会断开 TCP 连接。等到客户端插回网线后，向服务端发送了数据，因为服务端已经断开了与客户端相同四元组的 TCP 连接，所以就会**回 RST 报文**，客户端收到后就会断开 TCP连接。至此， 双方的 TCP 连接都断开了。

### 如何关闭一个TCP连接

- 最粗暴就是kill进程
- 使用工具
  - killcx：killcx 工具可以用来**关闭活跃和非活跃的 TCP 连接**，因为 killcx 工具是主动发送 SYN 报文，这时对方就会回复 Challenge ACK ，然后 killcx 工具就能从这个 ACK 获取到正确的序列号。从而给两端发RST来断开TCP连接。
  - tcpkill：只能用来**关闭活跃的 TCP 连接**，无法关闭非活跃的 TCP连接，因为 tcpkill 工具是等双方进行 TCP 通信后，才去获取正确的序列号，如果这条 TCP 连接一直没有任何数据传输，则就永远获取不到正确的序列号。也是发给两端发RST来断开TCP连接。

### TCP Keepalive 和 HTTP Keep-Alive 是一个东西吗？

- **HTTP 的 Keep-Alive 也叫 HTTP 长连接**，该功能是由「应用程序」实现的，可以使得用同一个 TCP 连接来发送和接收多个 HTTP 请求/应答，减少了 HTTP 短连接带来的多次 TCP 连接建立和释放的开销。
- **TCP 的 Keepalive 也叫 TCP 保活机制**，该功能是由「内核」实现的，当客户端和服务端长达一定时间没有进行数据交互时，内核为了确保该连接是否还有效，就会发送探测报文，来检测对方是否还在线，然后来决定是否要关闭该连接。

## 传输层-TCP与UDP

### TCP和UDP区别是什么？

- 连接：
  - TCP 是面向连接的传输层协议，传输数据前先要建立连接；
  - UDP 是不需要连接，即刻传输数据。
- 服务对象：
  - TCP 是一对一的两点服务，即一条连接只有两个端点。
  - UDP 支持一对一、一对多、多对多的交互通信。
- 可靠性：
  - TCP 是可靠交付数据的，数据可以无差错、不丢失、不重复、按序到达。
  - UDP 是尽最大努力交付，不保证可靠交付数据。
- 拥塞控制、流量控制：
  - TCP 有拥塞控制和流量控制机制，保证数据传输的安全性。
  - UDP 则没有，即使网络非常拥堵了，也不会影响 UDP 的发送速率。
- 首部开销：
  - TCP 首部长度较长，20字节
  - UDP 首部只有 8 个字节
- 传输方式：
  - TCP 是流式传输，没有边界，但保证顺序和可靠。
  - UDP 是一个包一个包的发送，是有边界的，但可能会丢包和乱序。

### TCP协议有什么缺点？

- 升级 TCP 的工作很困难
- TCP 建立连接的延迟
- TCP 存在队头阻塞问题
- 网络迁移需要重新建立 TCP 连接

### 如何基于 UDP 协议实现可靠传输？

基于 UDP 协议实现的可靠传输协议现在就是应用在HTTP/3上的QUIC协议。

**TCP 可靠传输的特性（序列号、确认应答、超时重传、流量控制、拥塞控制）**在应用层实现一遍。

- **连接迁移：**QUIC支持通过连接标识符在网络变化时快速迁移连接，例如从WiFi切换到移动数据网络，以保持连接的可靠性。
- **重传机制：**QUIC使用重传机制来确保丢失的数据包能够被重新发送，从而提高数据传输的可靠性。
- **前向纠错：**QUIC可以使用前向纠错技术，通过在数据中添加冗余信息，在接收端修复部分丢失的数据，降低重传的需求，提高可靠性和传输效率。
- **拥塞控制：**QUIC内置了拥塞控制机制，可以根据网络状况动态调整数据传输速率，以避免网络拥塞和丢包，提高可靠性。

###  tcp粘包怎么解决？

**TCP 是流式传输，没有边界，但保证顺序和可靠，所以我们需要保证边界**

- **固定长度的消息**
- **特殊字符作为边界：**如果特殊字符出现在消息中，要进行转义。
- **自定义消息结构：**定义消息结构体，用四字节大小的变量表示数据长度，后面跟真正的数据。

### TCP的拥塞控制介绍一下？

- **慢启动：**TCP 连接刚开始时，发送的初始拥塞窗口很小。每收到一个确认包，窗口就会增大，以指数增长。
- **拥塞避免：**：当窗口大小达到阈值后，**慢启动** 阶段结束，进入 **拥塞避免** 阶段。在这个阶段，拥塞窗口以线性增长的方式增加。
- **快速重传：**如果一个数据包丢失，接收方会重复发送 **重复确认**（Duplicate ACK）。发送方会在收到三个重复的 ACK 后，认为数据包丢失，立即进行重传。
- **快速恢复：**当检测到丢包时，TCP 会将拥塞窗口减半（这是为了避免再次拥塞）。接着继续使用线性增长的方式逐步恢复数据的发送速率。

### TCP 和 UDP 可以使用同一个端口吗？

TCP 和 UDP 传输协议，在内核中是由两个完全独立的软件模块实现的。

当主机收到数据包后，可以在 IP 包头的「协议号」字段知道该数据包是 TCP/UDP，所以可以根据这个信息确定送给哪个模块送给 TCP/UDP 模块的报文根据「端口号」确定送给哪个应用程序处理。

因此， TCP/UDP 各自的端口号也相互独立，互不影响。

### 多个 TCP 服务进程可以同时绑定同一个端口吗？

- 如果两个 TCP 服务进程同时绑定的 IP 地址和端口都相同，那么执行 bind() 时候就会出错，错误是“Address already in use”。
- 如果两个 TCP 服务进程绑定的端口都相同，而 IP 地址不同，那么执行bind() 不会出错。

### 如何解决服务端重启时，报错“Address already in use”的问题？

当我们重启 TCP 服务进程的时候，意味着通过服务器端发起了关闭连接操作，于是就会经过四次挥手，而对于主动关闭方，会在TIME_WAIT 这个状态里停留一段时间，这个时间大约为 2MSL。

- 要么继续等待
- 要么对 socket 设置 SO_REUSEADDR 属性。这样即使存在一个和绑定 IP+PORT 一样的 TIME_WAIT 状态的连接，依然可以正常绑定成功，因此可以正常重启成功。

### 客户端的端口可以重复使用吗？

在客户端执行 connect 函数的时候，只要客户端连接的服务器不是同一个，内核允许端口重复使用。

TCP 连接是由四元组（源IP地址，源端口，目的IP地址，目的端口）唯一确认的，那么只要四元组中其中一个元素发生了变化，那么就表示不同的 TCP 连接的。

### 用了 TCP 协议，数据一定不会丢吗？

- **丢包不可避免。**
- **大部分时候TCP的重传机制保证了消息可靠性。**
- **TCP只保证传输层的消息可靠性，并不保证应用层的消息可靠性。**如果我们还想保证应用层的消息可靠性，就需要应用层自己去实现逻辑做保证。

### linux中的8080端口有多少个TCP连接，怎么看

使用`netstat`命令

`netstat` 命令可以列出所有的网络连接，包括 TCP 连接。你可以通过 `netstat` 查看 8080 端口的 TCP 连接数量。

```shell
netstat -an | grep ':8080' | grep 'TCP' | wc -l
```

- `netstat -an`：列出所有活动的网络连接，`-a` 显示所有连接和监听端口，`-n` 显示数字地址而不是域名。
- `grep ':8080'`：过滤出所有与 8080 端口相关的行。
- `grep 'TCP'`：进一步过滤出 TCP 连接。
- `wc -l`：计算过滤结果的行数，即当前与 8080 端口的 TCP 连接数。

### linux如何查看一个线程端口

`netstat` 是一个常用的网络工具，可以列出所有网络连接（包括监听的端口）。你可以结合 `grep` 命令查找特定进程的端口。

```shell
netstat -tuln | grep <PID>
```

`-tuln` 选项表示：

- `t`: 显示 TCP 连接。
- `u`: 显示 UDP 连接。
- `l`: 显示监听的端口。
- `n`: 显示数字形式的地址和端口。

### 数据包被劫持，TCP和UDP的反应

- **TCP**：由于其面向连接和可靠的数据传输特性，TCP 受到的攻击类型通常是会话劫持、数据篡改等。攻击者可能伪造数据包、修改序列号等方式进行攻击。通过 SSL/TLS 加密可以有效避免这些问题。

- **UDP**：由于 UDP 的无连接性和不可靠性，它更容易遭受伪造源地址、流量放大和数据篡改等攻击。为了应对 UDP 攻击，可以通过应用层加密、身份验证以及流量控制等方法加强安全性。

### 王者荣耀应该用tcp还是udp

《王者荣耀》使用 **UDP** 协议来保证游戏的实时性、低延迟和高效的数据传输，而 TCP 协议则通常用于对数据完整性要求较高的场景（如文件传输等）。因此，UDP 适合这种快速、实时互动性强的在线游戏。

## 网络层

### ping是如何工作的？

- **发送请求**：`ping` 发送一个ICMP回送请求包（Type 8）到目标地址（比如一个网站或IP）。
- **接收回复**：目标主机收到请求后，返回一个ICMP回送响应包（Type 0）。
- **计算时间**：`ping` 计算从发送请求到收到响应的时间（即网络延迟），并显示出来。
- **显示结果**：`ping` 会显示成功收到响应的时间，或者显示超时（如果没有响应）。

ICMP大致分为ICMP**差错报告**和**查询消息**。在 `ping` 中主要使用的是查询消息中的回送请求与回送响应。差错报告消息主要用于通知发送方网络中的问题。常见的差错报告类型包括：

- 目标不可达消息（Type 3）：分为网络（Code 0）、主机（Code 1）、协议（Code 2）、端口（Code 3）不可达。
- 重定向消息（Type 4）：用于告知发送方通过更优的路由器来发送数据包。
- 超时信息（Type 11）：当数据包在网络中停留的时间超过其生存时间（TTL）时，会返回此消息

 ### 断网了，还能 ping 通 127.0.0.1 吗？

即使计算机断网，只要操作系统和网络堆栈没有出现问题，`ping 127.0.0.1` 依然能成功响应。

- 本地回环地址 `127.0.0.1` 是计算机的一个虚拟接口，专门用于本机网络通信。
- 这个接口的作用是将计算机内部的通信请求发送到自己的网络堆栈中，不需要物理网卡的参与。

###  127.0.0.1 和 localhost 以及 0.0.0.0 有区别吗

- `localhost` 就不叫 `IP`，它是本地域名，默认会把它解析为 `127.0.0.1` ，当然这可以在 `/etc/hosts` 文件下进行修改。IPv6下默认 `::1` 
- `0.0.0.0`在`IPV4`中表示的是无效的**目标地址**。是一个特殊的 **通配符地址**，它通常表示“所有 IP 地址”或“未指定的地址”。
  - 在 **服务器端**，`0.0.0.0` 常常用于表示监听 **所有可用的网络接口**，不管是本地回环、局域网、还是外网接口。
  - 在 **客户端**，`0.0.0.0` 通常用于表示没有指定目标地址，通常与网络路由的配置和绑定相关。

### 服务器ping不通但是http能请求成功，会出现这种情况吗?什么原因造成的?

ping 走的是 icmp 协议，http 走的是 tcp 协议。

有可能服务器的防火墙禁止 icmp 协议，但是 tcp 协议没有禁止，就会出现服务器 ping 不通，但是 http 能请求成果。

# 计算机操作系统面试篇

## 用户态和内核态的区别？

### 用户态和内核态的区别？

- 内核态（Kernel Mode）：在内核态下，CPU可以执行所有的指令和访问所有的硬件资源。这种模式下的操作具有更高的权限，主要用于操作系统内核的运行。
- 用户态（User Mode）：在用户态下，CPU只能执行部分指令集，无法直接访问硬件资源。这种模式下的操作权限较低，主要用于运行用户程序。

## 进程管理

### 线程和进程的区别是什么？

- **定义：**
  - 进程是操作系统资源分配的基本单位
  - 线程是任务调度和执行的基本单位。
- **调度及切换：**
  - 每个进程都有独立的代码和数据空间（程序上下文），程序之间的切换会有较大的开销。
  - 线程可以看做轻量级的进程，同一类线程共享代码和数据空间，每个线程都有自己独立的运行栈和程序计数器（PC），线程之间切换的开销小。
- **内存分配：**
  - 系统在运行的时候会为每个进程分配不同的内存空间。
  - 对线程而言，除了CPU外，系统不会为线程分配内存（线程所使用的资源来自其所属进程的资源），线程组之间只能共享资源。
- **稳定性：**
  - 进程中某个线程如果崩溃了，可能会导致整个进程都崩溃。
  - 进程中的子进程崩溃，并不会影响其他进程。
- **包含关系：**
  - 没有线程的进程可以看做是单线程的。
  - 如果一个进程内有多个线程，则执行过程不是一条线的，而是多条线。

### 进程，线程，协程的区别是什么？

- 进程是操作系统中进行资源分配和调度的基本单位，它拥有自己的独立内存空间和系统资源。每个进程都有独立的堆和栈，不与其他进程共享。进程间通信需要通过特定的机制，如管道、消息队列、信号量等。由于进程拥有独立的内存空间，因此其稳定性和安全性相对较高，但同时上下文切换的开销也较大，因为需要保存和恢复整个进程的状态。
- 线程是进程内的一个执行单元，也是CPU调度和分派的基本单位。与进程不同，线程共享进程的内存空间，包括堆和全局变量。线程之间通信更加高效，因为它们可以直接读写共享内存。线程的上下文切换开销较小，因为只需要保存和恢复线程的上下文，而不是整个进程的状态。然而，由于多个线程共享内存空间，因此存在数据竞争和线程安全的问题，需要通过同步和互斥机制来解决。
- 协程是一种用户态的轻量级线程，其调度完全由用户程序控制，而不需要内核的参与。协程拥有自己的寄存器上下文和栈，但与其他协程共享堆内存。协程的切换开销非常小，因为只需要保存和恢复协程的上下文，而无需进行内核级的上下文切换。协程需要程序员显式地进行调度和管理，相对于线程和进程来说，其编程模型更为复杂。

### 创建一个协程的过程

使用 `CompletableFuture` 实现异步协程

在 Java 中，`CompletableFuture` 提供了对异步任务的支持。它允许你定义异步任务并在任务完成时做一些处理，从而实现类似协程的效果。 

### 线程运行过程中申请到的东西在切换时是否全部要保存，比如线程中有个循环，或者声明了很多对象，这些是否都要保存，也存在线程私有区吗？

- 线程运行过程中申请到的东西（如循环中的状态、声明的局部对象等）在切换时，不需要全部保存。线程切换时，操作系统只会保存和恢复线程的**上下文信息**（如栈指针、寄存器、程序计数器等）。
- 对于局部变量，它们存储在栈中，线程切换时会自动保存和恢复。**线程私有区**中的数据会保留，因为它是与线程相关联的。

### 多线程比单线程的优势，劣势？

- 优势：
  - 提高程序的运行效率，可以充分利用多核处理器的资源
  - 同时处理多个任务，加快程序的执行速度。
- 劣势：
  - 存在多线程数据竞争访问的问题，需要通过锁机制来保证线程安全，增加了加锁的开销，还有死锁的风险。

### 进程切换和线程切换的区别？

- **进程切换**：
  - 进程是由内核管理和调度的，所以进程的切换只能发生在内核态。
  - 操作系统需要保存当前进程的状态（如寄存器值、程序计数器等），然后加载目标进程的状态。这不仅包括虚拟内存的切换，还包括内核堆栈、文件描述符、信号处理等资源的切换。
- **线程切换**：
  - 同一进程中的线程共享相同的 **虚拟内存**，所以内存切换的开销要比进程切换小。
  - 线程切换只需要保存和加载线程的上下文（如寄存器、栈指针等），而不需要切换整个进程的内存空间。

### 进程上下文有哪些？

进程的上下文切换不仅包含了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。

### 进程间通讯有哪些方式？

- **管道pipe：**用于在同一主机上的进程间传输数据的一种通信方式。通常在**父子进程之间**使用。
  - 使用**内存缓冲区**，数据写入一端后，从另一端读取。
- **消息队列：**
- **共享内存：**不同进程间共享一块内存区域的通信方式，允许进程直接读写这块共享内存。
  - 需要使用 **信号量** 或 **互斥锁** 等同步机制，避免并发访问时出现数据冲突。
- **信号signal：**信号是操作系统用来通知进程某个事件发生的一种机制。
- **信号量（Semaphore）：**信号量是一种用于同步和控制对共享资源的访问的计数器。它可以被视为一个整数，用于表示资源的数量。
- **socket：**套接字是一种跨网络的进程间通信方式，通常用于不同主机之间的通信。它支持面向连接（如 TCP）和无连接（如 UDP）通信。支持全双工通信。

### 说一下同步和异步

- **同步（Synchronous）：**同步是指任务的执行是按顺序进行的，后续任务必须等前一个任务完成后才能开始执行。
- **异步（Asynchronous）：**异步是指任务的执行不需要等待前一个任务完成，任务可以并行执行，后续任务可以在前一个任务未完成时开始。

### 异步过程中谁来通知谁

- 通常是由**事件循环**、**回调函数**或**Future对象**来“通知”A任务。具体来说，A任务可以通过`await`或回调机制来等待和接收结果。
- 如果A任务依赖于I/O操作或其他异步任务的完成，通常会使用上述机制来管理任务状态的切换，确保任务完成时，能够通知A任务并继续执行后续操作。

## 内存管理

### 堆和栈的区别？

- **分配方式**：堆是动态分配内存，由程序员手动申请和释放内存，通常用于存储动态数据结构和对象。栈是静态分配内存，由编译器自动分配和释放内存，用于存储函数的局部变量和函数调用信息。
- **内存管理**：堆需要程序员手动管理内存的分配和释放，如果管理不当可能会导致内存泄漏或内存溢出。栈由编译器自动管理内存，遵循后进先出的原则，变量的生命周期由其作用域决定，函数调用时分配内存，函数返回时释放内存。

- **大小和速度**：堆通常比栈大，内存空间较大，动态分配和释放内存需要时间开销。栈大小有限，通常比较小，内存分配和释放速度较快，因为是编译器自动管理。

### 虚拟内存

- **传统内存：**
  - **一次性：**作业必须一次性全部装入内存后才能开始运行。
  - **驻留性：**一旦作业被装入内存，就会一直驻留在内存中，直至作业运行结束。
- **虚拟内存：**虚拟内存的最大容量是由计算机的地址结构（CPU寻址范围）确定的。虚拟内存的实际容量= min{内存和外存容量之和，CPU寻址范围}。32位计算机地址结构为4GB。
  - **多次性：**无需在作业运行时一次性全部装入内存，而是允许被分成多次调入内存。
  - **对换性：**在作业运行时无需一直常驻内存，而是允许在作业运行过程中，将作业换入、换出。
  - **虚拟性：**从逻辑上扩充了内存的容量，使用户看到的内存容量，远大于实际的容量。

虚拟内存实现有3种方式：**请求分页存储管理、请求分段存储管理、请求段页式存储管理。**

无论哪种方式都需要一定的硬件支持：包括一定容量的内存与外存；页表机制、段表机制作为主要的数据结构；中断机制：当用户程序要访问的部分会调入内存时产生中断；地址转换机构逻辑地址到物理地址的转换。

### 分段，分页对比

- **存储特性：**
  - 页是信息的物理单位。分页的主要目的是为了实现离散分配，提高内存利用率。分页仅仅是系统管理上的需要，完全是系统行为，对用户是不可见的。
  - 段是信息的逻辑单位。分段的主要目的是更好地满足用户需求。一个段通常包含着一组属于一个逻辑模块的信息。

- **存储大小：**
  - 页的大小固定且由系统决定。
  - 段的长度却不固定，决定于用户编写的程序。
- **用户进程地址维度：**
  - 分页的用户进程地址空间是一维的，程序员只需给出一个记忆符即可表示一个地址。
  - 分段的用户进程地址空间是二维的，程序员在标识一个地址时，既要给出段名，也要给出段内地址。
- **碎片问题：**
  - 分页的内存利用率高，不会产生外部碎片，只会有少量内部碎片。
  - 如果段长过大，会产生外部碎片。

## 中断和异常

### 硬中断、软中断讲一下

**硬中断**（Hardware Interrupt）和**软中断**（Software Interrupt）是计算机系统中中断机制的两种基本类型，它们都是用来打断当前的程序执行，转而处理一些重要任务或事件。

- **硬中断（Hardware Interrupt）：**由外部**硬件设备发出的信号**，用来通知CPU有紧急的事件或任务需要处理。
  - **定时器中断**：CPU通过定时器定期中断自己，以便执行操作系统的任务调度。
  - **I/O设备中断**：例如，网络接口卡接收到数据包时，触发中断通知CPU进行处理。
- **软中断（Software Interrupt）：**软中断是由**程序中的软件指令触发的中断**，它通常用于程序之间的通讯、系统调用或实现某些特定功能。
  - **系统调用**：用户程序通过触发软中断向操作系统请求资源，如文件读写、进程管理等。

### 异常

异常（Exception），陷入，也成为内中断，来自于CPU内部的事件。

如非法操作码、地址越界、算术溢出、虚存系统缺页等等，异常不能被屏蔽，一旦出现应当立即处理。

## 网络IO

### BIO、和NIO、AIO区别

| 特性         | BIO                                | NIO                                         | AIO                              |
| ------------ | ---------------------------------- | ------------------------------------------- | -------------------------------- |
| **工作模式** | 阻塞 I/O，数据流模式               | 非阻塞 I/O，缓冲区 + 通道 + 选择器          | 异步 I/O，回调通知               |
| **线程模型** | 每个连接一个线程                   | 多个通道由单线程管理（通过 Selector）       | 异步 I/O，无需阻塞或轮询         |
| **I/O 操作** | 阻塞，直到 I/O 操作完成            | 非阻塞，可以轮询多个通道的事件              | 异步，不会阻塞，完成时回调通知   |
| **适用场景** | 并发连接数少，低性能需求           | 高并发、大量连接，I/O 密集型应用            | 超高并发、大数据量 I/O 操作      |
| **性能问题** | 并发连接数多时性能差（线程开销大） | 比 BIO 性能好，但需要轮询（高并发时更有效） | 性能非常高，尤其适用于高并发应用 |

### select、poll、epoll 的区别是什么？

- **select：**
  - `select` 是最早出现的一种 I/O 多路复用机制，它允许在多个文件描述符上监视是否有 I/O 事件发生（如可读、可写等）。`select` 调用会阻塞并等待直到文件描述符集中的某个或某些文件描述符准备就绪，之后就返回。
  - 缺点：
    - **文件描述符限制**：`select` 限制了每次监视的文件描述符数量，通常是 1024（可以通过修改编译时的宏来增大，但还是有限制）。
    - **性能问题**：每次调用 `select` 都需要遍历所有的文件描述符集合，这对于大规模的文件描述符集会导致性能瓶颈。
    - **重新设置**：每次调用 `select` 时，必须重设文件描述符集，带来了额外的开销。
- **poll：**
  - `poll` 是对 `select` 的改进，具有与 `select` 相似的功能，但没有文件描述符数量限制。`poll` 用一个 `pollfd` 结构体数组来表示多个文件描述符及其事件，它返回时会告知哪些文件描述符有 I/O 事件发生。
  - 优点：
    - **没有文件描述符限制**：相较于 `select`，`poll` 不再有最大文件描述符数量的限制。
    - **动态文件描述符集合**：`poll` 可以动态地管理文件描述符集合，而不需要像 `select` 一样每次都重设整个集合。
  - 缺点：
    - **性能瓶颈**：尽管去除了文件描述符的数量限制，但 `poll` 仍然需要遍历整个文件描述符集合，每次调用时都需要遍历整个数组，这对于大量文件描述符来说性能较差。
    - **返回值不够直观**：`poll` 只是返回哪些文件描述符有事件发生，没有直接告诉应用程序事件发生的位置，因此需要遍历 `pollfd` 数组进行判断。
- **epoll：**
  - `epoll` 是 Linux 中引入的 I/O 多路复用机制，是对 `select` 和 `poll` 的进一步优化，专门为高并发场景设计。`epoll` 使用事件驱动的机制，提供了更高效的事件通知方式。`epoll` 会通过内核级别的事件通知，不需要每次遍历所有的文件描述符，只有在某些文件描述符准备就绪时，内核才会通知用户进程。
  - 优点：
    - **无文件描述符数量限制**：`epoll` 不受文件描述符数量限制，可以处理成千上万的连接。
    - **高效的性能**：`epoll` 使用基于回调的机制，只在事件发生时通知用户程序，不需要每次都遍历文件描述符集合。
    - **边缘触发和水平触发**：`epoll` 支持两种触发模式，**水平触发（Level Triggered）和边缘触发（Edge Triggered）**，提供了更灵活的控制。
    - **内存管理**：`epoll` 在内核中维护一个事件通知队列，通过 `epoll_wait` 来通知应用程序哪些文件描述符有事件发生，效率较高。
  - 缺点:
    - **只能在 Linux 上使用**：`epoll` 是 Linux 特有的系统调用，无法在其他操作系统上使用。
    - **复杂度较高**：相较于 `select` 和 `poll`，`epoll` 需要更复杂的设置和管理，尤其是在使用边缘触发模式时。

### epoll过程

1. `lfd=socket()`创建lfd套接字—>`bind()`绑定地址—>`listen()`设置监听上限—>`epoll_create()`创建监听红黑树—>`epoll_ctl()`把lfd加入红黑树—>while(1) 服务器端上线等待连接。
2. `epoll_wait()`服务器监听—>有事件发生—>`epoll_wait()`返回监听满足数组—>一旦有事件发生则lfd一定在满足数组中，`lfd`进行`accept()`—>用lfd进行`accept()`返回的连接套接字`cfd`放到红黑树中—>执行读操作—>进行大小写转换操作—>把`cfd`节点的属性从`EPOLLIN`变为`EPOLLOUT`—>再把`cfd`重新挂上红黑树去监听写事件—>等待`epoll_wait()`返回—>有返回说明能写—>执行写操作—>把`cfd`从红黑树中拿下来—>再把`cfd`节点的属性从`EPOLLIN`变为`EPOLLOUT`—>再把cfd重新挂上红黑树去监听读事件—>`epoll_wait()`服务器监听。从而形成一个完美的闭环。

###  epoll 的 边缘触发和水平触发有什么区别？

epoll 支持两种事件触发模式，分别是**边缘触发（edge-triggered，ET）和水平触发（level-triggered，LT**）。

| 特性             | **水平触发 (LT)**                                  | **边缘触发 (ET)**                                |
| ---------------- | -------------------------------------------------- | ------------------------------------------------ |
| **通知方式**     | 当文件描述符的事件条件满足时，每次调用都会返回     | 只有文件描述符的事件从未就绪变为就绪时才通知一次 |
| **事件处理**     | 只要事件没有处理完，`epoll` 会继续返回该文件描述符 | 事件一旦通知后，必须完全处理，否则不会再次通知   |
| **是否重复通知** | 会重复通知，直到事件完全处理                       | 不会重复通知，只有状态变化时才通知一次           |
| **适用场景**     | 适合大多数简单应用，处理事件比较慢时使用           | 适合高性能、高并发应用，要求迅速处理事件         |
| **性能**         | 性能较低，重复通知会增加开销                       | 性能较高，因为不重复通知                         |
| **实现难度**     | 简单，容易实现                                     | 稍复杂，要求处理得当，否则可能错过事件通知       |

- 使用水平触发模式，当内核通知文件描述符可读写时，接下来还可以继续去检测它的状态，看它是否依然可读或可写。
- 使用边缘触发模式，I/O 事件发生时只会通知一次，而且我们不知道到底能读写多少数据，所以在收到通知后应尽可能地读写数据，以免错失读写的机会。因此，我们会**循环**从文件描述符读写数据，那么如果文件描述符是阻塞的，没有数据可读写时，进程会阻塞在读写函数那里，程序就没办法继续往下执行。所以，**边缘触发模式一般和非阻塞 I/O 搭配使用**，程序会一直执行 I/O 操作，直到系统调用返回错误，错误类型为 EAGAIN 或 EWOULDBLOCK。

### redis，nginx，netty 是依赖什么做的这么高性能？

主要是依赖**Reactor 模式**实现了高性能网络模式，这个是在i/o多路复用接口基础上实现的了网络模型。

Reactor 模式主要由 Reactor 和处理资源池这两个核心部分组成，它俩负责的事情如下：

- Reactor 负责监听和分发事件，事件类型包含连接事件、读写事件；
- 处理资源池负责处理事件，如 read -> 业务逻辑 -> send；

#### Redis

**Redis 6.0 之前使用的 Reactor 模型就是单 Reactor 单进程模式。**

- 因为只有一个进程，**无法充分利用 多核 CPU 的性能**；
- Handler 对象在业务处理时，整个进程是无法处理其他连接的事件的，**如果业务处理耗时比较长，那么就造成响应的延迟**

单 Reactor 单进程的方案**不适用计算机密集型的场景，只适用于业务处理非常快速的场景**。所以Redis的瓶颈不在CPU上，网络IO才是瓶颈。

因此在Redis6.0后，Redis 在启动的时候，默认情况下会**额外创建 6 个线程**（*这里的线程数不包括主线程*）：

- Redis-server ： Redis的主线程，主要负责执行命令。
- bio_close_file、bio_aof_fsync、bio_lazy_free：三个后台线程，分别异步处理关闭文件任务、AOF刷盘任务、释放内存任务；
- io_thd_1、io_thd_2、io_thd_3：三个 I/O 线程，io-threads 默认是 4 ，所以会启动 3（4-1）个 I/O 多线程，用来分担 Redis 网络 I/O 的压力。

#### Netty

Netty 是采用了多 Reactor 多线程方案

![1](C:/Users/93752/Desktop/工作面试/小林coding/01-java基础res/1.png)

每个客户端通过通道（Channel）与服务端进行数据交互，客户端通过端口向服务端发送连接请求。服务端使用一个线程，通过多路复用器（Selector）来监听多个客户端的连接请求和数据事件，服务端会将每个客户端的通道注册到 Selector 上进行管理。

- 主线程和子线程分工明确，主线程只负责接收新连接，子线程负责完成后续的业务处理。
- 主线程和子线程的交互很简单，主线程只需要把新连接传给子线程，子线程无须返回数据，直接就可以在子线程将处理结果发送给客户端。

**Netty特点：**

- 事件驱动和异步机制：通过回调机制处理I/O结果
- boss-worker：boss只负责管理，具体处理交给worker，还有更加智能的线程管理
- 内存管理优化：采用内存池化区别于传统每次读取都会分配新的字节数组
- 支持多种协议：不仅有tcp、udp还有http、https、websocket等等

#### Nginx

nginx 是多 Reactor 多进程方案，不过方案与标准的多 Reactor 多进程有些差异。

- 主进程中仅仅用来初始化 socket，并没有创建 mainReactor 来 accept 连接
- 由子进程的 Reactor 来 accept 连接，通过锁来控制一次只有一个子进程进行 accept（为了避免多个子进程同时响应同一个请求，也就是惊群现象），子进程 accept 新连接后就放到自己的 Reactor 进行处理，不会再分配给其他子进程。

### 零拷贝是什么？

- 传统 IO 的工作方式，从硬盘读取数据，然后再通过网卡向外发送，我们需要进行 4 上下文切换，和 4 次数据拷贝，其中 2 次数据拷贝发生在内存里的缓冲区和对应的硬件设备之间，这个是由 DMA 完成，另外2 次则发生在内核态和用户态之间，这个数据搬移工作是由 CPU 完成的。

  ![2](C:/Users/93752/Desktop/工作面试/小林coding/01-java基础res/2.png)

- 零拷贝技术，它通过一次系统调用（sendfile 方法）合并了磁盘读取与网络发送两个操作，降低了上下文切换次数。另外，拷贝数据都是发生在内核中的，天然就降低了数据拷贝的次数。使用 `sendfile()`，内核直接将文件内容从磁盘缓冲区传输到网络接口，无需通过用户空间。

  ![3](C:/Users/93752/Desktop/工作面试/小林coding/01-java基础res/3.png)

**适用场景：**

- **Web 服务器**：在 Web 服务器中，零拷贝可以用来提高大文件（如视频、图片等）的传输效率。例如，Nginx 使用了 `sendfile()` 实现零拷贝。
- **高性能网络应用**：零拷贝特别适用于需要频繁进行大规模数据传输的应用，如文件传输、数据库备份、视频流等。
- **数据库管理系统**：数据库的磁盘和网络传输操作常常使用零拷贝技术，以提高数据读取和写入的效率。

# 数据结构与算法面试题

## 数据结构

### 红黑树说一下

**红黑树（Red-Black Tree）**是一种自平衡的二叉搜索树，它在插入和删除操作后能够通过旋转和重新着色来保持树的平衡。红黑树的特点如下：

- 每个节点都有一个颜色，红色或黑色。
- 根节点是黑色的。
- 每个叶子节点（NIL节点）都是黑色的。
- 如果一个节点是红色的，则它的两个子节点都是黑色的。
- 从根节点到叶子节点或空子节点的每条路径上，黑色节点的数量是相同的。

红黑树通过这些特性来保持树的平衡，确保最长路径不超过最短路径的两倍，从而保证了在最坏情况下的搜索、插入和删除操作的时间复杂度都为O(logN)。**epoll 用了红黑树来保存监听的 socket。**

### 跳表说一下？

**跳表（Skip List）**是一种基于链表的数据结构，它通过添加多层索引来加速搜索操作。

- 跳表中的数据是有序的。
- 跳表中的每个节点都包含一个指向下一层和右侧节点的指针

跳表通过多层索引的方式来加速搜索操作。最底层是一个普通的有序链表，而上面的每一层都是前一层的子集，每个节点在上一层都有一个指针指向它在下一层的对应节点。这样，在搜索时可以通过跳过一些节点，直接进入目标区域，从而减少搜索的时间复杂度。

跳表的平均搜索、插入和删除操作的时间复杂度都为O(logN)，但空间复杂度稍高。跳表常用于需要高效搜索和插入操作的场景，如数据库、缓存等。**redis 用了跳表来实现 zset。**

###  LRU是什么？如何实现？

LRU 是一种缓存淘汰算法，当缓存空间已满时，优先淘汰最长时间未被访问的数据。

实现的方式是哈希表+双向链表结合。

![4](C:/Users/93752/Desktop/工作面试/小林coding/01-java基础res/4.png)

- 使用哈希表存储数据的键值对，键为缓存的键，值为对应的节点。
- 使用双向链表存储数据节点，链表头部为最近访问的节点，链表尾部为最久未访问的节点。
- 当数据被访问时，如果数据存在于缓存中，则将对应节点移动到链表头部；如果数据不存在于缓存中，则将数据添加到缓存中，同时创建一个新节点并插入到链表头部。
- 当缓存空间已满时，需要淘汰最久未访问的节点，即链表尾部的节点。

上面这种思想方式，LRU 算法可以在 O(1) 的时间复杂度内实现数据的插入、查找和删除操作。

### 布隆过滤器怎么设计？时间复杂度？

「**布隆过滤器**」可以用来解决类似的问题，具有运行快速，内存占用小的特点，它是一个保存了很长的二级制向量，同时结合 Hash 函数实现的。

而高效插入和查询的代价就是，它是一个基于概率的数据结构，**只能告诉我们一个元素绝对不在集合内，对于存在集合内的元素有一定的误判率。**

- **初始化**：当我们创建一个布隆过滤器时，我们首先创建一个全由0组成的位数组（bit array)。同时，我们还需选择几个独立的哈希函数，每个函数都可以将集合中的元素映射到这个位数组的某个位置。
- **添加元素**：在布隆过滤器中添加一个元素时，我们会将此元素通过所有的哈希函数进行映射，得到在位数组中的几个位置，然后将这些位置标记为1。
- **查询元素**：如果我们要检查一个元素是否在集合中，我们同样使用这些哈希函数将元素映射到位数组中的几个位置，**如果所有的位置都被标记为1，那么我们就可以说该元素可能在集合中。如果有任何一个位置不为1，那么该元素肯定不在集合中**。

## 排序算法

### 排序算法及空间复杂度

- **插入类排序：**
  - **直接插入排序：**将待排序元素逐个插入到已排序序列的合适位置，形成有序序列。
    - 时间复杂度：平均为O(N^2)，最好情况下为O(N)，最坏情况下为O(N^2)。
    - 空间复杂度：因为每回只移动一个所以空间复杂度为O(1)。
    - 稳定性：稳定。
  - **折半插入排序：**将排好元素一分为二来进行查找插入的位置。
    - 时间复杂度：平均为O(N^2)，最好情况下为O(NlogN)，最坏情况下为O(N2)。
    - 空间复杂度：因为每回只移动一个所以空间复杂度为O(1)。
    - 稳定性：稳定。
  - **希尔排序：**将待排数组分成若干个稀疏的子序列，分别进行直接插入排序，使得稀疏的子序列较为有序，然后再全部进行次直接插入排序，即可完成。
    - 时间复杂度：业界统一认为为O(N^1.3)。
    - 空间复杂度：因为每回只移动一个所以空间复杂度为O(1)
    - 稳定性：不稳定。
- **交换类排序：**
  - **冒泡排序：**在扫描的过程中顺次比较相邻的两个元素的大小，若逆序就交换位置。
    - 时间复杂度：平均为O(N^2)，最好情况下为O(N)，最坏情况下为O(N^2)。
    - 空间复杂度：因为每回只移动一个所以空间复杂度为O(1)。
    - 稳定性：稳定。
  - **快速排序：**
    - 时间复杂度：平均为O(NlogN)，最好情况下为O(NlogN)，最坏情况下为O(N^2)。
    - 空间复杂度：使用递归进行深搜，所以为O(NlogN)。
    - 稳定性：不稳定。
- **选择排序：**
  - **简单选择排序：**从第一个记录开始，通过n-1次关键字比较，从n个记录中选择出关键字最小的记录，并和第一个记录进行比较。
    - 时间复杂度：平均为O(N^2)，最好情况下为O(N^2)，最坏情况下为O(N^2)。
    - 空间复杂度：因为每回只移动一个所以空间复杂度为O(1)。
    - 稳定性：不稳定。
  - **堆排序：**把待排序的数字看成一颗完全二叉树的顺序表示，每个结点表示一个记录，第一个记录作为二叉树的根，对剩下的记录依次逐层从左到右顺序排序，（i从0开始）任意节点r[i]的左孩子是r[2r+1]，右孩子是r[2i+2]，双亲是r[(i+1)/2-1]。对这颗完全二叉树进行调整。**大根堆：** r[i]>r[2i+1]且r[i]>r[2i+2]，也就是父节点大于孩子节点的完全二叉树称为大根堆。
    - 时间复杂度：平均为O(NlogN)，最好情况下为O(NlogN)，最坏情况下为O(NlogN)。
    - 空间复杂度：因为每回只移动一个所以空间复杂度为O(1)。
    - 稳定性：不稳定。

# 其他及云原生

## 消息队列

### 你的项目为什么要用消息队列？

- **解耦系统**：让系统的各部分之间不直接依赖，通过消息传递进行沟通，提升系统的灵活性和可维护性。
- **异步处理**：让一些耗时操作（比如发送邮件、扣库存等）可以异步执行，不影响用户体验。

- **流量控制**：当系统流量激增时，消息队列可以暂时存储消息，等系统有空闲时再处理，避免系统崩溃。
- **提高可靠性**：即使某些部分出问题，消息队列可以暂存信息，等恢复后再处理，减少数据丢失。
- **负载均衡**：多个消费者可以并行处理队列中的消息，平衡系统负载，提高处理效率。
- **扩展性**：当业务增长时，可以方便地增加消费者来处理更多消息，支持系统扩展。

### 你说说 Kafka 为什么是高性能的？

- **顺序写入**：它将消息按顺序写入磁盘，避免了随机写入带来的性能瓶颈。
- **零拷贝**：Kafka 使用零拷贝技术，直接从磁盘传输数据到网络，减少了内存复制，提高了效率。
- **分区并行**：数据分成多个分区，多个消费者可以并行读取，提升处理速度。
- **批量处理**：Kafka 支持批量消息写入和读取，减少了网络和磁盘操作的开销。

### RocketMQ、Kafka 和 RabbitMQ

- **Kafka**：Kafka 是基于 **分区**（Partition）和 **副本**（Replication）设计的分布式消息队列。它使用一个 **发布-订阅** 模型，生产者将消息写入到主题（Topic）中的分区，消费者可以根据分区消费消息。**只支持 发布-订阅 模式**。
  - **优点**：超高吞吐量，适合大规模数据流和实时处理。
  - **适用场景**：日志聚合、流处理、大数据分析。
  - **缺点**：不支持复杂的路由机制，消息顺序只保证在分区内。
- **RocketMQ**：RocketMQ 也采用分布式架构，支持 **主题（Topic）** 和 **队列（Queue）**。消息可以被多个消费者消费，也支持严格的 **顺序消息**、**事务消息** 和 **定时消息**。支持 **发布-订阅** 和 **点对点** 两种模式，允许同一个消息被多个消费者消费，适合广播消息。
  - **优点**：支持事务消息、顺序消息、消息精确一次投递。
  - **适用场景**：金融、电商等需要高可靠性、顺序消费和事务的场景。
  - **缺点**：性能稍逊于 Kafka，配置较复杂。
- **RabbitMQ**：RabbitMQ 使用 **AMQP（Advanced Message Queuing Protocol）** 协议，采用 **交换机（Exchange）** 和 **队列（Queue）** 的模式，消息通过交换机路由到队列，消费者从队列中获取消息。支持 **发布-订阅**、**点对点** 和 **工作队列** 等多种消息模型，适用于各种灵活的消息传递场景。
  - **优点**：支持多种协议，灵活的消息路由（如交换机），适合多种消费模式。
  - **适用场景**：小到中规模应用，微服务架构，复杂路由需求。
  - **缺点**：吞吐量和性能不如 Kafka 和 RocketMQ，延迟稍高。

### RabbitMq怎么消息被消费

**消息持久化**：确保消息不丢失。

**消息确认**：确保消息被成功消费。

**死信队列**：处理消费失败的消息。

**重试机制**：为消费失败的消息提供重试功能。

**并发和负载均衡**：提高消息消费效率。

## 系统设计

### 你项目是怎么存密码的？

原先使用一些很简单的加密算法，比如MD5、SHA-1。直接用 MD5 加密存储的，因为 MD5 是不可逆的，也就是不能从MD5加密后的结果转换回加密之前的结果，所以当时就想当然的认为这是安全的。听说 MD5 虽然不可逆，但是破解起来也是相对容易的，比如碰撞攻击和彩虹表攻击，都有可能将 MD5 解密。

- **碰撞攻击**是指通过找到两个不同的输入，得出相同的输出值（即哈希值）。对于 MD5 和 SHA-1，这种攻击相对容易实现。随着计算机性能的提高，攻击者可以生成大量的哈希值，寻找碰撞。
- **彩虹表**是一种预计算的哈希值字典，它能够通过存储大量常见密码的哈希值来加速破解过程。如果密码没有加盐，攻击者只需要在彩虹表中查找对应的哈希值，迅速得到原密码。

后来采用**密码加盐**

- **加盐**是指在用户的密码之前或之后加入一个随机生成的字符串（盐值，salt）。然后，再对包含盐值的密码进行哈希计算。由于每个密码都加上不同的盐值，即使两个用户的密码相同，它们的哈希结果也会不同。

或者采用多因素认证（MFA）或更权威的第三方认证。

手机验证码或者把登录交由更加权威的第三方认证。

### 如何设计一个分布式ID？

- 如果对性能和顺序性有较高要求，且有分布式系统的需求，**Snowflake算法**是一个常用且有效的方案。

  ![2](C:/Users/93752/Desktop/工作面试/小林coding/11-云原生res/2.png)

  - 雪花算法生成步骤：
    - **获取当前时间戳**：记录当前时间（精确到毫秒）。
    - **时间回拨处理**：如果当前时间小于上次生成 ID 的时间，发生了回拨（时钟回退）。
    - **计算 ID 生成的各个部分**：
      - **时间戳**：计算当前时间与自定义起始时间（Epoch）之间的差值，得到时间戳。**41位**
      - **机器 ID+服务ID**：由机器/节点的标识决定，通常是由配置文件或其他方式分配的。**这个可以自由组合，只要能区分开就行。**
      - **序列号**：如果同一毫秒内生成多个 ID，序列号会递增。当同一毫秒内生成超过最大数量时，算法会等待下一毫秒。**12位**
    - **拼接生成唯一 ID**：将以上的各个部分拼接起来，形成一个 64 位的唯一 ID。

- 如果只需要保证全局唯一性，并且ID长度不敏感，可以选择**UUID**。

- 如果希望数据库生成ID并且控制分布式锁的使用，可以选择**数据库自增ID + 分布式锁**。

### 单点登录是怎么工作的？

单点登录（SSO，Single Sign-On）是一种认证机制，它允许用户在一次登录后就能访问多个相互关联的应用系统，无需在每个应用中重复登录。SSO的基本原理是将用户身份认证的过程集中管理，用户只需在一个地方登录，其他需要认证的系统通过某种机制知道用户已通过认证。

JWT是一个由三部分组成的字符串，分别为：**头部**（Header）、**负载**（Payload）和**签名**（Signature）。它们之间通过 `.` 分隔。`<Header>.<Payload>.<Signature>`

- 用户在登录界面提供用户名和密码，提交给认证服务器。认证服务器验证用户身份后，生成一个JWT，通常会将一些用户信息（如用户ID、角色、权限）存放在负载部分，同时设置过期时间等。认证服务器使用密钥对JWT进行签名，生成最终的JWT令牌并返回给用户。
- 用户在访问需要身份验证的资源时，会在请求头中携带JWT令牌。通常会使用 `Authorization` 头部字段，格式为：`Authorization: Bearer <jwt_token>`
- 服务器收到请求后，首先从请求中提取JWT令牌。服务器使用预先共享的密钥（或者公钥）验证JWT的签名。如果签名有效，表示JWT没有被篡改，服务器会从JWT中提取用户信息（如用户ID、角色等），并允许用户访问资源。

**JWT优点：**

- **无状态**：JWT令牌是自包含的，它在令牌中就携带了所有需要的信息（如用户身份），不需要查询数据库或维护会话，节省了服务器的存储。
- **跨平台**：JWT基于JSON格式，可以在不同平台（如Web、移动端、微服务等）之间传输，兼容性好。
- **支持跨域认证**：由于JWT通常存储在浏览器的 `localStorage` 或 `sessionStorage` 中，且JWT为无状态认证，不依赖于服务端的会话，可以在多个域间传递JWT。
- **灵活性**：JWT的负载部分可以存储用户的自定义信息（如角色、权限等），便于服务端进行授权控制。

**JWT缺点：**

- **过期问题**：JWT通常有一个有效期，过期后需要重新获取。对于长时间登录的用户，可能需要实现刷新令牌机制。
- **安全性问题**：如果私钥泄露，攻击者可以伪造JWT，因此需要妥善保护签名密钥。
- **无法撤销**：一旦JWT被发放，直到过期前都无法撤销，除非通过特定的机制（如黑名单）来标记无效JWT。

### 分布式中CAP理论

三个主要特性之间的权衡关系：**一致性**（Consistency）、**可用性**（Availability）和 **分区容错性**（Partition Tolerance）。

- **一致性 (Consistency)**：
  - 所有节点在同一时刻看到的数据是一样的，也就是说，所有读操作返回的结果都是最新写入的数据。
  - 一致性要求：无论在任何节点进行数据读取，所读取到的数据都是完全一致的。
- **可用性 (Availability)**：
  - 系统在每个请求到来时，都能够返回一个有效的响应（即使数据未更新），即使部分节点不可用。
  - 可用性要求：系统始终能响应客户端请求，保证服务不被中断。
- **分区容错性 (Partition Tolerance)**：
  - 系统能够继续正常工作，即使网络分区或者节点间的通信出现问题（网络延迟、节点失联等）。
  - 分区容错性要求：系统能够应对在某些节点之间失联的情况下继续正常运行。

多数组合为CA、CP、AP

- **CA（一致性 + 可用性）**：传统的单节点数据库或集群中没有网络分区的情况。例如，MySQL、PostgreSQL 通过主从复制可以保证一致性和可用性，但如果网络出现问题，它们会丢失一些请求。
- **CP（一致性 + 分区容错性）**：HBase、Zookeeper 等，它们会在网络分区时牺牲可用性，保证数据的一致性。某些请求会被延迟或丢失，但数据的正确性是被保证的。
- **AP（可用性 + 分区容错性）**：Cassandra、MongoDB、Couchbase 等，优先保证系统始终能够响应请求，即使在网络分区发生时，数据也可能是最终一致的。这些系统通常会容忍数据不一致，直到网络恢复后进行数据同步。

## 场景提问

### CPU 100% 问题怎么排查？

1. 使用 `top` 或 `htop` 检查 CPU 使用情况：
   - `top` 命令可以显示 CPU 使用最多的进程，`htop` 是 `top` 的一个增强版本，具有更好的界面。**指定单个进程使用`top -p <pid>`**
   - 如果发现 `java` 进程占用了大量 CPU 资源，可以进一步进行分析。
2. **硬盘 I/O**：硬盘故障或 I/O 阻塞也可能导致 CPU 占用过高。
3. 检查是否有异常的守护进程（如 cron、systemd 服务等）消耗了过多资源。
4. 是否面临大量的流量（例如 DDoS 攻击），可能会导致 CPU 高负荷。

### 是java的问题

1. 使用 `jstack` 或 `jmap` 获取 Java 堆栈和内存信息。
2. 使用 Arthas 进行更详细的分析。

### 给个十亿个qq号，设计一个数据结构来管理

- 如果对存储和查询效率要求较高，可以选择 **哈希表** 或 **Trie 树**。
- 如果内存非常有限且只需要存在性检测，可以使用 **布隆过滤器**。
- 如果数据集超过内存容量，考虑使用 **数据库** 或 **分布式存储**。

### windows和linux创建空文件

- windows

  ```shell
  type nul > filename.txt
  ```

- linux

  ```shell
  touch filename.txt
  ```

### 有个java的服务，现在发现其中有个服务延迟很高，要你排查的话怎么解决这个事情

1. **确认延迟发生的具体服务**：确保是哪个具体的服务或接口发生了延迟。你可以通过日志、监控工具（如 Prometheus等）来确定。
2. **检查服务端日志：**检查该服务的日志文件，看是否有异常（如错误堆栈、警告信息、连接问题等）。
3. **查看性能监控工具：**使用如 **JVM监控**（例如：JVisualVM、JProfiler 或通过 Prometheus 的 Java Agent）来查看服务的内存使用、GC 频率、线程使用情况等，是否有内存泄漏或过度的垃圾回收造成延迟。
4. **网络层面排查：**使用 **tcpdump**抓包，检查服务间的网络通信是否存在问题（如超时、丢包等）。如果服务之间通过HTTP调用，可以使用 **Fiddler** 检查API请求/响应的延迟。
5. **数据库查询性能：**如果服务与数据库进行交互，检查数据库查询性能，尤其是慢查询。
6. **分析线程和锁：**查看是否存在线程池饱和、线程阻塞或死锁的情况。可以使用线程 dump 工具（如 jstack）来查看线程状态，找出可能的瓶颈。
7. **资源瓶颈分析：**查看服务是否存在 CPU 饱和的情况，使用 `top` 或 `htop` 工具查看资源使用情况。
8. **负载测试：**使用工具（如 **JMeter** 或 **Gatling**）对该服务进行负载测试，观察在高并发时的表现，帮助定位是否是由于负载过高导致的延迟。

## 数据库

### MySQL 的架构是怎么样的？

**客户端**：发起数据库请求的地方，比如应用程序或命令行工具。

**连接管理**：管理客户端和服务器之间的连接。

**查询解析**：解析客户端发来的 SQL 请求，确保语法正确。

**查询优化**：选择最优的执行计划来提高查询效率。

**执行引擎**：实际执行查询操作，访问数据并进行处理。

**存储引擎**：管理数据存储和检索的模块。常见的存储引擎有 InnoDB（支持事务）和 MyISAM（不支持事务）。

**缓冲池**：缓存数据以减少磁盘读取，提高查询速度。

**日志**：记录操作日志，用于数据恢复、复制和性能调优。

**复制**：支持将数据从主数据库复制到多个从数据库，实现数据同步。

**管理和监控工具**：用于数据库的管理、监控和性能优化。

### Redis 除了用作缓存还能干嘛？

- **消息队列**：支持异步任务处理，通过列表（List）和发布/订阅（Pub/Sub）机制实现消息传递。或者Zset做延迟队列。
- **实时统计**：可以用来统计点击量、访问量等，支持高效的计数器。
- **排行榜**：通过排序集合（Sorted Set）实现游戏得分或文章点赞等排名功能。
- **分布式锁**：保证多个进程间的资源访问互斥。
- **去重**：通过集合（Set）去重，防止重复数据操作。

### ES倒排索引说一下？

倒排索引（Inverted Index）是搜索引擎中常用的一种数据结构，目的是为了高效地进行文本搜索。具体到 Elasticsearch（ES），倒排索引用于存储文档中各个词项（词语）及其出现位置，以便快速查找包含某个词的文档。

**倒排索引的基本原理：**

- **词项（Term）**：文档中的每个单词或短语。

- **文档（Document）**：可以理解为一个包含多个词项的内容单元（如一篇文章、一条记录等）。

  ![1](C:/Users/93752/Desktop/工作面试/小林coding/11-云原生res/1.png)

### 倒排索引构建过程

- **文档索引**：每个文档被拆分成多个词项，这些词项会被映射到文档上。
- **查询**：当你执行查询时，ES 会通过倒排索引找到包含查询词项的文档，快速返回搜索结果。
- **反向分析**：ES 在搜索时，首先将查询的词进行分析和分词，然后用倒排索引查找匹配的文档。

## 云原生

### Docker和传统虚拟机有什么区别？

- **架构：**
  - **虚拟机**：虚拟机在物理服务器上运行一个完整的操作系统，每个虚拟机都有自己的操作系统（包括内核）。
  - **Docker**：Docker 是基于容器的，容器共享宿主操作系统的内核，但它们运行自己的用户空间。
- **资源利用**：
  - **虚拟机**：虚拟机需要分配一部分硬件资源（如 CPU、内存、硬盘等）给每个虚拟机操作系统
  - **Docker**：Docker 容器利用宿主机的操作系统内核，资源消耗相对较少。
- **启动时间**：
  - **虚拟机**：启动虚拟机需要加载完整的操作系统，通常需要几分钟时间。
  - **Docker**：Docker 容器利用宿主操作系统内核，启动速度非常快，通常只需要几秒钟。
- **隔离性**：
  - **虚拟机**：虚拟机提供更强的隔离性，因为每个虚拟机都有独立的操作系统和内核。
  - **Docker**：Docker 容器提供了相对较弱的隔离性，因为容器共享宿主机的内核，但容器之间通过命名空间和控制组进行隔离。
- **灵活性和可移植性**：
  - **虚拟机**：虚拟机的迁移和部署较为复杂，因为每个虚拟机都是一个完整的操作系统，迁移时需要确保目标主机的资源和配置匹配。
  - **Docker**：Docker 容器是独立于操作系统的，它们可以在任何支持 Docker 的平台上运行，极大地提高了应用的可移植性和灵活性。
- **使用场景**：
  - **虚拟机**：适用于需要完全隔离的环境，或者需要不同操作系统的场景。常用于数据中心、虚拟化环境。
  - **Docker**：适用于微服务架构、开发和测试环境。由于 Docker 容器更轻便，广泛应用于 DevOps、持续集成/持续部署（CI/CD）等场景。

### Docker 和 k8s 之间是什么关系？

Docker 和 Kubernetes（K8s）是密切相关的，但它们的功能不同：

- **Docker**：是一个容器化平台，用于打包、分发和运行应用。它提供了创建、管理和运行容器的工具。容器可以理解为轻量级的虚拟机，但它们共享宿主操作系统的内核。
- **Kubernetes（K8s）**：是一个容器编排平台，用于自动化容器的部署、扩展和管理。它可以帮助管理多个 Docker 容器，处理容器的生命周期、调度、负载均衡等任务。

Docker 是 K8s 使用的容器运行时（container runtime）。也就是说，K8s 管理和调度的是 Docker 容器的实例。K8s 可以启动、停止和扩展 Docker 容器，但它不提供容器创建的功能，那个是由 Docker 提供的。

Docker 用于创建和运行容器，K8s 用于管理多个 Docker 容器的集群。 

## 部署

### java源码打包成jar包括什么

- 编译后的 `.class` 文件。
- `META-INF/MANIFEST.MF` 文件。
- 任何项目中的资源文件（如 `.properties`、`.xml` 等）。
- 根据构建配置，可能还会包含外部依赖（如果使用了 Maven Shade 插件等）。\

### git使用工作流程

**工作流程步骤**：

1. 从 `master` 分支创建一个新的功能分支

   ```shell
   git checkout -b feature-branch
   ```

2. 在功能分支上进行开发和提交

   ```shell
   git add .                ### 添加更改
   git commit -m "feature: add new feature"  ### 提交更改
   ```

3. 保持功能分支与 `master` 分支同步在功能开发过程中，定期将 master分支的最新更改合并到功能分支，以防止冲突。

   ```shell
   git checkout master      ### 切换到主分支
   git pull origin master   ### 拉取最新的 master
   git checkout feature-branch
   git merge master         ### 将 master 的更新合并到 feature 分支
   ```

4. 功能完成后将功能分支合并到 `master` 分支

   ```shell
   git checkout master
   git merge feature-branch
   ```

5. 推送到远程仓库并删除功能分支

   ```shell
   git push origin master    ### 推送更新到远程仓库
   git branch -d feature-branch  ### 删除本地功能分支
   git push origin --delete feature-branch  ### 删除远程功能分支
   ```

**优点**：每个功能开发都在独立的分支中进行，避免了不同功能的代码干扰。 

**缺点**：需要频繁合并，管理多个分支可能稍显复杂。

### git merge 、git rebase、 git fetch的区别

| 命令         | 作用                               | 特点                                 |
| ------------ | ---------------------------------- | ------------------------------------ |
| `git merge`  | 将两个分支的更改合并               | 保留分支历史，产生合并提交           |
| `git rebase` | 将一个分支的提交重放到另一个分支上 | 重写历史，产生线性历史               |
| `git fetch`  | 从远程仓库拉取最新的提交和元数据   | 不修改本地工作区，只更新远程跟踪分支 |

- 使用 `git merge` 时，你的分支历史会保留原样，适合保留不同开发路径的痕迹。
- 使用 `git rebase` 时，历史会变得更整洁，适合在合并前清理历史。
- 使用 `git fetch` 时，只有从远程仓库拉取数据，并不会对本地分支产生直接影响。

































































































































































